
Robust Mobile Malware Detection (2020) by Khoda, Mahbub
Abstract:
The increasing popularity and use of smartphones and hand-held devices have made them the most popular target for malware attackers. Researchers have proposed machine learning-based models to automatically detect malware attacks on these devices. Since these models learn application behaviors solely from the extracted features, choosing an appropriate and meaningful feature set is one of the most crucial steps for designing an effective mobile malware detection system. There are four categories of features for mobile applications. Previous works have taken arbitrary combinations of these categories to design models, resulting in sub-optimal performance. This thesis systematically investigates the individual impact of these feature categories on mobile malware detection systems. Feature categories that complement each other are investigated and categories that add redundancy to the feature space (thereby degrading the performance) are analyzed. In the process, the combination of feature categories that provides the best detection results is identified. Ensuring reliability and robustness of the above-mentioned malware detection systems is of utmost importance as newer techniques to break down such systems continue to surface. Adversarial attack is one such evasive attack that can bypass a detection system by carefully morphing a malicious sample even though the sample was originally correctly identified by the same system. Self-crafted adversarial samples can be used to retrain a model to defend against such attacks. However, randomly using too many such samples, as is currently done in the literature, can further degrade detection performance. This work proposed two intelligent approaches to retrain a classifier through the intelligent selection of adversarial samples. The first approach adopts a distance-based scheme where the samples are chosen based on their distance from malware and benign cluster centers while the second selects the samples based on a probability measure derived from a kernel-based learning method. The second method achieved a 6% improvement in terms of accuracy. To ensure practical deployment of malware detection systems, it is necessary to keep the real-world data characteristics in mind. For example, the benign applications deployed in the market greatly outnumber malware applications. However, most studies have assumed a balanced data distribution. Also, techniques to handle imbalanced data in other domains cannot be applied directly to mobile malware detection since they generate synthetic samples with broken functionality, making them invalid. In this regard, this thesis introduces a novel synthetic over-sampling technique that ensures valid sample generation. This technique is subsequently combined with a dynamic cost function in the learning scheme that automatically adjusts minority class weight during model training which counters the bias towards the majority class and stabilizes the model. This hybrid method provided a 9% improvement in terms of F1-score. Aiming to design a robust malware detection system, this thesis extensively studies machine learning-based mobile malware detection in terms of best feature category combination, resilience against evasive attacks, and practical deployment of detection models. Given the increasing technological advancements in mobile and hand-held devices, this study will be very useful for designing robust cybersecurity systems to ensure safe usage of these devices.Doctor of Philosoph

FullText:
ROBUST MOBILE MALWARE DETECTIONMAHBUB E KHODAA dissertation submitted in partial fulfillment ofthe requirements for the degree ofDOCTOR OF PHILOSOPHYFEDERATION UNIVERSITY AUSTRALIASchool of Engineering, IT and Physical SciencesSEPTEMBER 2020c© Copyright by MAHBUB E KHODA, 2020All Rights ReservedDeclarationThis thesis is my own work and has not been submitted in any form for another de-gree or diploma at any university or other institute of tertiary education. Informationderived from the published and unpublished work of others has been acknowledgedin the text and a list of 



References is given.Mahbub E Khoda4 September 2020iiAcknowledgmentAll praise be to Allah, the almighty, the most merciful. I praise and thank Himabundantly for giving me the capability, intellect, and opportunity to undertake thisresearch.I express my profound indebtedness to my supervisors Joarder Kamruzzaman, IqbalGondal, and Ashfaqur Rahman for their constant support, effective guidance, in-sightful advice, helpful criticisms, valuable suggestions, and endless patience towardsthe completion process of this thesis. I was given enough freedom to explore theresearch areas of my choice and guided when I felt lost. Without their insights andencouragements, this research would not have been completed.My sincere gratitude goes to my mother Zahanara Khatun for her endless love andinspiration throughout my life. I am especially thankful to my brother Neyamul Kabirwho has been a guardian and a father figure for me since my childhood. He took careof me in the absence of my father, provided financial support, and was always therefor me whenever I needed him. I also thank my beloved wife Sadia Islam for her love,care, patience, and understanding.An important acknowledgment goes to Capstone Editing who provided copyedit-ing and proofreading services, according to the guidelines laid out in the university-endorsed national ‘Guidelines for Editing Research Theses’. I thank Federation Uni-versity Australia for granting me the Australian Government Research Training Pro-gram (RTP) Stipend and RTP Fee-Offset Scholarship, and CSIRO for Data61 top-upscholarship. Finally, I thank all the staffs, graduate students, and friends at theSchool of Engineering, Information Technology and Physical Sciences, Gippsland, fortheir support and encouragement during the last few years.iii



AbstractThe increasing popularity and use of smartphones and hand-held devices have madethem the most popular target for malware attackers. Researchers have proposedmachine learning-based models to automatically detect malware attacks on thesedevices. Since these models learn application behaviors solely from the extractedfeatures, choosing an appropriate and meaningful feature set is one of the most cru-cial steps for designing an effective mobile malware detection system. There are fourcategories of features for mobile applications. Previous works have taken arbitrarycombinations of these categories to design models, resulting in sub-optimal perfor-mance. This thesis systematically investigates the individual impact of these featurecategories on mobile malware detection systems. Feature categories that complementeach other are investigated and categories that add redundancy to the feature space(thereby degrading the performance) are analyzed. In the process, the combinationof feature categories that provides the best detection 

Results is identified.Ensuring reliability and robustness of the above-mentioned malware detection systemsis of utmost importance as newer techniques to break down such systems continueto surface. Adversarial attack is one such evasive attack that can bypass a detectionsystem by carefully morphing a malicious sample even though the sample was orig-inally correctly identified by the same system. Self-crafted adversarial samples canbe used to retrain a model to defend against such attacks. However, randomly usingtoo many such samples, as is currently done in the literature, can further degradedetection performance. This work proposed two intelligent approaches to retrain aclassifier through the intelligent selection of adversarial samples. The first approachadopts a distance-based scheme where the samples are chosen based on their distancefrom malware and benign cluster centers while the second selects the samples basedon a probability measure derived from a kernel-based learning method. The secondmethod achieved a 6% improvement in terms of accuracy.To ensure practical deployment of malware detection systems, it is necessary to keepthe real-world data characteristics in mind. For example, the benign applications de-ployed in the market greatly outnumber malware applications. However, most studieshave assumed a balanced data distribution. Also, techniques to handle imbalanceddata in other domains cannot be applied directly to mobile malware detection sincethey generate synthetic samples with broken functionality, making them invalid. Inthis regard, this thesis introduces a novel synthetic over-sampling technique thatensures valid sample generation. This technique is subsequently combined with a dy-namic cost function in the learning scheme that automatically adjusts minority classweight during model training which counters the bias towards the majority class andstabilizes the model. This hybrid method provided a 9% improvement in terms ofF1-score.Aiming to design a robust malware detection system, this thesis extensively studiesmachine learning-based mobile malware detection in terms of best feature categorycombination, resilience against evasive attacks, and practical deployment of detectionmodels. Given the increasing technological advancements in mobile and hand-helddevices, this study will be very useful for designing robust cybersecurity systems toensure safe usage of these devices.vAcronymsADB Android Debug BridgeAPI Application Programming InterfaceAPK Android PacKageAUC Area Under the CurveBERT Bidirectional Encoder Representation from TransformersCC Category CenterCD Constructive DivergenceCE Cross EntropyCERT Cyber Emergency Response TeamCNN Convolutional Neural NetworkDAE Deep AutoEncoderDBN Deep Belief NetworkDNN Deep Neural NetworksDVM Dalvik Virtual MachineFL Focal LossFN False NegativeFP False PositiveICC Inter Component CommunicationKBL Kernel Based LearningKNN K-nearest neighborsMFE Mean False ErrorMFNE Mean False Negative ErrorMFPE Mean False Positive ErrorviMRV Malware Recomposition VariationMSE Mean Squared ErrorOS Operating SystemRBF Radial Basis FunctionRBM Restricted Boltzman MachineReLU Rectified Linear UnitRF Random forestROC Receiver Operating CharacteristicSMOTE Synthetic Minority Over-sampling TechniqueSMS Short Message ServiceSSL Secure Sockets LayerSVM Support Vector MachineTN True NegativeTP True PositiveUI User InterfaceXML Extensible Markup LanguageviiNomenclatureΩ Global feature listE Energy function of a neural networkv Visible unit datah Hidden unit datac Visible unit biasb Hidden unit biasp Activation probability of a neuronCDk K-step constructive divergenceα Learning rate||w|| Length of vector wsign(.) Sign (positive or negative) of a valueγ Free parameter for RBF functionloss() Loss function of a machine learning algorithmx̂ Adversarial sample crafted from original sample x∇x(y) Derivative of y with respect to xΥ Maliciousness indicator scoreWui Weight value for UI callback eventWbe Weight value for 

Background eventΓ Contextual feature value of an API functionK Maximum separation value between user-aware and unaware APIcallk Steepness parameter for a sigmoid functionU Set of all contextual values of user aware and unaware calls of anAPI functionviiiEn(X) Entropy of a random variable XIg(X|Y ) Information gain of a random variable X given Yϑ Relevancy thresholdδx Perturbation added to sample xy′ Predicted class label by a classifieryt Target class label of an adversarial attackJM Jacobian matrixSd Distance score of an adversarial sampleDistk Distance of a sample from cluster center kPA,B Platt’s probabilityN+ Number of positive samplesN− Number of negative samplesXmal Malware feature matrixXben Benign feature matrixNmal Number of malware samplesNben Number of benign samplesρ Imbalance ratioIdxm List of indices of mutable featuresvb Centroid of class bMCi(x) Membership degree of a sample x to a class CiPw Dynamic minority class weightix



Table of ContentsPageAcknowledgment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii



Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ivAcronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viNomenclature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii



List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii



List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvPublications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii1 



Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.1 Research Challenges and Motivation . . . . . . . . . . . . . . . . . 21.2 Research Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . 41.3 Research 



Methodology . . . . . . . . . . . . . . . . . . . . . . . . 51.3.1 Dataset Collection . . . . . . . . . . . . . . . . . . . . . . . 51.3.2 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . 61.3.3 Innovative Model Design and Evaluation . . . . . . . . . . 61.4 

Overview of Contributions . . . . . . . . . . . . . . . . . . . . . . 61.5 Organization of the Thesis . . . . . . . . . . . . . . . . . . . . . . 92 



Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112.1 Android Operating System . . . . . . . . . . . . . . . . . . . . . . 122.2 Android Application Structure . . . . . . . . . . . . . . . . . . . . 132.3 Mobile Malware Types . . . . . . . . . . . . . . . . . . . . . . . . 152.4 Propagation of Malware . . . . . . . . . . . . . . . . . . . . . . . . 162.5 Machine Learning Techniques in Mobile Malware Detection . . . . 172.6 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232.6.1 Data Pre-processing and Cleanup . . . . . . . . . . . . . . 24x2.7 Tools for Extracting Features from Applications . . . . . . . . . . 252.7.1 Static Feature Extraction Tools . . . . . . . . . . . . . . . 252.7.2 Dynamic Feature Extraction Tools . . . . . . . . . . . . . . 262.8 Performance Metrics for Malware Detection Systems . . . . . . . . 262.9 Mobile Malware Detection Systems Based on Feature Categories . 282.9.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 292.9.2 Dynamic Analysis . . . . . . . . . . . . . . . . . . . . . . . 342.9.3 Hybrid Approach . . . . . . . . . . . . . . . . . . . . . . . 362.10 Adversarial Attack . . . . . . . . . . . . . . . . . . . . . . . . . . 402.10.1 Adversarial Samples . . . . . . . . . . . . . . . . . . . . . . 402.10.2 Adversarial Sample Crafting Techniques and Defense Mech-anisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412.11 Imbalanced Data Problem for Mobile Malware Detection . . . . . 462.11.1 Data Level Approaches . . . . . . . . . . . . . . . . . . . . 462.11.2 Algorithm Level Approaches . . . . . . . . . . . . . . . . . 492.11.3 Hybrid Approaches . . . . . . . . . . . . . . . . . . . . . . 512.12 Open Research Challenges . . . . . . . . . . . . . . . . . . . . . . 542.13 

Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 Impact of Feature Categories in Mobile Malware Detection . . 583.1 Detection Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593.1.1 Feature Extraction Component . . . . . . . . . . . . . . . . 593.1.2 Feature Matrix Formation . . . . . . . . . . . . . . . . . . 633.2 Feature Category Characteristics . . . . . . . . . . . . . . . . . . . 643.3 Encoding Contextual Information as Feature Value . . . . . . . . 713.4 Experiments and 

Results . . . . . . . . . . . . . . . . . . . . . . . 753.4.1 Characteristics of Individual Feature Categories and TheirEffects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753.4.2 Malware Detection using Different Combinations of FeatureCategories . . . . . . . . . . . . . . . . . . . . . . . . . . . 773.4.3 Effect of Contextual Information . . . . . . . . . . . . . . . 803.4.4 Validating with Other Widely Used Classifiers . . . . . . . 803.4.5 Relevancy Analysis of Feature Categories . . . . . . . . . . 833.4.6 Further Analysis of ICC Feature Category . . . . . . . . . 853.5 

Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 874 Robustness Against Adversarial Malware Attacks . . . . . . . . 894.1 



Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89xi4.2 Adversarial Examples . . . . . . . . . . . . . . . . . . . . . . . . . 904.3 



Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 924.3.1 Crafting Adversarial Malware Samples . . . . . . . . . . . . 934.3.2 Proposed Selection Methods for Adversarial Retraining . . 964.4 Experiments and 

Results . . . . . . . . . . . . . . . . . . . . . . . 984.4.1 Resilience by Retraining with Deep Neural Network . . . . 984.4.2 Resilience by retraining with Other Classifiers . . . . . . . 1054.5 

Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1095 Tackling Data Imbalance in Model Training . . . . . . . . . . . . 1105.1 



Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1105.2 



Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1115.2.1 Proposed Technique for Synthetic Malware Oversampling . 1115.2.2 Proposed Approach 1: Modified Fuzzy-SMOTE . . . . . . . 1155.2.3 Proposed Approach 2: Modified Loss Function with Dy-namic Minority Class Weight . . . . . . . . . . . . . . . . . 1175.3 Experiments and 

Results . . . . . . . . . . . . . . . . . . . . . . . 1195.3.1 Dataset Preparation . . . . . . . . . . . . . . . . . . . . . . 1195.3.2 



Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205.4 

Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1296 

Conclusion and 



Future Works . . . . . . . . . . . . . . . . . . . . . 130



References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134xii



List of Tables2.1 Popular support vector machine kernels . . . . . . . . . . . . . . . . . 212.2 

Summary of Android malware detection techniques based on featuretypes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292.3 

Summary of key works in mobile malware detection . . . . . . . . . . 372.4 

Summary of key research and their main contributions on adversarialattacks in mobile malware detection. . . . . . . . . . . . . . . . . . . 452.5 Imbalanced data handling approaches 

Overview. . . . . . . . . . . . . 472.6 

Summary of key research and their contributions on mobile malwaredetection with imbalanced data. . . . . . . . . . . . . . . . . . . . . . 533.1 Malware detection performance with individual feature category. Deeplearning model is used for classification. . . . . . . . . . . . . . . . . . 763.2 Malware detection performance with combination of two feature cate-gories. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783.3 Malware detection performance with combination of more than twofeature categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793.4 Malware detection performance using different categories of feature andtheir combination with embedded contextual information in relation toAPI calls. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 813.5 Information accessed by generated colluding apps. . . . . . . . . . . . 853.6 Malware detection performance with generated collusion attacks usingICC feature category . . . . . . . . . . . . . . . . . . . . . . . . . . . 864.1 Accuracy and G-mean of adversarial retraining with different deep neu-ral network architectures . . . . . . . . . . . . . . . . . . . . . . . . . 103xiii4.2 Accuracy of different classifiers when tested on clean samples and sam-ples mixed with adversarial samples. . . . . . . . . . . . . . . . . . . 1095.1 Detailed performance comparison for 10% and 5% imbalanced ratio. . 125xiv



List of Figures1.1 An illustration of the overall contribution of this thesis. . . . . . . . . 72.1 Android OS architecture . . . . . . . . . . . . . . . . . . . . . . . . . 132.2 Android Application Structure . . . . . . . . . . . . . . . . . . . . . . 142.3 DNN with n hidden and m visible units . . . . . . . . . . . . . . . . . 182.4 Illustration of an SVM classifier . . . . . . . . . . . . . . . . . . . . . 202.5 K-Nearest Neighbors classifier . . . . . . . . . . . . . . . . . . . . . . 222.6 Decision tree classifier . . . . . . . . . . . . . . . . . . . . . . . . . . 232.7 Random forest classifier . . . . . . . . . . . . . . . . . . . . . . . . . 242.8 Example of an ROC curve and AUC. . . . . . . . . . . . . . . . . . . 282.9 SMOTE algorithm for handling imbalanced datasets . . . . . . . . . . 493.1 Mobile malware detection system . . . . . . . . . . . . . . . . . . . . 603.2 Installation screen showing the required permission for an application. 653.3 Most commonly occurring permission features in malware and benignapplications in our dataset. . . . . . . . . . . . . . . . . . . . . . . . . 663.4 Most commonly occurring API features in malware and benign appli-cations in our dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . 673.5 Apps communicating through ICC mechanism. . . . . . . . . . . . . . 693.6 Collusion attack using ICC mechanism. . . . . . . . . . . . . . . . . . 693.7 Most commonly occurring ICC features in malware and benign appli-cations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 703.8 Call graph of Moon SMS malware . . . . . . . . . . . . . . . . . . . . 733.9 ROC curve and AUC for individual feature categories . . . . . . . . . 773.10 ROC curve and AUC for combination of feature categories . . . . . . 793.11 ROC curve and AUC with embedded contextual information . . . . . 81xv3.12 Performance of different classifiers with various feature category com-binations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823.13 Number of relevant features . . . . . . . . . . . . . . . . . . . . . . . 843.14 ROC curve and AUC with generated collusion attacks using ICC fea-ture category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 863.15 Classifier training time for different combinations . . . . . . . . . . . 874.1 Illustrative example of an adversarial sample . . . . . . . . . . . . . . 914.2 Work flow of selective adversarial retraining . . . . . . . . . . . . . . 944.3 Accuracy and recall values of deep neural network after adversarialretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1004.4 G-mean and F1-score of DNN after adversarial retraining with randomand selective samples . . . . . . . . . . . . . . . . . . . . . . . . . . . 1024.5 ROC curve and AUC for adversarial retraining using random, the Eu-clidean distance-based and KBL-based selection . . . . . . . . . . . . 1044.6 Malware detection accuracy of adversarial training for a) SVM b) Ran-dom forest and c) Bayesian classifier. . . . . . . . . . . . . . . . . . . 1064.7 G-mean value of adversarial training for a) SVM b) Random forest andc) Bayesian classifier. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1074.8 F1-score value of adversarial training for a) SVM b) Random forestand c) Bayesian classifier. . . . . . . . . . . . . . . . . . . . . . . . . 1085.1 Work flow of proposed imbalanced data handling approaches . . . . . 1125.2 Illustrating membership degree for sample xi and xj to class Cb and Cm1165.3 Minority class weight characteristic curve . . . . . . . . . . . . . . . . 1195.4 Effect of imbalance on malware detection (Imbalanced ratio of 10%indicates only 10% of the total samples are malware) . . . . . . . . . 1215.5 Performance comparison of the proposed approaches with over-samplingand under-sampling techniques and when no imbalance-specific tech-nique is used. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123xvi5.6 ROC curve and AUC of the proposed approaches, over-sampling andunder-sampling techniques, and when no imbalance-specific techniqueis used. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1275.7 Performance of the proposed approach 1 with varying fuzzy region. . 128xviiPublications from this ResearchPublished works1. Mahbub E Khoda, Tasadduq Imam, Joarder Kamruzzaman, Iqbal Gondal, Ash-faqur Rahman “Robust Malware Defense in Industrial IoT Applications usingMachine Learning with Selective Adversarial Samples”, IEEE Transactions onIndustry Applications (Rank: JCR-Q1, impact factor=3.488)2. Mahbub E Khoda, Joarder Kamruzzaman, Iqbal Gondal, Tasadduq Imam “Mo-bile Malware Detection-An Analysis of the Impact of Feature Categories”, Inter-national Conference on Neural Information Processing (ICONIP), 2018 (CORE/former ERA rank: A)3. Mahbub E Khoda, Joarder Kamruzzaman, Iqbal Gondal, Tasadduq Imam,Ashfaqur Rahman “Mobile Malware Detection: An Analysis of Deep LearningModel”, 2019 IEEE International Conference on Industrial Technology (ICIT)(CORE/former ERA rank: B)4. Mahbub E Khoda, Joarder Kamruzzaman, Iqbal Gondal, Tasadduq Imam, Ash-faqur Rahman “Selective Adversarial Learning for Mobile Malware”, 18th IEEEInternational Conference On Trust, Security And Privacy In Computing AndCommunications (TrustCom) 2019 (CORE/ former ERA rank: A)5. Mahbub E Khoda, Joarder Kamruzzaman, Iqbal Gondal, Tasadduq Imam, Ash-faqur Rahman “Mobile Malware Detection with Imbalanced Data using a NovelSynthetic Oversampling Strategy and Deep Learning”, 16th International Con-ference on Wireless and Mobile Computing, Networking and Communications(WiMob) 2020 (CORE/former ERA rank: B)Awaiting decision1. Mahbub E Khoda, Joarder Kamruzzaman, Iqbal Gondal, Tasadduq Imam, Ash-faqur Rahman “Malware Detection in Edge Devices with Fuzzy Oversamplingand Dynamic Class Weighting”, Journal of Applied Soft Computing (Rank:JCR-Q1, impact factor=5.472)xviii



Chapter One



IntroductionSmartphones, tablets, and other hand-held mobile devices are being increasingly usedin mobile commerce, online transactions, and sharing of personal data due to their easeof use, continuous connectivity, and convenience [1–4]. Statista [5] reports that thenumber of smartphone users worldwide surpassed 3.5 billion in 2020. This widespreaduse has made mobile devices an inevitable target of cyber-crime through dissemina-tion of malware [6]. In 2019, Cyber Emergency Response Team (CERT) Australiareported responding to more than 13,672 cyber-attacks that amounted to an esti-mated annual loss of US$328 million [7]. Such attacks are shifting more towardsmobile and handheld devices [8, 9]. Malware developers target these devices to stealinformation and gain privileged access, which is then used for criminal activities.Therefore, detecting malware in these platforms is becoming increasingly important.Malware detection in such platforms offers some unique challenges. A significantamount of work has been conducted in the field of malware detection for desktopand wired environments, but, some key differences between the desktop and mobileenvironments necessitate addressing this issue separately for mobile platforms. Forexample, the application structure, techniques, and execution methods are very dif-ferent in mobile devices, and the operating systems for mobile devices are optimizedfor particular execution environments. As a result, the applications (apps) developed1for these systems are fundamentally different. Thus, specific techniques for malwaredetection tailored to suit these environments is essential. The structure and organi-zation of such a system (Android) and its applications are described in detail in Sec.2.1 and 2.2. Device location is another point of difference between wired and mobileplatforms. By their nature mobile devices connect to various networks including pub-lic, private, personal, and work networks. Naturally, security settings vary in suchnetworks, making mobile devices vulnerable to various novel exploits and attacks thatdiffer from the wired devices where change of location is usually not an issue.Researchers have proposed several mobile malware detection systems based on ma-chine learning models. Considering the requirements of an effective and robust mobilemalware detection system and our study on the current literature, we have identifieda number of important research challenges. These are discussed below.1.1 Research Challenges and MotivationFor designing a machine learning-based malware detection system, analyzing the ap-plications and extracting meaningful and effectual features from them is a crucialstep. Features are properties of applications used to train a machine learning model.From a security point of view, an application is analyzed in two different ways namely,static analysis and dynamic analysis. Static analysis decodes the application file andparses them to extract features while dynamic analysis executes the application in asuitable environment and records its run-time behavior. Static features of an applica-tion have three sub-categories: permissions, Inter Component Communication (ICC),and Application Programming Interface (API) calls. Additional information can alsobe associated with some of the feature categories, for example, an API call can beinitiated by user interaction or by a system-generated event. These are known ascontextual information about features and can provide additional assistance for an-alyzing application behavior. A detailed description of feature categories and, static2and dynamic analysis is presented in Sec. 2.9.A number of works in the literature have proposed malware detection systems usingthese feature categories. For example, some works considered the permission features[10–12], some considered API features [13, 14], and a few considered ICC features[15] that exposed application collusion attacks (detailed in Sec.3.2). However, theseworks chose the feature categories in a randomized fashion without paying attentionto the individual effect of these categories. Since these feature categories representdifferent characteristics of mobile applications, it is necessary to undertake a sys-tematic investigation of the behaviors of these feature categories regarding malwaredetection. Previous works have not considered this. Also, an attempt to find the bestcombination is necessary because a blind combination of all the feature categoriesmay not lead to good performance. For example, dynamic features complement thepermission features very well, whereas, our experiments showed that some featurecategories introduce redundancy when combined with others.Despite the promising performance of machine learning-based malware detection sys-tems, ensuring the reliability and robustness of these systems remains an importantissue. Specifically, the recent discovery of sophisticated evasive techniques to breakdetection systems makes this an attractive research topic. For example, a machinelearning-based system can be evaded through adversarial samples crafted by carefullymodifying a previously developed malware. A successful adversarial sample is able togo undetected by a detection system even though the original version was correctlyidentified by the same system. Though adversarial retraining is proposed as a defensemechanism against these attacks, using an excessive number of arbitrary samplesfor retraining can further lead to performance degradation since the model starts toover-fit. Existing works choose the samples in a random manner, which often leadsto a sub-optimal choice of the samples for retraining. Selecting an appropriate setof samples that will make a system resilient and significantly enhance the detectionperformance is a major research challenge here.3Moreover, class-balanced datasets used in most prior studies reflect an unrealistic as-sumption since in the real world benign applications outnumber malware samples bya significant margin making the actual data distribution highly imbalanced. As a re-sult, imbalanced data handling ability is one of the primary requirements for a robustdetection system. Existing techniques in the literature cannot be applied here sincethey modify features randomly and generate invalid samples with broken functionality[16, 17]. Therefore, a customized data balancing technique for generating syntheticmalware samples preserving valid functionality is required. Additionally, combiningalgorithm-level techniques with the data balancing for improved performance needsto be investigated.1.2 Research ObjectivesThe principal aim of this research is to design a robust mobile malware detectionsystem. Considering the various research opportunities outlined in previous 



Sections,the following major research objectives were identified:1. Systematic investigation of the characteristics of individual feature categoriesand their impact on mobile malware detection. Additionally, extract the con-textual information, embed it in the feature space, and examine its effect onclassification performance. This objective leads to the following research ques-tions:RQ-1: How does combining different categories of features affect classifier per-formance, and which combination provides the best performance?RQ-2: Does embedding contextual information into the feature space improveclassifier performance?2. Addressing the vulnerability of malware detection systems to adversarial attacksand formulating strategies to make a malware detection system robust against4such attacks. The corresponding 

Research Questions addressed this objective:RQ-3: What are the adverse effects of adversarial samples on a malware de-tection system?RQ-4: How to selectively choose samples for adversarial retraining to makeclassifier robust against adversarial attacks?3. Handling imbalanced data problem while avoiding generation of invalid syn-thetic samples. The 

Research Questions pertaining to this objective are:RQ-5: How to balance the data with valid synthetic malware samples thatpreserve code functionality?RQ-6: Can the performance be improved by a hybrid method combining thenovel synthetic sampling in training set preparation and a dynamic cost schemain model training?1.3 Research 



MethodologyThis 



Section describes the overall research 



Methodology that will be undertaken inthis work to address the above-mentioned objectives.1.3.1 Dataset CollectionTo validate our proposed approaches through experiments, publicly available datasetswill be used and for completeness, more recent and newer samples will be collected.For this purpose, the most prominent datasets used in mobile malware detectionresearch (i.e., Malgenome and Drebin [14, 18]) will be used and processed to makethem usable for machine learning models. However, these datasets contain only asmall portion of recent malware. Hence, the Androzoo [19] tool will be used to obtaina more recent set of samples and they will be processed accordingly. 



Section 2.6describes the characteristics of the datasets in detail.51.3.2 Feature ExtractionSince our first research objective addresses the impact of different feature categorieson mobile malware detection, an exhaustive list of static and dynamic features fromdifferent categories will be extracted in our work. 



Section 3.1 discusses the featureextraction process and the characteristics of different feature categories in detail.1.3.3 Innovative Model Design and EvaluationOur research will adopt innovative approaches to achieve the research objectives.For example, to tackle adversarial attacks, we will explore the possibility of usinga selective sampling mechanism that finds out the best possible set of adversarialsamples which will be used to retrain a machine learning model to make it robust.Additionally, we will explore designing an innovative over-sampling method and/ormodification of machine learning algorithms to better handle imbalanced data prob-lem. The well-known stratified 10-fold cross-validation [20] approach will be usedfor performance evaluation. A range of widely used performance measures includingaccuracy, recall, F1-score, and G-mean will be used for quantitative analysis. 



Section2.8 defines the performance measures used in this thesis. Additionally, the WilcoxonSigned-Rank Test [21] will be used to verify the statistical significance of the improve-ments achieved by our proposed approaches compared to recent competing methods.1.4 

Overview of ContributionsTo accomplish the major research objectives mentioned in Sec. 1.2, this work presentsa number of original contributions as illustrated in Fig. 1.1. The individual contri-butions are divided into blocks that work towards a robust mobile malware detectionsystem. Block 1 in Fig. 1.1 represents the works for Objective 1 which investigatesthe individual feature categories and identifies the combination that provides thebest detection performance. Knowledge from this contribution provides the baseline6Impact analysis of feature categoriesIncorporate contextual informationRobust malware detection systemSelective adversarial retrainingHybrid imbalanced data handlingEvade detection with adversarial samplesIntelligent sample selectionDynamic cost schemaSynthetic sample preserving functionalityFigure 1.1 An illustration of the overall contribution of this thesis.for later works. Block 2 represents our contribution to making a mobile malwaredetection system robust against adversarial attacks. Block 3 outlines the works forObjective 3 which addresses the imbalanced data problem. The overall contributionof this thesis is elaborated on below:1. A comprehensive investigation of the impact of various feature categories inmobile malware detection is presented. The best combination of feature cate-gories the is, the combination that best enhances the performance of a machinelearning-based malware detection system is identified in the process. Addition-ally, we incorporated contextual information of API call feature into this work.Contextual information can provide additional knowledge regarding the usage ofcertain features and application behavior. Existing works use contextual infor-7mation in an isolated manner which is not suitable for use in combination withother features. We formulated a mechanism that embeds this contextual infor-mation into the feature space. This facilitates the usage of this information incombination with other feature categories, further improving the performance ofa malware detection system. We found ICC feature category adds redundancyto the feature space and degrades classification performance when comparedwith other feature categories. Findings from this work have been published in[22] and [23].2. An improved selective adversarial retraining strategy is proposed to achieveresilience against adversarial attacks. It is shown that if too many adversarialsamples are used for retraining, as has been done in the prior works, the modelstarts to over-fit and performance degrades. Hence, careful selection of thesamples used for retraining is required. To address this issue, two differentapproaches are proposed. First, a sample selection strategy is proposed based ontheir distance from malware and benign cluster center. Second, a kernel-basedselective sampling technique is proposed to identify the adversarial samples thathave a higher probability of being incorrectly classified. Experiments show thatusing the kernel-based samples for retraining improve detection accuracy by6% compared to the existing random selection-based techniques and makes theclassification system robust against such attacks. This work has been publishedin [24] and [25].3. With a view to handling imbalanced data for mobile malware detection, a hy-brid of data-level and algorithm-level method is proposed. For data balancing,a novel malware over-sampling technique is formulated that generates syntheticmalware samples with valid functionality, thereby mitigating the issues of priorworks. This technique improves the fuzzy set theory-based over-sampling tech-nique as it does not generate invalid samples as in [16]. Further, we devised acost schema that dynamically assigns weight to the minority class samples (i.e.,8malware) which simultaneously counters the bias of the classifier towards themajority class and reduces model instability. This technique provided a 9% im-provement in terms of F1-score compared to existing techniques. The findingsfrom this work have been published in [26].1.5 Organization of the ThesisThe overall organization of this thesis is outlined below.



Chapter 2 presents an 

Overview of the relevant research works in mobile malwaredetection. The operating system and application structure of the most popularmobile platform (Android) is presented to demonstrate the particular character-istics of this domain. Different machine learning tools used for mobile malwaredetection models are discussed. A detailed study of the feature categories ispresented. Related research works associated with these feature categories arealso presented and their 

Limitations identified. Advanced evasive techniques thatbreak down malware detection systems are discussed and the scarcity of relevantworks in this domain is highlighted. The inapplicability of existing techniques isevidenced and the necessity of domain-specific research is established.



Chapter 3 shows the characteristics of individual feature categories for mobile mal-ware detection. It investigates classification performance using all possible com-binations of the feature categories and identifies the combination that providesthe best 

Results. It also details our proposed technique for embedding the con-textual information into the feature space. Performance improvement is verifiedthrough extensive experimentation. This achieves our first objective and providesthe baseline for subsequent works.



Chapter 4 presents the works related to our second research objective. It illus-trates a robust defense technique against adversarial attacks through intelligentadversarial retraining. We proposed two novel sample selection strategies in this9



Chapter to find the best possible subset of adversarial samples for retraining. De-tails of these strategies are described in this 



Chapter. The increased robustness ofthe resulting detection model compared to the random selection used in previouswork is demonstrated through experimental 

Results.



Chapter 5 presents the works for our third research objective. It investigates theimbalanced data problem for mobile malware detection systems and proposesa hybrid of data-level and algorithm-level technique. The detail of our novelsynthetic malware over-sampling strategy is presented here. At the algorithmlevel, a dynamic cost schema is proposed and how it automatically adjusts theminority class weight based on validation class performance is presented. The

Results of extensive experiments with different levels of data imbalance are shownto validate the performance improvements.



Chapter 6 concludes the thesis by summarizing the overall contributions and dis-cusses new research directions based on the findings from this work.10



Chapter Two



Literature ReviewAs highlighted in 



Chapter 1, designing a robust malware detection system for mobileplatforms is the primary research goal of this thesis. The mobile platforms includesmartphones, tablets, and other hand-held devices which are increasingly being usedfor work purpose as well as personal use. [27–32]. Among different mobile operatingsystems including Windows and iOS, Android is currently the most popular oper-ating system for smartphones and hand-held devices [33–37]. Research in [38] and[39] showed that there are two billion Android devices active monthly and this hasallowed the Android system to capture more than 80% of the mobile operating systemmarket. Hence, this work focuses on the Android platform. However, the techniquesproposed in this work are equally applicable to other platforms. The following 



Sectiondescribes Android operating system and its application structure. This is followedby a 



Discussion on mobile malware, their types, and their propagation techniques.Thereafter, a detailed review of the existing works on mobile malware detection ispresented and their 

Limitations discussed.112.1 Android Operating SystemFigure 2.1 shows the architecture of the Android operating system. Android OS iswritten in Java and C/C++. It has five major 



Sections:Linux Kernel: The bottom-most layer of the Android OS is a modified Linux kernel.It provides a level of 



Abstraction between the device hardware and contains all thenecessary device drivers.Libraries: This 



Section sits on top of the Linux kernel and consists of a set oflibraries including WebKit for web browsing, SSL for internet security, and SQLitefor database management.Android Runtime: This 



Section provides a key component called Dalvik VirtualMachine, which is a modified java virtual machine specifically designed and optimizedfor Android applications. This enables every Android application to run in its ownprocess sandbox. This 



Section also provides some core Android libraries that are usedby developers to write Android applications.Application Framework: The application framework 



Section provides many high-level services to Android applications in the form of java classes. Application devel-opers can use these services in their applications.Applications: All the applications that are used by the user sit in this layer. Whena new application is installed it gets added to this layer.An Android application is generally written in Java code which is then compiledto Java bytecode. This bytecode is further converted to Dalvik executable that isoptimized for Android platforms and can be executed in the Dalvik Virtual Machine.In the following 



Section, we present the structure of an Android application (app)which will be useful to understand the feature extraction process for malware detec-tion.12



Chapter 2

Background2.1 

Overview of Android OSIn the Figure 2 [5], the Android software stack items in green are the writtenin C/C++ and the blue ones are written in Java which executed using the DalvikVM [5]. Here the Android Linux Kernel is a modified Linux Kernel which includesfeatures like wake locks (memory management for optimizing the memory consump-tion), Binder IPC Drivers and other features which play a key role in mobile embed-ded platform [21]. The libraries item plays a vital role in optimizing CPU, memoryconsumption as well as the audio and video codecs for the device.Figure 2: Android architectureAndroid runtime is the managed runtime that is capable to compiling Androidapplications during the installation time. This component comprises of Dalvik vir-tual machine and core libraries of Java. During an Android application compilation,the Java bytecode is converted into Dalvik bytecode (Dalvik executable code) usingdx tool, which is executed on Dalvik virtual machine. The classes.dex file consists4Figure 2.1 Android OS architecture [40]2.2 Android Application StructureFigure 2.2 shows the structure of an Android application. These applications arestored in a compressed format known as Android PacKage (APK) file. This is a zipfile consisting of Dalvik bytecodes (classes.dex files), compiled and plain resources,assets (e.g., music, texts), and XML manifest file. Dalvik bytecodes is the sourcecode of the app compiled and optimized for Android systems. The resources foldercontains graphics, strings, and screen layouts. The complied resources folder containsthe compiled binary version of the files in the resources folder. The native librariesfolder contains the outer libraries required for the app and the assets folder containsother files required in their raw format. Signatures and certificates are also included13Android Manifest (AndroidManifest.xml)Assets(assets/)Signatures and certificates (META-INF)Compiled resources (Resources.arsc)Native libraries (lib/)Dalvik bytecode (classes.dex)Resources (res/)Figure 2.2 Android Application Structurein the application package to verify the application and the developer. Finally, theandroidmanifest.xml, also known as the manifest file, specifies the meta informationsuch as, requested permission and application components.Android applications are made of four kinds of components that serve different pur-poses. These components must be declared in the manifest file before they can beused in the code. Following is a brief description of these components.Activity: Activity is the component that interacts with the application users. Itpresents the user interface and takes different commands through the interface.Service: Services are used to perform tasks in the 

Background. Generally, computation-intensive tasks are performed through the services.Content Providers: Content providers are used for sharing data among the appli-cations. This data can be application-specific or system generated.Broadcast Receivers: Broadcast receiver of an application is the component used to14receive broadcast message objects from other components. An application can registerbroadcast receivers to listen to various events including the battery level change, thescreen shut off, and change in the network level. These events are broadcast throughobjects known as intents.2.3 Mobile Malware TypesThe first malware attack for mobile systems was reported in 2004 for Symbian OS [41].However, due to the ever-increasing popularity of Android system since its launchin 2007, it has become the most popular target operating system among malwareattackers, with new attacks being launched every few weeks [41–47]. Below we brieflydescribe the types of malware targeting Android systems.Trojans and viruses. Trojans are malicious applications that appear like legitimateapps. When executed, they can cause severe damage to the system [48]. Trojans areone of the major threats to the Android system since they take control of the systemand steal critical information such as banking information, credit card numbers, andcontacts. Some versions of Trojans can take root control (elevated access) of thesystem and gain unauthorized access to sensitive files and memory.Spyware and adware. Spyware applications secretly steal user information andupload them to the attacker that may later be exploited for financial gain or futureattacks [49]. This user information is also used by adware: a form of malware thatshows intrusive advertisements and notifications on the screen [50]. These malwareapplications cause privacy leaks and often get installed by disguising themselves aslegitimate apps.Phishing apps. Phishing apps use the same principles as web phishing and usesthem on mobile websites and apps [51]. These apps covertly steal user informationsuch as account passwords and credit card details. Some Trojanized apps also usephishing schemes disguising as system updates or gaming applications.15Botnets. Botnets are networks of devices infected with malicious apps known as bots[52]. Through bots, attackers can take control of many infected mobile devices overa network. This network of devices can be used to launch powerful and sophisticatedattacks to commit digital crimes.2.4 Propagation of MalwareAndroid malware uses various sophisticated methods to propagate through mobiledevices [53]. Below we list the most commonly used Android malware propagationtechniques.Infected apps and websites. The most common way to spread mobile malware isthrough infected apps and malicious websites. Malware developers repackage popularapps with hidden malicious activities. Additionally, malicious websites exploit systemvulnerabilities for malware propagation.Third party app store. Third-party app stores have less security as comparedto the official ones. These third-party app stores are one of the main platforms formalware developers to distribute malicious apps. Most of the repackaged apps arefound in these app stores with appealing offers (e.g., paid functionalities unlocked) tolure users into installing them.Spams. Spams are another widely used technique for malware propagation. Anattacker sends emails to users with attractive offers that contain malicious links. Adevice gets infected once an unsuspecting user clicks such links.Adware. Adware applications display appealing on-screen ads to mobile users.Sometimes these ads are targeted based on user’s information collected by othermalicious software. When clicked, these ads can take a user to a malicious site or canexploit other vulnerabilities.Worms. Worms have self-propagating functionality. Once a device is infected, a16worm can propagate to other devices through different vulnerability-exploits.Dynamic loading and fake update. Some malware apps keep their maliciousfunctionality hidden before installation. When installed, it loads the malicious codesthrough dynamic loading. Fake update is another way to load malicious codes. Theapplication appears to perform a necessary update while the device gets infected inthe 

Background.2.5 Machine Learning Techniques in Mobile MalwareDetectionAs mentioned previously machine learning models have been widely deployed forautomated malware detection systems due to the infeasibility of manual inspectionfor various reasons. Also, we have seen from the current literature that researchershave adopted a variety of machine learning models for malware detection systems.Most notable among them are Support Vector Machine (SVM), K-nearest neighbors(KNN), Decision Tree, and Random Forest. Additionally, Deep Neural Networks(DNN) based detection systems have gained recent popularity among the malwareresearchers due to their promising performance in different fields such as voice recog-nition, image classification, object detection, and pattern matching [25, 54–79]. Wehave also used the deep neural network-based classifier in our experiments along withother classifiers. Below we describe the various machine learning models most com-monly used for malware detection systems.Deep Neural Network (DNN)A neural network is a machine learning architecture that is structured with layersof neurons which are the primary computing units of the network. Multiple hiddenlayers between the output and input layers make a neural network deep allowing morecomplex relationships among the features to be modeled. The input layer takes the17b1bmb2c1cnc2. . .. . .w11w12wmnh1h2h3v1v2v3. . .. . .. . .. . .Output (malware/benign)InputFigure 2.3 RBM with n hidden and m visible units in two layers (b = biasesfor visible units, c = biases for hidden units, w = connection weights)feature vectors as input. Each subsequent layer of the network takes input from theprevious layer and transforms it into some 



Abstract representation which is fed as theinput for the next layer. The network hierarchically extracts complex relationshipsamong the features through the learning process. Deep neural networks were popu-larized by Hinton et al. who showed the fundamentals of the training process of aDeep Belief Network (DBN) in [80]. DBN architecture is constructed from multiplelayers of Restricted Boltzman Machines (RBMs) [81]. An RBM consists of a visiblelayer and a hidden layer where each node in the visible layer is connected to everynode in the hidden layer. The energy function of the network is given byE = −hTWv − cTv − bTh (2.1)where W is the matrix representing the weights of connections between visible andhidden layer, v and h are vectors representing data on the visible unit and the hiddenunit respectively, b and c are the bias vectors for two the layers. The activationprobability of visible and hidden units are given by18p(hj = 1|v) = sig(bj +Wjv) (2.2)p(vi = 1|h) = sig(ci + hTWi) (2.3)where i is the index of a visible unit and j is the index of a hidden unit. Here, sigmoidfunction (sig()) is shown as example of the activation function for the nodes. Severalother activation functions are also used, REctified Linear Unit [82] (RELU) is anotherpopular activation function in this regard.Figure 2.3 shows an RBM structure with n hidden and m visible units. MultipleRBMs are stacked on top of one another to construct a DBN where the hiddenlayer of the first RBM serves as the visible layer of the second RBM and so on. Aftertraining each of the RBMs individually DBN fine-tunes the parameters through k-stepConstructive Divergence (CD-k) method [83] where the gradient θ of the log-likelihoodof one training pattern v0 is approximated by the following equationCDk(θ,v0) =∑hp(h|vk)δE(vk,h)δθ− p(h|v0)δE(v0,h)δθ(2.4)To update the connection weights, first, the positive gradient denoted by vhT is com-puted. Afterward, using the previous activation h, subsequent visible and hiddenlayer activation is computed denoted by v′ and h′ respectively. Thereafter, the neg-ative gradient v′h′T is computed and the weights of the network are updated usingthe following equation∆Wij = α(vhT − v′h′T ) (2.5)here α de

Notes the learning rate.19Figure 2.4 Illustration of an SVM classifier. Dashed line represents a can-didate hyper-plane while the solid line represents the optimal hyper-plane.Support Vector Machine (SVM)Support vector machine is based on the statistical learning theory developed by Vap-nik [84]. It is a supervised learning algorithm that categorizes samples by calculatinga separating hyper-plane.A hyper plane in an n-D feature space can be represented by the following equation:f(x) = xTw + b =n∑i=1xiwi + b = 0 (2.6)where x and w are n-dimensional vectors and b represents intercepts of the verticalaxes. Dividing by ||w||, we getxTw||w||= − b||w||(2.7)indicating that the projection of any point x on the plane onto the vector w is always−b/||w||, i.e., w is the normal direction of the plane, and |b|/||w|| is the distance fromthe origin to the plane. Note that the equation of the hyper plane is not unique.The n-D space is partitioned into two regions by the plane. Specifically, we define amapping function y = sign(f(x)) ∈ {1,−1},f(x) = xTw + b = > 0, y = sign(f(x)) = 1, x ∈M< 0, y = sign(f(x)) = −1, x ∈ B (2.8)20Any point x ∈ M on the positive side of the plane is mapped to 1 (i.e., malware),while any point x ∈ B on the negative side is mapped to -1 (i.e., benign). A point xof unknown class will be classified to M if f(x) > 0, or B if f(x) < 0.Non-linear SeparabilityFigure 2.4 shows a simple classification scenario for SVM where the samples arelinearly separable. The dashed line represents a candidate hyper-plane whereas thesolid line optimizes the separation margin between the two classes. In case of a morecomplex scenario where samples are not linearly separable, SVM maps the data intoa higher dimensional feature space where constructing a separating hyper-plane ismore feasible. Table 2.1 shows some popular kernels used for this purpose.Table 2.1 Popular support vector machine kernelsKernel Expression CommentPolynomial K(x, z) = (xTz + 1)d Maps the originaln-dimensional samples tond dimensionGaussian K(x, z) = exp(−‖x−z‖2)2σ2σ demotes the standarddeviation of the samplepoints.RBF K(x, z) = exp(γ‖x− z‖2) , γ > 0 ‖x− z‖ de

Notes euclideandistance and γ is a freeparameter used to controlover-fittingK-Nearest Neighbors (KNN)KNN [85] is a very popular classification technique for its simplicity and its highlycompetitive performance. KNN is a non-parametric and lazy algorithm in that, itdoes not make any assumption about the structure of the data rather the structure isderived from the data itself, and also, it does not do any kind of generalization usingthe training data. KNN works based on a majority voting scheme for classification.To classify a new sample KNN will inspect the class label of its k nearest neighbors21MalwareBenignDecision = MalwareFigure 2.5 K-Nearest Neighbors classifier. Yellow dot represents an un-known sample which is classified as malware based on the majority votingfrom its nearest k(=5) neighbors.based on some distance measure (i.e., Manhattan, Euclidean, Minkowski). The newsample will be assigned to the class which is most common among those nearestneighbors. Figure 2.5 illustrates the working principle of a KNN classifier. Here, thealgorithm considers 5 nearest neighbors of a sample for the majority voting scheme.In Fig. 2.5, red dots represent malware samples and green dots represent benignsamples. The yellow dot is an unknown sample that is classified as malware since 3of its 5 nearest neighbors are malware.Decision TreeA decision tree is a data structure that looks like a tree consisting of nodes and leavesrepresenting attributes and class labels respectively. Each node in the tree selects anattribute to split the data into two subsets that become two branches of the node.To classify an unknown sample the tree is traversed according to the feature value tothe samples until a leaf node is reached. The sample is classified as per the label ofthe leaf node.Random ForestRandom forest (RF) [86] is an extension of decision tree classifiers that fits manyclassification trees to a dataset and combines them to predict an outcome in response22Feature X?Feature A? BenignMalwareFeature C?BenignFeature M?MalwareBenignFigure 2.6 Decision tree classifier. Each node subdivides the tree based ona particular feature until a leaf node (decision) is reached.to an input. The RF algorithm starts off by choosing many bootstrap samples fromthe data. Approximately 63% of typical bootstrap of the original observations occursat least once [87]. However, those observations in the original data that do not occurin the bootstrap sample are called out-of-bag observations. A classification tree ismapped to each bootstrap sample. However, each considers only a small numberof randomly chosen variables for the binary partitioning. Lastly, the trees will befully grown and each will be utilized to predict the out-of-bag observations. Thecalculation of the predicted class of an observation is based on the majority vote ofthe out-of-bag predictions for that observation, with the ties split randomly.2.6 DatasetMalicious application sets for research and experimental purpose is mainly obtainedthrough research communities such as Contagio database and Virustotal [88, 89].Some researchers also collect and share datasets on request such as the Malgenomedataset [18]. However, the most prominently used dataset in Android malware re-search is the Drebin dataset [14] which is available to the researchers upon requestand used by most of the major works in Android malware detection [54, 56, 90–98].The key characteristics of this dataset are listed below.23Tree 1 Tree 2Tree 4Tree 3Tree 5Tree 1 : MalwareTree 2: BenignTree 3: MalwareTree 4: MalwareTree 5 : BenignMalwareDecisionMajority votingFigure 2.7 Random forest classifier. The final outcome of the classifier isbased on majority voting from a collection (forest) of trees.• The dataset comprises of 5,560 malware and over 120,000 benign applications.• The samples in the dataset have been collected from August 2010 to October2012.• It provides the static features extracted from the samples including permission,API calls, and Android activities.Due to its prominence in the research community, we also used this dataset for ourwork. Additionally, samples from Androzoo [19] were included to account for morerecent malware techniques that are developed between 2016 and 2019. Further pro-cessing for specific experiments was conducted which are described in their due course.2.6.1 Data Pre-processing and CleanupEven though the Drebin dataset provides the extracted features along with the mal-ware application files, we have noticed a lot of noisy data in the feature vectors. For24example, some applications record an API call feature with the name of ’;’ (a singlesemicolon). Since these features are extracted by parsing outputs from older versionsof third party static analysis tools, it is possible that these noises were generatedduring the analysis or the parsing phase. To mitigate this, we extracted features forthese files with more recent and advanced analysis tools. However, the dataset con-tains some very old application files which became obsolete since none of the currentexecution environments (mobile or emulator) will accept them. These applicationseither raised exception during the analysis phase or resulted in empty feature filesand were consequently removed from the dataset. A similar process was conductedfor Androzoo dataset for double-checking however, it did not create much of an issuesince most of the files are very recent. We extracted both static and dynamic featuresfor all the files and the specifics of feature extraction and vector creation are describedin the next 



Chapter.2.7 Tools for Extracting Features from ApplicationsSeveral tools are available for extracting static and dynamic features from a mobileapplication. These are commonly known as reverse engineering tools. In the following,we describe some of most popular reverse engineering tools for Android applications.2.7.1 Static Feature Extraction ToolsApktool [99] is one of the most popular tools for extracting static features froman Android application. It is a Java programming language-based tool that candecompile an APK file and decode its resources. As the APK files are nothing buta compressed zip file, traditional software can be used to unzip its resource filessuch as AndroidManifest.xml, classes.dex and META-INF files. However, the filesare encoded and cannot directly be used for analysis. Apktool can decode thesefiles and make them readable. The decoded AndroidManifest.xml file can be used to25extract various features such as permission, Android activities, Intents, and Broadcastreceivers while the classes.dex files can be used to extract API calls and call graphs.Androguard [100] is another reverse engineering tool for static analysis developed byGoogle and gaining more popularity for its sophistication and ease of use. Whilethe files decoded by Apktool needs to be manually parsed, Androguard does thisautomatically for feature extraction.2.7.2 Dynamic Feature Extraction ToolsFor dynamic feature extraction, the application needs to be executed either on anactual device or an emulated environment. Cuckoodroid [101] is one of such tools forautomatic dynamic analysis of Android apps. It executes an Android application ona sandboxed environment and monitors its dynamic behavior. However, in our work,we implement our own dynamic analysis system for greater flexibility. We executedthe apps on an emulator [102] and logged the system calls while random events weregenerated using the Monkey Tool [103] during the execution.2.8 Performance Metrics for Malware Detection Sys-temsSeveral performance metrics are used for machine learning-based malware detectionsystems. Below we describe the most widely used performance measures along withtheir formulae. In the following equations, TP is the number of true positives, i.e.,malware accurately predicted; FP is the number of false positives, i.e., benign appsthat were predicted as malware; TN is the number of true negatives, i.e., benign appsaccurately predicted and FN is the number of false negatives, i.e., malware that werepredicted as benign apps.26Accuracy: Accuracy is the measure of how accurately a classifier can detect malwareand benign applications:Accuracy =TP + TNTP + FP + TN + FN× 100% (2.9)Precision: Precision indicates the percentage of actual malware among the appsthat are classified as malware:Precision =TPTP + FP× 100% (2.10)Recall: It indicates the percentage of malware that were correctly classified. It isalso known as sensitivity or true positive rate:Recall =TPTP + FN× 100% (2.11)Specificity: It is also known as true negative rate. It indicates the portion of actualbenign apps that are correctly identified.Specificity =TNTN + FP× 100% (2.12)G-mean: The geometric mean (G-mean) is the square root of the product of class-wise sensitivity. This measure tries to maximize the accuracy of each of the classeswhile maintaining a balance.G-mean =√specificity × sensitivity (2.13)F-score: F-score is a measure of accuracy that tries to balance the precision andrecall by calculating their harmonic mean.F1-score = 2.P recision×RecallPrecision+Recall(2.14)ROC curve and AUC: Receiver Operating Characteristic (ROC) curve and AreaUnder the Curve (AUC) are two of the most commonly used performance measures for27True Positive RateFalse Positive RateAUC = 0.80 11Figure 2.8 Example of an ROC curve and AUC.machine learning models [104–106]. These two measures give a visual description ofthe model’s performance. Figure 2.8 shows an example of an ROC curve and AUC.ROC is a probability curve that illustrates the classifier’s capability to distinguishbetween the samples of different classes while AUC is a probability measure thatrepresents the area under the ROC curve. For example, an AUC value of 0.8 meansthat the area under the ROC curve is 0.8 and there is an 80% probability that themodel will be able to distinguish between benign and malware samples.2.9 Mobile Malware Detection Systems Based on Fea-ture CategoriesThe analysis techniques of machine learning-based malware detection can be classi-fied into two types: static and dynamic, dealing with static and dynamic featuresrespectively [107]. The static features can further be divided into three categorieswhich will be described in Sec. 2.9.1. There is another approach known as hybridanalysis that combines static and dynamic analysis techniques. Table 2.2 summarizesthe properties, advantages, and disadvantages of these analysis techniques. Below we28Table 2.2 

Summary of Android malware detection techniques based on fea-ture types.Analysis technique Properties Advantage DisadvantageStatic analysis Requires todisassemble anddecode the files fromthe applicationpackage. The decodedfiles are then parsed toextract static features.Does not requirethe application tobe executed. It iscomputationallyless expensive andrelatively faster.Vulnerable to severalattacks includingcode obfuscation anddynamic codeloading.Dynamic analysis Requires anapplication to beexecuted in a real oremulated environment.Different events aregenerated to emulaterun-time behaviorwhich is dynamicallylogged for extractingthe features.Suitable fordetecting attackssuch as codeobfuscation anddynamic codeloading.Requires theapplication to beexecuted. Sometimescan fail to detectdynamic attacks dueto the correspondingevents not beinggenerated during theanalysis phase.Hybrid analysis Combines the featuresextracted from staticand dynamic analysisin an intelligentmanner.Can cover a widerange of attacks inboth static anddynamicsituations.The process iscomplex and it is themostcomputationallyexpensive analysismethod.present the most notable works in the literature based on these techniques.2.9.1 Static AnalysisStatic analysis methods find malicious activities in an application by analyzing theapplication binary and code without actually running the application [108–110]. Forstatic analysis, the Android Application Package (APK) file (described in Sec. 2.2) isdisassembled and its different components are parsed to extract the features. Staticfeatures of an Android application can be divided into three categories: permission,Application Programming Interface (API) calls, and Inter Component Communica-tion (ICC) features. These features are extracted from the androidmanifest.xml andclasses.dex files included in the APK package.Different works have considered different categories of features to develop static mal-29ware detection systems. In [10] Aiman et al. introduced a basic categorization systemfor Android applications. The work extracted the permission of the applications andstudied two categories of application: business and tools. The work classified theapplications by applying the k-means clustering algorithm and achieved 71% recallrate.In [11] Suleiman et al. proposed a Bayesian classifier based mobile malware detectionsystem. The work extracted permission features from a dataset of 2000 samples andachieved an overall accuracy of 93%. Veelasha et al [12] proposed a technique that,in addition to the requested permissions in the manifest file, analyzed the applicationcode to extract the permissions that were actually used in the code. Applying adata mining approach, the work found contrasting properties between the patternsof requested and used permission. The work concludes that these contrasts amongbenign and malware applications can be useful for malware detection. However, themajor drawback of the above approaches is that they only considered the permissionfeature category. These methods were effective for the early generation of malwareapps since they often tried to gain over-privilege by requesting more permissionsthan necessary. However, later malware applications came with smarter techniquesto conduct malicious activities. As a result, later approaches additionally considereddifferent other feature categories.Li and Li in [13] proposed a static analysis method that focused on the Android codeorganization. It first parsed the classes.dex file to extract the API calls and afterwardbuilt a characteristic tree based on the usage and calls of those APIs. It devised anovel method for analyzing similarities among these trees and proposed that it couldbe useful for malware classification since malware samples have similar characteristics.The work, however, did not report its performance in an actual malware detectionscenario.Drebin [14] is another static analysis method that included a wide range of static30features for malware detection. The work categorized the static features extractedfrom an application into eight sets: hardware features, requested permissions, appcomponents, filtered intents, restricted API calls, used permissions, suspicious APIcalls, and network addresses. Using an SVM classifier, the work achieved an overallaccuracy of 94%. Drebin is one of the most prominent works in the literature thatextensively considered a broad range of static features including permission, API calls,and ICC. However, since it did not consider the dynamic behavior, it is vulnerable tocode obfuscation and dynamic code loading attacks [111].In addition to extracting the API calls, further information about the behavior ofan application can be obtained by analyzing the context of an API call. Androidsystems rely heavily on call back methods. These call back methods are related to theevents that cause an API function to be invoked. For example, an API method (e.g.,sendSMS()) could be called when a user clicks on a button. In this case, a call backmethod such as “onClickListener" will be tied to the button click event and sendingSMS will be handled by this callback method. On the other hand, in a potentialmalicious case, the same API could be called based on some system-generated eventsuch as a change in the signal strength. In this case, the SMS will be sent in a differentcontext and handled by a different callback method (e.g., a broadcast receiver). Inthe above scenario, the user is aware of the first call but unaware of the second. Theseare known as contextual information and a detailed 



Discussion on this is presentedin Sec. 3.3. A few works in the literature considered this contextual information formalware detection.AppContext [112] proposed a method that analyzed the call graph of an applicationto trace back the entry point of the API calls. These entry points are related tothe activation events of an API call. It also extracted the associated environmentalinformation such as if a database query is made or if the system time is obtainedbefore an API call. All this information was encoded to construct a feature vector. Itthen used an SVM based classifier to identify maliciousness of an API call. Though31the work reported a 95% recall rate, the dataset used in the work is fairly small (only202 malicious apps) and the method is very expensive computationally. Moreover,the work poses a 5MB file size constraint making it very unlikely to be applicable inreal-world 



Scenarios.Narayanan et al. [113] introduced a graph similarity-based approach for malwaredetection based on API and contextual information. The work first obtained the callgraph of an application where each node in the graph represents an API call. Itthen traced the activation events of these API calls which were used to determine thecontext, i.e., whether the user was aware or unaware of the call. Each node of thegraph is then re-labeled based on this contextual information. A contextual graphkernel is then computed from this labeled graph and unknown samples are classifiedaccording to the similarity of the graphs. The work was further improved in [114]that considered multiple views of the graph in addition to the API dependency graphsuch as, permission dependency graph and information source-sink graph. The laterwork reported an overall accuracy of 98%. One major difference of their works fromothers is that they considered the structural information of the application. However,capturing all the structural information is an overwhelmingly difficult task due to theheavy reliance on call-back methods and execution jumps of Android applications.Moreover, the execution structure can easily be changed for applications by simplyaltering the execution order, or using code obfuscation and dynamic code loadingtechniques which will render this type of detection method ineffective.The third category of static features is known as Inter Component Communication(ICC) features. In Sec. 2.2 we described the different components of an Android ap-plication. ICC mechanism allows these components to communicate with each otherwhich is known as Inter Component Communication. The communicating compo-nents can be from the same or different application(s). Android operating systemfacilitates this ICC communication to enable the reuse of codes so that the function-ality of a component can be used and shared by others. This reduces the burden from32the application developers [115]. However, researchers have found that malware mayabuse this ICC mechanism to evade detection and to conduct malicious attacks (e.g.,collusion attacks) [116–119]. Different tools like Amandroid [120], Epicc [121], andIC3 [122] have been developed to extract the communicating components (sourcesand sinks) using ICC mechanism within an Android application. There are also toolslike IccTA [123] that identify taints of ICC leaks by constructing precise control flowgraphs.ICCDetector [15] is a technique that proposed detection of malicious applications us-ing the ICC features. It designed a parser on top of Epicc [121] to extract ICC relatedfeatures such as component, explicit intent, implicit intent, and intent filters. Themethod used an SVM to classify malicious behaviors and obtained 88.8% accuracy.Feizollah et. al. in [124] considered “Android intents" (explicit and implicit) as ICCfeatures and combined it with required permissions. They argued that the “Androidintents" are semantically significant features that could be used to reveal hidden ma-licious behaviors more effectively when compared to other features such as permis-sions. Using the Bayesian Network algorithm the work evaluated different detectionapproaches that considered the features separately and in combination and achieved91% accuracy.The methods considering ICC features are effective in revealing particular maliciousintentions such as information leaks and collusion attacks. However, they also addredundancy in the feature space and adversely affect the classifier since all the com-municating components are not related to security threats. Hence, we see that theapproaches in [15, 124] are outperformed when other features are considered in com-bination with ICC.332.9.2 Dynamic AnalysisDynamic analysis requires the application to be executed in a suitable environmentsuch as an actual device or an emulator [125–131]. During the execution of an appli-cation, the dynamic behavior of the application is monitored, logged and analyzed.Dynamic analysis can be used to detect attacks involving code obfuscation and dy-namic code loading.Different features are considered as dynamic within an Android system including An-droid system calls, network communication properties, and run-time usage of systemcomponents. Among these, system calls are the most commonly used features indynamic analysis [132–134]. Every application requests resources and services fromthe Android operating system using system calls. For example, opening, reading orwriting files, reading connection status, etc. There are more than 250 system callsavailable in Android operating system from Linux kernel [135]. Below we present theworks in the literature that adopted the dynamic analysis approach.Xiao et al. [132] proposed a technique for malware detection based on the sequencesof system calls. The work trains two deep neural networks one for the maliciousclass and one for the benign application class. When a new application is tested, twosimilarity score is computed from these two networks and the application is classifiedbased on this score. The work obtained 93.7% accuracy.Deep et al. [136] proposed an Android malware classification system that extractedsystem calls as features. It computed the value of a particular feature based on itsTerm Frequency-Inverse Document Frequency (TF-IDF) value [137]. In this method,the system calls that occur more commonly in a particular class get higher valuesand the ones that occur rarely get lower values. It also incorporated a Rough Set andStatistical Test-based feature selection and was able to reduce the false positive rateto 1% using Random Forest as the classifier.Abderrahmane et al. [138] proposed a system call based Android malware detection34system where each application is analyzed in a remote server. The server installsand executes the application in a simulated environment and the system calls of theapplication are logged during this execution time. The work considers system callsfrom the Linux kernel and represents each of the applications by a 250× 250 matrix.Each entry in the matrix represents the dependency of a pair of system calls basedon their distance on the execution sequence. The method reported 93.29% accuracyusing a deep neural network-based classifierAfonso et al. [133] proposed a dynamic malware detection system that logs thefrequency of Android system calls to detect malicious behavior. The work evaluatedthe performance of the system using different classifiers including Random Forest,Naive Bayes, and Decision Tree, and was able to achieve 96.82% accuracy for malwaredetection. The work however is limited to a certain version of API in Android system.Crowdroid [135] was proposed by Burguera et al. that classifies malicious applicationsbased on dynamic behavior. It first installs an analyzing application in the devicewhich then attaches a tracing process (strace) to an application to log the systemcalls. This log is sent to a remote server where K-means clustering algorithm is usedfor classification. One major drawback of this system is that it requires the user toinstall an additional application and also it is highly dependent on the number ofusers using this application.Marko et al. [134] developed the Maline system that extracted the system call se-quence of an Android application. It recorded the frequency of each system call andalso formed a dependency graph among them based on their paired distance in a se-quence. The work experimented with different classifiers including SVM and RandomForest and achieved an overall accuracy of 96% on Drebin dataset.Bhatia and Kaushal [139] proposed a system that also used system calls for malwareclassification. The work applied Decision Tree and Random Forest as classifiers andobtained 88% accuracy. However, the dataset considered in the work is very small35(only 50 malicious applications) and hence, the applicability of the system in real



Scenarios needs to be evaluated.Even though dynamic analysis is necessary for uncovering certain attacks such asdynamic code loading and code obfuscation, the methods proposed here has somemajor drawbacks. To reduce the risk of infecting an actual device, extraction ofdynamic features is performed within a sandbox or an emulated environment. Thereare malware applications that can detect these execution environments and can evadedetection by hiding their malicious functionalities during the analysis phase. Also,during the automated execution process, random events are generated to exploredifferent functionalities, execution sequences, and dynamic behavior of an application.It is not always possible to exhaustively generate all the events so that each and everyexecution sequence of an application is covered.2.9.3 Hybrid ApproachStatic analysis reveals a lot of information about an application including over-privilege, security-sensitive operations, and information leaks. However, it fails todetect code modification and intrusion of virus. Static analysis cannot capture dy-namic loading methods because the corresponding code remains encrypted which isdecrypted and fetched in the memory during execution. This characteristic can alsobe used by malware developers to evade static detection. Dynamic analysis, on theother hand, can address these obfuscations, repackaging, and encryption attemptssince they are exposed during the execution time. However, dynamic analysis has itsdrawback which is mentioned in the earlier 



Section. As a result, malware researchershave opted for hybrid methods that combine both static and dynamic analysis tech-niques for a comprehensive analysis of an application.Jang et al. [140] devised a hybrid approach considering both malware-centric andcreator-centric information. As features, it extracts certificate serial number, permis-36Table 2.3 

Summary of key works in mobile malware detectionWork FeaturetypesContribution Dataset Performance 

LimitationsYerima et al.[11]Static (Per-mission)Proposed a bayesianclassifier-based system thatutilized permission featuresfor malware detection.Malgenome Accuracy = 93%Only considerspermissionfeatures.Vulnerable tonew generationof malware.Samra et al.[10]Static (Per-mission)Proposed a basiccategorization system ofAndroid application basedon k-means clustering andpermission features.Personal Recall = 0.71Ganesh et al.[90]Static (Per-mission)Designed a system thatsends data to a remoteserver where it istransformed into an imageformat. Uses aConvolutional NeuralNetwork (CNN) forclassification.Personal Accuracy = 93% Classificationcan be misledduring datatrasmission.Karbab et al.[94]Static(API)Designed MalDozer systemthat identifies malwarefamilies by analyzing thesequence of API calls.Drebin Precision = 0.96Recall = 0.96F1-score = 0.96Vulnerable tocodeobfuscation anddynamic codeloading attacks.Li et al. [91] Static(API)Utilized a weight-adjusteddeep learning technique formalware detection.Drebin Recall = 0.94Hou et al. [142] Static(API)Developed AutoDroid thatextracts APIs from smalicodes and uses a DeepBelief Network for malwaredetection.Personal Accuracy =96.66%Zhang et al.[93]Static(API)PresentedDeepClassifyDroid thatextracted five differentfeature sets through staticanalysis and used CNN formalware detection.Personal Accuracy =97.4%Precision = 0.97Recall = 0.98F1-score = 0.97Nix and Zhang[143]Static(API)Extracted API call sequenceand used a CNN forlearning featurerepresentations which arethen used for classification.Contagio Accuracy =99.4%Precision = 1.00Recall = 0.98Narayanan etal.[114]Static(Context)Utilizes graph similaritythat matches thedependency graph of anapplication with that ofbenign and malicioussamples.Personal Accuracy = 98% Consideredcontext inisolation.Cannot becombined withother featurecategoriesYang et al.[112]Static(Context)Devised a malwaredetection technique thatextracted environmentinformation associated withsensitive API calls.Personal Accuracy =94.8%Considereddataset is verysmall. 5MB appsize constraintlimits itsapplicability.Xu et al. [15] Static(ICC)Proposed a technique todetect collusion attacksamong maliciousapplications by utilizingICC features.Drebin Accuracy = 97.4 Only considersICC features.Vulnerable to arange of otherattacks.Table 2.3 

Summary of key works cont.Work FeaturetypesContribution Dataset Performance 

LimitationsSu et al. [54] Static (Per-mission,API)Utilizes a deep beliefnetwork for malwaredetection using permissionand API call features.Drebin Accuracy = 96% Vulnerable tocodeobfuscation anddynamic codeloading attacks.Li et al. [95] Static (Per-mission,API)Applies a deep neuralnetwork-based system onrequired permission andAPI calls for malwaredetection.Drebin Pecision = 0.97Recall = 0.94F1-score = 0.96Considers avery smallfeature set.Wang et al.[144]Static (Per-mission,API)Uses a Deep Autoencoder(DAE) to extract featuresand a convolutional neuralnetwork for malwaredetection.Personal Accuracy =99.8%Recall 0.99F1-score = 0.99Depends on theDAE’s ablity toextractmeaningfulfeatures.Arp et al. [14] Static (Per-mission,API, ICC)Introduced an SVM basedmalware detection systemthat considered a widerange of static featurecategories includingpermission, API calls, andICC features.Drebin Accuracy = 94% Constructs avery sparsefeature spacethan can lead toover-fitting.Jiang et al.[145]Static (Per-mission,API, ICC)Introduced a fine-graineddangerous permissionfeature-based detectionsystem that identifies themost dangerous featuresand uses them for malwaredetection.Personal F1-score = 0.94Recall = 0.94Vulnerable todynamicloading andcodeobfuscationattacks.Dimjašević etal.[134]Dynamic Proposed a malwaredetection system based onthe frequency of systemcalls and their dependency.Drebin Accuracy = 96%Exhaustivecoverage ofexecutionsequences is notfeasible.Martinelli et al.[97]Dynamic Utilizes a ConvolutionalNeural Network on systemcall sequences ofapplications for malwaredetection.Drebin Accuracy = 95%Liang et al.[146]Dynamic Considers system callsequences as text anddetects malware byextracting themes from thetext.Personal Accuracy =93.1%Precision = 0.96F1-score = 0.87Vulnerable tomalware attacksthat cansuppressactivities inemulatedenvironments.Yuan et al.[147]Hybrid Proposed a hybrid featurebased malware detectionsystem that utilizedpermission, sensitive APIcalls, and dynamicbehaviors.Malgenome Accuracy = 96.8 Uses coarsegrainedfeatures.Yuan et al. [55] Hybrid Introduced a deep beliefnetwork-based hybridapproach for Androidmalware detection.Contagio Accuracy = 96.5 Considers avery smallfeature set.Alshahrani etal. [98]Hybrid Developed the DDefendersystem that utilizes datafrom a client-sideapplication for malwareclassification.Drebin Accuracy = 95% Requiresadditionalapplication tobe installed.38Table 2.3 

Summary of key works cont.Work FeaturetypesContribution Dataset Performance 

LimitationsVinayakumar etal. [148]Hybrid Proposed the use of aLong-Short TermMemory-based detectionsystem for Android malwaredetection.Personal Accuracy =97.5%Suffers fromlong trainingandclassificationtime.sions, API sequence, and the combination of system commands and forged files. Forclassification, it computes the similarity score based on these features. It then matchesthis score against a pre-computed score of benign and malicious behavior. The workreduced the false-negative rate to 1.77%. However, there are a few drawbacks of suchapproaches. Since it uses the serial number as a feature, the detection system canpotentially be broken by obtaining a new serial number. Also, malware creators canalter the sequence of API calls to bypass the detection while being capable of carryingout malicious behavior [141].Tong and Yan [149] adopted another hybrid approach that dynamically collected thedata and statically analyzed them. The work first created a database for maliciouspatterns and benign patterns by calculating the frequency and weight of sequentialsystem calls at different depths based on a prefixed threshold. Then it inspected thesystem call sequences of the unknown app and matched them with a known signaturedatabase. Decision was made based on whether the sequence matched the maliciouspattern or benign pattern. Kapratwar [150] designed a hybrid method taking thepermissions as static feature and system calls as dynamic feature. The work studieddifferent machine learning approaches including Decision Tree, Random Forest, andNaive Bayes.The above approaches evidenced that considering a hybrid of static and dynamic fea-tures is more effective in malware detection rather than adopting a static or dynamicapproach on its own. However, these methods are evaluated in a small scale environ-ment considering a minimal subset of the features and they are largely dependent onthe number of applications that are already analyzed to build their database.39In table 2.3 we provide a 

Summary of the most notable works in the literature de-tailing their characteristics, key contributions, and performance. This concludes the



Literature Review pertaining to our first research objective. In the following, we reviewan evasive attack technique, known as adversarial attack, which breaks classificationsystems and challenges their robustness.2.10 Adversarial AttackMachine learning models can detect malicious applications with state-of-the-art per-formance if an optimal combination of feature categories is provided. However, recentliterature shows than such detection systems can be evaded with attacks using ad-versarial samples. Even though adversarial samples were first introduced to attackmachine learning classifiers in the image processing domain, similar principles canbe applied to evade malicious application detection systems as well. In this 



Section,we present an 

Overview of the adversarial attack and defense against such attacksproposed in the current literature.2.10.1 Adversarial SamplesAdversarial samples are artificial samples that are crafted by introducing small per-turbations to the original samples. Szegedy et al. [151] first discovered that severalmachine learning models, including neural networks, are vulnerable to adversarialsamples. The authors showed that carefully introducing perturbations into an origi-nal sample can mislead a machine learning classifier into misclassifying that sample.This careful 



Introduction of perturbation is different in several aspects from addingrandom noise into the dataset which is traditionally used to make a machine learn-ing model robust. Firstly, the perturbation is not introduced randomly rather itis carefully calculated. Secondly, the purpose of introducing this perturbation is tospecifically mislead the classifier unlike random noises that may or may not affect the40classifier performance. And thirdly, the perturbation is kept at a minimum so thatthe crafted sample and the original are not easily distinguishable. Following the workin [151], several other methods to craft such adversarial samples have been proposedin the literature. Some of the notable works in this regard are described below.2.10.2 Adversarial Sample Crafting Techniques and DefenseMechanismsMost of the approaches to craft adversarial samples suggest to minimize the distancebetween the adversarial sample and the sample to be modified, i.e., minimize theperturbation required to shift the classification to an incorrect class [152]. Szegedy etal. [151] introduced a method to craft adversarial samples by minimizing the followingloss functionloss(f̂(x+ r), ŷ) + c.|r| (2.15)where x is the original sample, ŷ is the desired outcome, r is the perturbation added tothe sample and c is a parameter used to balance the distance between the original sam-ple and the perturbation. The author proposed to solve this using a box-constrainedL-BFGS [153]. It essentially tries to find the minimal amount of perturbation (r)needed to misclassify the sample x to class ŷ rather than to its original class.Goodfellow et. al proposed a fast gradient sign method in [154] that uses gradientsof the output with respect to the input features for adversarial sample crafting. Themethod adds a small value, ε, to each of the features of a sample so that the sampleis misclassified. The method solves the following equation for adversarial samplecrafting.x̂ = x+ ε · sign(∇x(loss)) (2.16)41Equation (2.16) computes the gradient of the loss function with respect to the input.However, only sign (+ve or -ve) of the gradient is used to determine whether to addor subtract the perturbation. When the gradient is positive it adds a small value tothe features and when the gradient is negative the value is subtracted. This allowsthe method to add values in the direction of the gradient, i.e., and increase the lossfunction value so that a sample is misclassified.Su et. al [155] proposed a technique that modified only a single feature for adversarialsample crafting. Similar to other adversarial samples crafting methods, it tries tokeep the adversarial sample as close to the original sample as possible. However,unlike other methods that try to find minimal perturbation, this method limits theperturbation to a single feature. Nevertheless, different properties of a single featurecan be modified. For example, in case of an image classification task with pixels asfeatures, the x- and y-coordinate, red, green, and blue (RGB) channels of a singlepixel is modified. It uses a genetic algorithm-based approach that starts with a set ofrandomly formed candidate samples and progresses in an iterative fashion. In eachiteration, it forms a new set of candidate solutions by modifying the current set ofsamples. The process stops when a candidate sample is successful to be misclassified.Additionally, some works studied the physical aspect of adversarial samples. Forexample, Brown et. al [156] developed a technique that creates printable adversarialpatches that can be stuck beside an actual image to fool a classifier. Athalye et. al[157] proposed adversarial samples crafting techniques for 2D and 3D classifiers. Itshowed an example where a turtle was 3D printed and was able to be misclassified asa riffle.There are very limited works in the literature that have reported adversarial examplecrafting techniques and their impact on malware detection systems, while even lessattention has been focused on adversarial attacks in mobile malware domain. Mostof the earlier works in this domain proposed techniques only for crafting the adver-42sarial samples to fool a malware detection system. A few of the later works studiedthe defense strategies against such samples. Most notable works in this regard arepresented below.Anderson et al. [158] adopted a reinforcement learning-based approach for craftingadversarial examples. It modified a malicious sample in an iterative fashion to bypassmalware detection systems. In each iteration it modifies the malicious sample byperforming a single action from a set of acceptable actions that includes adding afunction, manipulating existing 



Section name, and modifying header checksum. Thisiterative process continues until the sample is misclassified by the target model. Thework achieved 16% evasion rate on a model which is trained for 100,000 round onoriginal data. The work did not devise a defense mechanism however, it proposedthat a model should be retrained on the adversarial samples to make it robust againstadversarial attacks.Dang et al. [159] proposed an adversarial malware sample crafting technique usingthree tools namely, a detector, a morpher, and a tester. The detector is the classifica-tion system that the attacker wishes to evade, the morpher is a tool that modifies amalicious sample and tester is a tool that checks whether a modified sample still con-tains the malicious functionality or not. A malicious sample is considered a successfuladversarial example if the tester verifies it as a malicious sample but the detector ac-cepts it as a benign sample. The method carries out a series of morphing operationsto obtain several modified copies of a malware sample. Afterward, it adopted a hill-climbing approach where the number of morphing steps was leveraged as a scoringmechanism to find the closest sample that is able to evade the detection. The workreported a 100% evasion rate by the adversarial malware samples crafted with thistechnique. However, it did not propose any defense strategy.Yang et al. [160] introduced a Malware Recomposition Variation (MRV) approachbased on the semantic analysis of various existing malware applications for crafting43adversarial malware samples. Through phylogenetic analysis, the work observed howdifferent features of malicious applications evolve over different versions. To craftadversarial malware samples, it used a semantic-feature mutation technique that au-tomatically mutated the malware bytecode to inject malicious behaviors into an ap-plication component. However, the method proposed in this work requires access tothe malware binaries and source code. Further, it also requires a specific componentof an Android application, i.e., broadcast receiver, which can easily be detected sinceit is a common characteristic of a malicious application to invoke sensitive APIs frombroadcast receivers.Grosse et al. [161] studied adversarial example crafting specifically for Android mal-ware detection. 63% of the adversarial samples crafted in the work were able tosuccessfully bypass the malware detection system. The work also explored adversar-ial training as a defense mechanism against adversarial attacks where adversariallycrafted malware samples are used to retrain a classifier to make it robust. It showedpromising 

Results against adversarial attacks where several architectures of deep neu-ral networks along with varying ratios of malware and benign samples were considered.However, retraining a classifier with too many adversarial samples can lead to over-fitting and degrade the performance and the work did not consider how to optimallyselect these samples for retraining.Unlike adversarial retraining, Papernot et al. [162] proposed a different defense mech-anism based on distillation. In this method, a classification model M is first trainedusing the original data and the probability of the samples belonging to a certain classis recorded. A second classification model M ′ is then trained using these probabili-ties as the label. However, this mechanism is not as effective in malware detectionas in other domains such as image processing and computer vision. Other defensesagainst adversarial attacks include classifying adversarial samples as a separate classand differentiating them based on their statistical property [163, 164].44Table 2.4 

Summary of key research and their main contributions on adver-sarial attacks in mobile malware detection.Work Contribution CommentsAnderson et al. [158] Utilizes a reinforcementlearning-based approach thatiteratively modifies a malwaresample to craft adversarialexamples.It achieves a 16%evasion rate. Does notsuggest any defensemachanism.Dang et al. [159] Utilizes a detector, a morpher,and a tester to craft adversarialmalware samples.Achieves a 100%evasion rate. Doestnot propose anydefense technique.Yang et al. [160] Adopts Malware RecompositionVariation (MRV) approachbased on the semantic analysisof malware samples. Injectsmalicious byte codes intoapplications to craft adversarialsamples.Requires access tomalware source code.Grosse et al. [161] Uses saliency map-basedadversarial sample craftingtechnique and achieves 63%evasion rate. Proposesadversarial retraining as adefense mechanism.Sub-optimalperformance due torandom choice ofsamples.The effectiveness of adversarial malware attack and its defense in mobile malware de-tection settings is a relatively unexplored area unlike image processing and computervision domain. Furthermore, intelligent selection of samples has shown improved per-formance in different areas of machine learning applications [165, 166]. However, suchselective sampling is yet to be applied for adversarial retraining in mobile malwaredetection domain. In our second research objective, we address this issue and improvethe robustness of malware detection systems against adversarial attacks using a novelselective sampling strategy.The following 



Section presents the 



Literature Review for our final research objectivethat deals with imbalanced data distribution.452.11 Imbalanced Data Problem for Mobile MalwareDetectionThe machine learning models descried earlier are designed to expect a balanced datadistribution, i.e., it expects that the number of benign and malware samples arenearly equal. If the dataset is imbalanced, the performance of such models can sufferto a great extent. A dataset is considered imbalanced if the number of samples inone class is significantly higher than that of another. This characteristic of datadistribution occurs naturally in malware detection systems since in the real world thebenign applications outnumber malicious applications to a great extent. In fact, Xuet al. [16] reported a class imbalance in the order of 2 in 1000 in mobile malwaredetection. Even though various fields of research including telecommunication, textclassification, and image processing have tried to deal with data imbalance [167–175],the problem becomes highly critical in the field of malware detection because the costof incorrectly classifying a malware (i.e. false negative) is significantly more thanthat of a benign application. Misclassifying a malicious application could result ina harmful application being launched in the market potentially leading to privacyleaks, information stealing, and financial loss. Existing methods in the literaturefor handling imbalanced data problems can be divided into three categories namely,data-level approach, algorithm-level approach, and hybrid approach [176]. Table 2.5summarizes the properties, advantage, and disadvantages of these approaches. In thefollowing, we present the most notable works in the literature.2.11.1 Data Level ApproachesData-level approaches or data augmentation is one of the most common approachesto handle imbalanced data [16, 169, 177–182]. This approach reduces data imbalanceby changing the distribution of the training data. Traditional approaches of dataaugmentation include over-sampling [17, 183] and under-sampling [174, 184] which46Table 2.5 Imbalanced data handling approaches 

Overview.Approach Properties Advantage DisadvantageData-level methods Tries to balance thedataset by changingthe distribution of thedata, e.g., removingmajority class samplesor creating syntheticminority classsamples.Does not depend on aparticular algorithm.Simpler to implement.Often lead to poorgeneralization,information loss, andmay generate invalidsamples.Algorithm-level methods Modifies an underlyingalgorithm to handleimbalanced data, e.g.,modified loss function.Relative importanceon majority andminority classes canbe adjusted.Solution is algorithmspecific.Hybrid methods Strategically combinesboth data-level andalgorithm-levelmethods.Generates balanceddata distribution andhas the capability ofassigning relativeimportance to eachclasses. Can betailored to be used invarious domains.The process iscomplex andcomputationallyexpensive.in their simplest form refer to simply copying random minority class samples anddiscarding random majority class samples, respectively [182]. However, these simpletechniques have major drawbacks. In the case of random over-sampling, since merecopies of minority class samples are created, the classifier often gets redundant infor-mation. This leads to over-fitting and poor generalization, i.e., the classifier doesn’tperform well on unseen test data. Random under-sampling on the other hand, dis-cards samples from the majority class that lead to information loss, i.e., the samplesthat are critical to learn the characteristics of the data often get removed from thetraining data. To mitigate these problems researchers have opted for devising in-telligent over-sampling and under-sampling methods to minimize redundancy andinformation loss.SMOTE (Synthetic Minority Over-sampling Technique) [17] proposed by Chawla etal. is one of the most notable intelligent over-sampling techniques in this regard.Rather than simply creating copies, it generates artificial samples for the minorityclass. Figure 2.9 illustrates how SMOTE algorithm works. For each minority class47sample, it selects a number of nearest samples from the same class using K-NN rule.Afterward, it randomly interpolates the features between the neighboring samplesto create artificial samples. In Fig. 2.9, green and red dots represent majority andminority samples respectively. The yellow dots are the interpolated samples betweenneighboring minority samples.Several variants of the SMOTE algorithm were proposed to further improve its perfor-mance [185, 186]. Before creating artificial samples these works examined the neigh-bors from the majority class and prevented over-sampling in overlapping regions tominimize generation of noisy data.Jo and Japkowicz [187] proposed another intelligent over-sampling technique thatconsidered multiple clusters of the minority class. The work first clustered the samplesand then performed over-sampling within each cluster to improve both intra- andinter-class imbalance.These methods however are only suitable for continuous features. Also, since they donot pay any particular attention to the characteristics of the features, an immutablefeature can be modified by these methods resulting in invalid artificial sample creation.Lee et al. [188] proposed a two-phase learning approach to classify highly-imbalanceddatasets. In the first phase, the proposed method down-sampled the majority classuntil it has at most 5000 samples. This down-sampled dataset is used to pre-traina classifier which is then fine-tuned using the complete dataset in the second phase.The work reported improved minority class performance, however, major drawbacksin this work are determining the creation process of the down-sampled dataset andfinding the optimal choice of the minimum number of samples.Mani and Zhang [189] proposed a k-nearest neighbor (K-NN) based under-samplingapproach. It devised multiple methods that removed a majority class sample fromthe training data based on its distance from the minority samples. For example,their NearMiss-1 method calculated the average distance of a majority sample from48Figure 2.9 SMOTE algorithm for handling imbalanced datasets. Red andgreen dots represent minority and majority classes respectively and yellowdots are the interpolated samples.three nearest minority samples and only kept the samples that have the smallestdistance. NearMiss-2 on the other hand calculated the average distance of a majoritysample from three farthest minority samples. The work presented a comparativestudy among these methods. However, the choice of different parameters such as,how many minority samples to use for distance calculation is a major drawback inthis work.Additionally, Barandela et al. [190] proposed a technique that removes majoritysamples from the class boundaries. Using K-NN rule based on Wilson’s editing [191]the work removes the samples that are misclassified from the training data. Kubatet al. [184] proposed a technique known as one-sided-selection that carefully selectsa representative subset of the majority class by removing noise, redundancy andborderline samples that 

Results in a more balanced training set.2.11.2 Algorithm Level ApproachesIn algorithm-level methods, the learning model is modified so that the bias imposed onthe classifier due to the prior probability of imbalanced data is mitigated. This is mostcommonly achieved by designing a loss function that penalizes the misclassificationof minority class samples more than that of the majority class.49Wang et al. [192] showed that the traditional Mean Squared Error (MSE) poorlycaptures the classification performance when high data imbalance exists. The workintroduced a new loss function, Mean False Error (MFE), by combining two com-ponents of MSE, i.e., Mean False Positive Error (MFPE) and Mean False NegativeError (MFNE). The method showed improved performance as compared to traditionalMSE loss. However, the datasets considered were fairly small and the improvementis highly problem-specific [193].Lin et al. [194] proposed Focal Loss (FL) function that modifies the traditionalcross-entropy (CE) loss function. The work multiplied the CE loss function witha modulating factor. This factor downgrades the easily classified samples to reducetheir effect on the classifier. Concurrently, it assigns more importance to the minorityclass samples to improve classification performance. This method is also transferableto hard sample problems, however, in [195], the traditional CE loss performed betterin some datasets suggesting that the performance improvement by this method isspecific to particular problem domains.Wang et al. [196] introduced a cost-sensitive deep learning technique to handle imbal-anced data problem. It modified the traditional CE loss function and a pre-definedcost matrix was incorporated with it. The cost matrix forces the loss function to paymore attention to correctly classify minority class samples. The work implemented acost scheme that specified the false negative error to be 2× the cost of false positiveerror. However, optimally computing the cost matrix is a very complex and timeconsuming task that can be impractical for large datasets.Zhang et al. [197] introduced a deep representation learning-based technique forimbalanced datasets. The Category Center (CC) method proposed in the work incor-porates transfer learning, deep feature extraction, and nearest neighbor classification.It first trains a deep neural network (DNN) on a balanced dataset. The output fromthe penultimate layer of this network is used as a deep feature representation of the50samples which are then used for clustering the samples. The same network is usedfor extracting deep features from the imbalanced dataset. This is used to computethe distance of the samples from the pre-computed cluster centers and the samplesare classified accordingly. The limitation of this work is that it greatly depends onthe DNN’s capability to extract discriminating features and requires a large balancedrepresentative dataset.Ding et al. [198] studied the possibility of using very deep neural networks for im-balanced data. The work studied the performance of deeper networks with 16 to 50hidden layers against shallower 6 to 10 hidden layer networks. The work suggestedthat additional hidden layers can make it easier to locate local minimum and accel-erate convergence. However, adding hidden layers exponentially increases computingand time complexity that 

Results in impractical solutions.2.11.3 Hybrid ApproachesHybrid-methods strategically combines data-level and algorithm-level approaches thatbalance the underlying data distribution and counters classification bias towards themajority class.LMLE [199] was introduces by Huang et al. to learn discriminative deep representa-tions of imbalanced data. The work introduced a combination of quintuplet samplingand triple-header hinge loss function to produce deep representations of the featureswhich preserve same-class locality and increase discrimination between classes. Thisdeep representation provides well-formed clusters that are used by a K-NN basedsystem for classification. The method showed good performance on the highly im-balanced CelebA dataset [200]. However, the technique proposed in the work is bothcomplex and computationally expensive. Moreover, the method requires pre-clustereddata for quintuplet sampling which is difficult to obtain in the absence of suitablefeature extractor.Dong et al. [201] proposed an end-to-end deep learning method for addressing high51class-imbalance. The work introduced a combination of hard-sample mining and classrectification loss and used mean sensitivity scores as the primary performance metric.At the data-level, the proposed method selects samples that are correctly classifiedwith low confidence and misclassified with high confidence. At the algorithm-level, itassigns more weights to the highly imbalanced classes and reduces weights to the lessimbalanced ones. The work, however, does not generalize well to different domainsespecially when the problem consists of low level of class imbalance and fails to provideany performance gain.Data imbalance in mobile malware detection problem has been acknowledged in var-ious works [202, 203]. However, no explicit attempt has been made to deal with theproblem until recently. Xu et al. [16] proposed fuzzy-SMOTE for imbalanced mobilemalware detection which is based on the fuzzy set theory. It generates synthetic ex-amples for the minority class in the fuzzy region where the minority examples havea low membership degree. Classification models pay more attention to the enhancedminority class when trained with these synthetic samples. Consequently, the decisionboundary of the minority class is enlarged and the classification bias towards themajority class is reduced. The work, however, assumes a continuous feature domain,and also, it does not discriminate between mutable and non-mutable features whengenerating synthetic samples that leads to invalid sample generation. Changing anon-mutable feature will result in the application losing certain malware functionali-ties.Oak et al. [204] proposed a technique that utilizes Bidirectional Encoder Represen-tations from Transformers (BERT) [205] for malware classification with imbalanceddata. The proposed method utilizes a monitoring service [206] to record the sequenceof various operations performed by an Android application. The extracted informa-tion is high-level representations, e.g., a file is opened on the device, which are thenencoded as integer values. This 

Results in a dataset where each application is repre-sented by a sequence of integers which is used for classification. However, a major52drawback of the method is that it relies heavily on the operation sequence generatedby the monitoring service. Generating such sequence requires an application to ex-ecute in a controlled environment and its not always possible to exhaustively coverevery possible execution sequence of an application. Further, some malicious appli-cations are intelligent enough to detect such environment and suppress its maliciousfunctionalities.Table 2.6 

Summary of key research and their contributions on mobile mal-ware detection with imbalanced data.Work ApproachtypeContribution 

LimitationsXu et al. [16] Data level Adopts a fuzzy settheory-based approachthat creates syntheticsamples from themalware applicationsthat are in the fuzzyregion.Generates invalidsamples.Oak et al. [204] Data level Utilizes a monitoringservice that recordssequences of operationsperformed by anapplication and analyzesthem for classificationGreatly depends on theefficacy of themonitoring service.Songqing Yue [207] Hybrid Coverts malware binariesto grey scale image.Utilizes a weightedsoftmax function forclassification using CNN.Limited by theconversion process whichmay incur loss ofinformation.Chen et al. [208] AlgorithmlevelUtilizes an imbalanceddata gravitationclassification model thatcomputes gravitation ofa sample using weightedEuclidean distance forclassificationCannot handle high levelof imbalance.Songqing Yue [207] proposed a method where malware binaries are converted into dig-ital grey-scale images. It devised a Convolutional Neural Network (CNN) based clas-sification system that incorporated a weighted softmax loss function for imbalancedmalware classification. In contrast to the traditional softmax loss where misclassifi-53cation of each sample is weighted equally, the work scaled the misclassification costof different classes according to their imbalance ratio. This counters the bias towardsthe majority class and improves the performance of minority class classification. Theperformance of the model, however, can be limited by the conversion process thatcan lead to loss of important characteristics of an application. Also, choosing theappropriate scaling of the loss function is a challenging task in this method.Chen et al. [208] proposed a method that analyzed network traffic with machinelearning models to classify malicious behavior with an imbalanced dataset. The workmonitored the network packets of mobile applications and utilized the statistical prop-erties of mobile traffic to identify malicious applications. However, the work cannothandle high level of imbalance and the performance declines when the level of imbal-ance increases [209].2.12 Open Research ChallengesMobile malware detection has become an attractive research area due to the un-deniable popularity of smartphones and other hand-held devices, and the increasedmalware attacks in these devices. Because of the large number of smartphone ap-plications (over five million [210]), automatic malware detection systems based onmachine learning models have been widely used. However, several key factors affectthe performance, resilience, and robustness of these models, and our extensive reviewof the current literature reveals the following open research challenges.• The existing machine learning-based techniques consider random feature cate-gories when designing their model. Not considering the effect of feature cate-gories in a systematic manner can render a classification system ineffective sincedifferent categories of features represent different behavioral aspects of mobileapplications. For example, many systems consider permission and API callsas feature categories while ignoring the dynamic category. These models are54incapable of identifying obfuscation or dynamic loading-based attacks. On theother hand, a blind combination of these features is also ineffective since addingredundancy can negatively impact model performance. Moreover, additionalinformation about some features (i.e., contextual information of API calls) canprovide further assistance in distinguishing different functional aspects of a mo-bile application. Incorporating such information with the associated featuresand using them in combination with others will significantly improve a model’sperformance. A few prior works have explore this idea, but they only consideredit in isolation. Thus, a model that comprehensively considers a wide range ofbehavioral aspects while incorporating contextual information can be of greatadvantage for effective malware detection, which is lacking in the current liter-ature.• Newer malware applications are surfacing everyday and specific techniques toevade detection systems are also becoming smarter. Preventive measures needto be enforced against adversarial attacks and successful adversarial samples caneffectively render a detection system as useless. The literature proposes proac-tively retraining the models with self-crafted adversarial samples as a preventivemeasure. However, this does not provide a full-proof solution since retrainingwith all or an indiscriminate choice of adversarial samples can lead to perfor-mance degradation due to over-fitting. Intelligent methods that can selectivelychoose higher quality samples are necessary to ensure robustness against suchattacks.• The classification models developed in most prior works assume a balanceddata distribution in terms of the number of malicious and benign applications.



Section 2.11 showed that imbalanced data is an inherent characteristic of thisproblem domain. Cross-domain approaches cannot be applied here since mal-ware detection in mobile platforms has unique characteristics, and this is amajor drawback of prior works in this area which are very limited. For exam-55ple, over-sampling by feature interpolation for data balancing cannot directly beapplied here since the resulting samples often represent applications with bro-ken functionality which makes them invalid samples. To ensure training withvalid samples, application characteristics have to be preserved, and, hence, it isnecessary to address this issue with domain-specific constraints.• Building in-device detection mechanism and knowledge transfer are some addi-tional research challenges in this domain. For instance, computing resource andlimited power constrains mobile devices from hosting a training environment fora malware detection system. This can be overcome by keeping the bare mini-mum in the device and acquiring knowledge from a server (e.g., transferring theconnection weights of a neural network). Upgrading the system by optimizingthe knowledge acquisition process as the model updates to account for newlyexposed attacks is another related research problem. However, these issues arerelated to research on resource-constrained environments and are outside thescope of this thesis.2.13 

ConclusionThis 



Chapter discussed the importance of malware detection in mobile platformswhile illustrating their characteristics and usage. Due to the inherent dependencyof machine learning-based systems on application features, models could result insub-optimal performance if different categories of features are used in a randomizedfashion. The feature categories need to be carefully investigated since they aid inexposing different aspects of application behavior. Further, these models need to em-ploy defense mechanisms against evasive techniques designed to deceive such detectionmethods. Also, considering real-world characteristics such as data imbalance, incor-porating techniques to handle this will ensure models’ practical applicability. In thisregard, a detailed review of the literature was presented in this 



Chapter. Reported56works’ contribution, advantages, and 

Limitations were discussed and open researchchallenges were identified. With a view to designing a robust mobile malware detec-tion system, the next 



Chapter addresses the fundamental issue of feature categories,their characteristic, and impact on machine learning systems for mobile malware de-tection.57



Chapter ThreeImpact of Feature Categories inMobile Malware DetectionThe previous 



Chapter identified that the appropriate choice of feature categories isone of the most crucial steps in designing an effective mobile malware detection sys-tem using machine learning. 



Section 2.9 introduced four different feature categoriesand described the related works. Based on the studies of current literature we havefound that no work has systematically considered all of these feature categories andthoroughly analyzed their individual and collective impact on mobile malware detec-tion. In this 



Chapter, we conduct a systematic study on this issue and investigate thebehaviors of these feature categories. We also identify the combination that gives thebest classification performance. Additionally, we formulate a technique to embed thecontextual information of API calls into the feature space to facilitate its combineduse with other feature categories. The following 



Section describes our detection modeland its components. 



Section 3.2 then undertakes a detailed 



Discussion on the featurecategories and application behaviors they capture. 



Section 3.3 details the contextualinformation, and Sec. 3.4 presents the experiments and 

Results.583.1 Detection ModelFigure 3.1 shows our mobile malware detection system which comprises two phases,training and testing. The training phase prepares the classification model for testing,which consists of two components, feature extraction and classifier training. Thefeature extraction component extracts various categories of features from the trainingset of malicious and benign samples. The extracted features are used to encode eachof the applications into (either benign or malicious) feature vectors which are fedto the classifier for training. This trained classifier is used in the testing phase forclassification of unknown applications. The components of our detection system aredescribed below.3.1.1 Feature Extraction ComponentThe feature extraction component consists of two parts for extracting static anddynamic features respectively. Each application is analyzed individually by this com-ponent and the features extracted from an application are saved in an individualfeature file. Thus, each application has a feature file associated with it that lists theonly features present in that particular application. Additionally, all the features areadded to a global feature set, Ω, which lists all the unique features extracted fromall the applications and is later used for feature vector formulation. Several featureextraction tools were introduced in Sec. 2.7, however, the specific tools used in ourwork along with the feature extraction process are described below.Static Feature ExtractionWe used a reverse engineering tool called Androguard [100] for static feature ex-traction. Androguard provides a Python library that can be used to analyze anapplication to extract permissions, API calls, and ICC features. After listing all theAPI calls in an application, we filtered it through the list provided by Pscout [211]59PermissionAPIICCSystem callClassifier ModelFeature Vector CreationTraining SamplesTest Samples Malicious/BenignStatic Feature ExtractionDynamic Feature ExtractionFeature ExtractionTraining PhaseTesting PhaseClassifier TrainingFeature ExtractionFigure 3.1 Mobile malware detection systemfor extracting the security-sensitive API calls. In the process, we extracted 600 per-missions, 1000 API calls, and 87000 ICC features. Algorithm 1 describes our staticfeature extraction process. For each application in the dataset, this algorithm callsAlg. 2 to 4 for extracting permission, ICC, and API call features respectively. Eachapplication is assigned an individual feature file that lists all the extracted featuresfor the application.Algorithm 1 Static Feature ExtractionInput:Apps← application set for feature extractionOutput:Ω← global feature set1: Import Androguard library for static analysis.2: for each Application in Apps do3: Create FeatureF ile for the application to list the extracted features.4: Call “AnalyzeAPK()" from Androguard library to obtain the decoded APKobject (DecodedApk) and the “Analysis" object (Dx).5: Call Algorithm 2 with FeatureF ile, DecodedApk, and Ω to extract permissionfeatures.6: Call Algorithm 2 with FeatureF ile, Dx, and Ω to extract API features.7: Call Algorithm 3 with FeatureF ile, DecodedApk, and Ω to extract ICCfeatures.8: end for60Algorithm 2 Permission Feature ExtractionInput:DecodedApk ← the decoded APK fileFeatureF ile← feature file for the applicationOutput:Ω← global feature set1: Import Androguard library.2: Call “get_permissions()" function from Androguard to extract the required per-missions for DecodedApk.3: List the extracted permission in FeatureF ile.4: Add the extracted permission to the global feature set Ω.Algorithm 3 ICC Feature ExtractionInput:DecodedApk ← the decoded APK fileFeatureF ile← feature file for the applicationOutput:Ω← global feature set1: Call “get_activities()" function on DecodedApk to extract activities.2: Call “get_services()" function on DecodedApk to extract services.3: Call “get_receivers()" function on DecodedApk to extract broadcast receivers.4: Call “get_providers()" function on DecodedApk to extract content providers.5: Call “get_intent_filter()" function on each activity, service, broadcast receiver,and content provider to extract intent filters.6: List the extracted features in FeatureF ile.7: Add the extracted features to the global feature set Ω.Algorithm 4 API Feature ExtractionInput:Dx← the “Analysis" object from AndroguardFeatureF ile← feature file for the applicationOutput:Ω← global feature set1: Call “get_classes()" function on Dx to get all the classes of the application code2: Call “get_methods()" function get all the method calls for each class3: Call “is_external()" function for each method to determine if it is an externalAPI call4: Filter the external API calls with the list of PScout [211] to extract the securitysensitive API calls.5: List the extracted APIs in FeatureF ile6: Add the extracted APIs to the global feature set Ω.61Dynamic Feature ExtractionSince extracting dynamic feature requires executing the application, we run eachapplication in an emulator to avoid the risk of an actual device being infected. Geny-motion [102] emulator was used for this purpose which provides a free license forpersonal use. A Python script was written to automate the process of installing andrunning the application utilizing the Android Debug Bridge (ADB) [212]. Monkeytool [103] was used for generating random events. We injected 1000 events duringthe execution of each application. The strace [213] utility from Linux was used toattach a tracing process during the application run-time and system calls made bythe application were listed in a log file. Afterward, the log files were parsed to ex-tract dynamic features. We extracted 122 system calls as dynamic features from ourdataset. Algorithm 5 lists our process of dynamic feature extraction.Algorithm 5 Dynamic Feature ExtractionInput:Apps← application set for feature extractionOutput:Ω← global feature set1: Run Genymotion emulator2: for each Application in Apps do3: Open FeatureF ile for the application to list the extracted features.4: Install and run the Application on Genymotion using ADB commands.5: Identify the process id (PID) of the running application.6: Create logF ile to log the dynamic behaviors.7: Attach strace to the process for logging system call.8: Inject random events using Monkey tool.9: Terminate application execution.10: Write output from strace to logF ile.11: Pull the logF ile out from the emulator memory.12: Parse logF ile to extract system call features.13: List the system calls in FeatureF ile.14: Add the the system calls to the global feature set Ω.15: end for623.1.2 Feature Matrix FormationAfter the features were extracted, each of the applications were then encoded as afeature vector. A feature vector is a binary vector whose length is the number of totalunique features extracted from all the samples (i.e., the length of the global featurevector Ω). Each component (i.e., index) of this vector corresponds to a feature.For each application, a feature vector is first initialized with 0 in all of its indices.Afterward, the feature file associated with the application was parsed and the valueof the indices corresponding to the features listed in the feature file was set to 1.Hence, an entry of 1 in the feature vector means the corresponding feature is presentin the application and 0 means it is absent. These feature vectors were compiled asa matrix representing the dataset where each row of the matrix represented a featurevector for an app. Algorithm 6 shows the process of feature matrix creation.Algorithm 6 Feature Matrix CreationInput:Apps← application set for feature matrix creationΩ← global feature setOutput:FeatureMat . The feature matrix1: FeatureMat← Φ . Initialize feature matrix2: for each Application in Apps do3: Open FeatureF ile of the Application4: V ec← allocate memory size = length(Ω) + 1 . Allocate one extra indexfor class label5: V ec← initialize all indices to 06: if Application is malware then . Set last index to 1 if it’s a malware7: V ec[size] = 18: end if9: for each Feature in FeatureF ile do10: idx← index of Feature in Ω11: V ec[idx] = 112: end for13: Append V ec to FeatureMatrix14: end for15: return FeatureMatrix633.2 Feature Category CharacteristicsIn Sec. 2.9, we briefly introduced different feature categories of an Android applica-tion. This 



Section discusses the characteristics of these feature categories in detail.We also discuss the behavioral aspects captured by these feature categories and howthey influence a malware classification model.PermissionEvery Android application must acquire the necessary permissions before executingthe corresponding functionality. Each Android application needs to declare the re-quired permissions in its manifest file. The user is prompted with these permissionsbefore the installation process begins. Figure 3.2 shows the installation screen of anapplication that displays the required permissions. There are two types of permissionfor an Android application: the official Android permissions and developer-definedpermissions that are used to protect the components of an application.Permissions are one of the most commonly used feature categories in mobile mal-ware detection since acquiring necessary permissions is the first barrier a maliciousapplication has to overcome. Figure 3.3 shows the most commonly occurring permis-sions in malware and benign applications in our database as described in 



Chapter 2.The figure shows that both malware and benign applications most commonly requestpermission for accessing the internet, which is understandable since most applica-tions (malware or benign) require internet access to carry out basic functionalities.However, a lot of the malicious applications also ask for permission to send and re-ceive SMSs which is abused for stealing money. Permission to check whether thephone has completed booting is another commonly requested permission by malwareapplications which aids them to carry out activities secretly in the 

Background.64Figure 3.2 Installation screen showing the required permission for an appli-cation.API callsAPI calls are functions of Android java library that are available to application de-velopers to request services from the operating system (OS) and to perform differentoperations. For example, to get the location of a device, the application will call the“TelephonyManager.getDeviceID()" function. Figure 3.4 shows the most commonlyoccurring API function calls in malware and benign applications. The figure showsthat most malware applications use the “SmsManager.sendTextMessage()" API func-tion for stealing money. “webkit.WebView()" is another function used by the malwareapps to direct users to websites with malicious contents and scams.ICC featuresThe Android OS allows the components of an app (describe in Sec. 2.2) to com-municate with each other through ICC mechanism. This was made available toprogrammers to reduce the burden of coding similar components. Figure 3.5 shows65 1500 2000 2500 3000 3500 4000 4500 5000android.permission.INTERNETandroid.permission.READ_PHONE_STATEandroid.permission.WRITE_EXTERNAL_STORAGEandroid.permission.ACCESS_NETWORK_STATEandroid.permission.SEND_SMSandroid.permission.RECEIVE_BOOT_COMPLETEDandroid.permission.ACCESS_WIFI_STATEandroid.permission.RECEIVE_SMSandroid.permission.WAKE_LOCKandroid.permission.READ_SMSNumber of malware samples(a) Permission features in malware samples 500 1000 1500 2000 2500 3000 3500 4000 4500 5000android.permission.INTERNETandroid.permission.ACCESS_NETWORK_STATEandroid.permission.WRITE_EXTERNAL_STORAGEandroid.permission.READ_PHONE_STATEandroid.permission.ACCESS_COARSE_LOCATIONandroid.permission.ACCESS_FINE_LOCATIONandroid.permission.VIBRATEandroid.permission.WAKE_LOCKandroid.permission.ACCESS_WIFI_STATEandroid.permission.CALL_PHONENumber of benign samples(b) Permission features in benign samplesFigure 3.3 Most commonly occurring permission features in malware andbenign applications in our dataset.66 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 3200android.telephony.TelephonyManager.getDeviceId()android.app.NotificationManager.notify()android.content.ContentResolver.query()android.net.ConnectivityManager.getActiveNetworkInfo()android.telephony.SmsManager.sendTextMessage()android.content.Context.startActivity()java.lang.Runtime.exec()android.webkit.WebView()android.content.Context.startService()java.net.HttpURLConnection()Number of malware samples(a) API calls in malware samples 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800android.net.ConnectivityManager.getActiveNetworkInfo()android.content.Context.startActivity()android.webkit.WebView()android.telephony.TelephonyManager.getDeviceId()android.location.LocationManager.getBestProvider()android.media.MediaPlayer.start()android.app.Activity.startActivity()android.content.ContentResolver.query()android.app.NotificationManager.notify()java.net.HttpURLConnection()Number of benign samples(b) API calls in benign samplesFigure 3.4 Most commonly occurring API features in malware and benignapplications in our dataset.67an example where ICC mechanism is useful. In this example, the Facebook app istrying to show the location of an address on a map. Using ICC, Facebook can transferthe address to a map application (e.g., Google maps) where the exact location of theaddress is displayed.Malware developers can exploit this facility to construct collusion attacks. Figure 3.6exemplifies one such attack scenario. The application on the left denoted by a redrectangle leaks user information by getting the device location and sending an SMS tothe attacker. However, since these two functions are done by the same application, itis relatively easy to detect such a privacy leak attack. Conversely, two apps denotedby the green rectangles on the right side only perform one of the two actions, i.e.,one application is only capable of getting the location and the other can only sendSMSs. On their own, these two apps seem harmless but they can accomplish the samefunctionality with a collusion attack by communicating through ICC while bypassingdetection.Figure 3.7 shows the most commonly occurring ICC features in malware and benignapplications. It is noteworthy that most of the benign and malicious applicationscontain “android.intent.action.MAIN" and “andoid.intent.category.LAUNCHER" asICC features since they are used in basic execution process of an Android application.But we see many malicious applications having “android.intent.action.BOOT_COMPLETED" and “android.intent.category.HOME" as ICC features thatenable them to carry out secret activities in the 

Background.System callsThe Android system is built on top of Linux kernel and most of its operations are donethrough Linux system calls. The system calls made by an application can be tracedduring run-time, thereby dynamically revealing the malicious activities of an appli-cation. For example, some malware applications hide their malicious code to bypassdetection when the program is launched. At a later period, they dynamically load68Figure 3.5 Apps communicating through ICC mechanism.Figure 3.6 Collusion attack using ICC mechanism.their malicious codes. This dynamic loading goes through a series of actions whichincludes making an “open()" system call for opening the target code and invoking“mmap()" to load it into the memory. These behaviors can be captured throughdynamic analysis of system calls.69 500 1000 1500 2000 2500 3000 3500 4000 4500 5000android.intent.action.MAINandroid.intent.category.LAUNCHERandroid.intent.action.BOOT_COMPLETEDandroid.intent.category.HOMEandroid.provider.Telephony.SMS_RECEIVEDandroid.intent.category.DEFAULT.Mainandroid.intent.action.SIG_STRcom.apperhand.device.android.AndroidSDKProviderandroid.intent.action.USER_PRESENTNumber of malware samples(a) Malware samples 0 1000 2000 3000 4000 5000 6000android.intent.action.MAINandroid.intent.category.LAUNCHERandroid.intent.category.DEFAULTandroid.intent.action.VIEWcom.admob.android.ads.AdMobActivity.MainActivitycom.admob.android.ads.analytics.InstallReceiverandroid.intent.action.BOOT_COMPLETEDandroid.appwidget.provider.MainNumber of benign samples(b) Benign samplesFigure 3.7 Most commonly occurring ICC features in malware and benignapplications.703.3 Encoding Contextual Information asFeature ValueThe contextual information of Android API calls was briefly introduced in Sec. 2.9.1.This 



Section discusses this in detail, as well as how we incorporate this informationinto feature vectors.The execution procedure of the Android system makes the applications heavily depen-dent on “callback" methods. For example, when an application starts, the “onCreate"callback method of its “main activity" is executed. Or when a user clicks a button,the “onClickListener" callback method is called to carry out the actions associatedwith the button click. These callback methods are tied to some activation events,i.e., for the above examples, the “onCreate" method is tied to the event of startingthe application and the “onClickListener" method is to tied to the event of clickinga button. These can be utilized to obtain additional information about an API callmade by an Android application.To illustrate the above, we take the example of the working principle of MoonSMSmalware. A simplified call graph of the application is shown in Fig. 3.8. This mal-ware appears as a benign application that sends greetings messages. In the process, itobtains the “SEND_SMS" permission so that it can call the “sendTextMessage()" func-tion. As shown in Fig. 3.8 “SmsManager.sendTextMessage()" is a security-sensitiveAPI call used to send SMSs which can be invoked by the application in several ways.First, the method is invoked when the user clicks the “send" button, which executesthe “SendTextActivity.onClick()" callback method that consequently sends an SMS.This is the expected use case highlighted with green arrows in Fig. 3.8. Second, whenthe signal strength of the device is changed, Android OS generates a correspondingevent. The application registers the “ActionReceiver.OnReceiver()" callback methodto respond to this event. This callback method checks if the current time is between11 pm and 5 am when the user is expected to be unaware of this covert action and71proceeds to send an SMS. Third, whenever the application is launched, its “SplashAc-tivity.OnCreate()" callback method is called which checks if the last connect time isat least 12 hours ago. Expecting that one SMS per 12-hour interval will go unnoticed,the malware application invokes the sendTextMessage() function.In the above example, the first use case is not malicious since the user is aware of theoperation. The user can actually see the information on the screen and can modifythe action if necessary. From an execution point of view, this is tied to the event ofbutton click by the user. However, the last two 



Scenarios are malicious since theyare being carried out unnoticed by the user and used to send premium SMSs to amalicious server to steal money from the user. These are tied with “signal change"event and “application start" event respectively.By analyzing the contextual information (i.e., the associated activation events), addi-tional information about the usage of an API call can be obtained and, consequently,the behavior of an application more accurately predicted. Extracting contextualinformation has two components, namely, entry point identification and activationevent resolution. For this, we first construct the call graph of an application usingthe Androguard [100] tool. Subsequently, the nodes in the call graph that contains asecurity-sensitive API call are identified. These nodes will represent the API featuresof an application. Thereafter the two components are executed consecutively. Theseare described bellow.Entry point identification. To identify the entry points of a security-sensitive APIcall, the call graph of the application is traversed upward in a Depth First Search(DFS) [214] manner until a root node is reached. This process is carried out in aniterative fashion to identify every entry point from which the corresponding API canbe reached. It noteworthy that an application call graph can have multiple root nodesor entry points since there is no main function in the Android application code andthe operations are basically initiated through callback methods.72ActionReceiver.onReceive() SpashActivity.onCreate()DummyMainMethodMainService.onCreate()If the current time is between 11 pm and 5 am start the MainService,SendTextActivity$4.onClick()If the last connect time was more than 12 hours ago send SMSSmsManager.sendTextMessage()Figure 3.8 Call graph of Moon SMS malwareActivation event resolution. Activation events of an Android application can be oftwo types, user interface (UI) activation (user aware context) and 

Background activa-tion (user unaware context) [215]. To resolve which type of activation an entry pointcorresponds to, we check its overridden class. If an entry point overrides any of thethree interfaces among “android.view.KeyEvent.Callback", “android.graphics.drawable.Drawable.Callback", and “android.view.accessibility.AccessibilityEventSource", it isresolved to a UI activation events since these are the top-level interfaces related touser interaction [215]. Otherwise, the entry point is resolved to a 

Background eventactivation.The example shown in Fig. 3.8 demonstrates that an API call initiated by userinteraction is less likely to be a malicious case, whereas, an API call initiated bya 

Background event has a greater potential of being malicious. Representing theAPI calls simply by a binary value as in [14, 54] cannot capture this contextualinformation which can lead to classifying both benign and malicious calls to the sameclass. Our initial experiment confirms that this way of representation produces morefalse negatives (i.e., misses malware). To encode the contextual information into anumeric value, we used the activation events to calculate a maliciousness indicatorscore Υ using the following equationΥ =Wuivui +WbevbeWui +Wbe. (3.1)here, Wui and Wbe are the weights associated with UI callback and 

Background event73callback respectively. vui and vbe are binary values representing whether the API callis initiated by the respective activation event (1) or not(0). This score (Υ) is fed tothe sigmoid function to obtain the feature value Γ:Γ = sig(kΥ). (3.2)The sigmoid function is defined as:sig(kx) =11 + exp(−kx)(3.3)here, k is the steepness parameter and x=Υ. The parameters in Eq. (3.1) and Eq.(3.2) need to be determined such that the computed feature values show a considerabledistinction between the user aware and unaware API calls. This is formulated as thefollowing optimization problem.argmaxWui,WbeK (3.4a)s.t. Wui,Wbe, k = [W1,W2,W3,W4, . . . ] (3.4b)U = [Γ, for vui = 0, 1 and vbe = 0, 1] (3.4c)Ψ = |Ua − Ub| for a = 1, . . . , (‖U‖ − 1) (3.4d)b = a+ 1, . . . ,‖U‖K = min(Ψ) (3.4e)where |Ua − Ub| de

Notes the absolute difference between the ath and bth indices ofU, and ‖U‖ de

Notes the length of U. We performed a grid search on the parametersWui, Wbe, and k varying the range from 0.5 to 5 with an interval of 0.5. The optimalvalues were found to be 0.5, 4, and 1 for UI event activation (Wui), 

Background eventactivation (Wbe), and k respectively.743.4 Experiments and 

ResultsThe primary aim of this 



Chapter and therefore the following experiments was to care-fully examine the impact of different feature categories on mobile malware detectionsystems and their relative importance. Due to the promising performance of DNN(introduced in Sec. 2.5) reported in the literature, we also adopted a DNN basedarchitecture for our first set of experiments. However, afterward we verified our find-ings with three different classifiers including SVM and Random forest. The first setof experiments was divided into two parts. First, the characteristics of individualfeature categories were investigated, and second, all possible combinations of featurecategories were taken and their effectiveness analyzed to find the best possible com-bination. Afterward, the contextual embedding described in Sec. 3.3 was includedin the experiment set and its performance was recorded. A relevancy analysis foreach feature category was then performed, followed by some additional experimentswith the ICC feature category. Each of the experiments was run for 20 trials and theaverage of these trials is reported in the 

Results below.3.4.1 Characteristics of Individual Feature Categories andTheir EffectsTable 3.1 shows the malware detection performance of a deep neural network usinga single feature category. It can be seen that, as a single feature, permission givesthe best result, attaining 96.69% accuracy. Since the first requirement an applicationmust have is to get the necessary permissions to perform any sort of operation, “per-mission", as a single feature category, reveals the general behavior of an applicationmore effectively than any other features. API calls and dynamic features provide com-parable performance, but they capture the behavior of an application from differentperspectives. API calls are statically extracted and represent the functionalities avail-able in the application code. Dynamic features capture the run-time functionalities75Table 3.1 Malware detection performance with individual feature category.Deep learning model is used for classification.MetricFeaturesPermission API Calls ICC Dynamic featuresTP 2831 2770 1690 2675TN 5435 5352 5020 5263FP 142 225 557 314FN 141 202 1282 297Accuracy 96.69 95.01 78.49 92.85Precision 0.952 0.925 0.752 0.895Recall 0.953 0.932 0.569 0.900Specificity 0.975 0.960 0.900 0.944G-mean 0.963 0.946 0.715 0.922F1-score 0.952 0.928 0.648 0.898while they are being executed. Most of the security-related operations can be tracedback to some API calls which can be mapped to the corresponding system call. For ex-ample, to send an SMS, an application has to call “SmsManager.sendTextMessage()"API through the code which, during the execution, will invoke the “sendto()" systemcall from the OS. As a result, we found similar performances from these two cate-gories. Dynamic features are useful to detect code obfuscation and dynamic codeloading which are used by malware applications to bypass static detection systems.However, all-inclusive coverage of every possible execution sequence of an applicationis not feasible. Hence, aggregation of dynamic features with static analysis is moreeffective for malware detection. Later experiments further expand on this.ICC features are found to be the lowest performing in this set of experiments. This isexplainable in that the names of the components that communicate with each otherare taken as ICC features. If different applications define different names for theircomponents while carrying out basically the same functionality the resulting features76 0 0.2 0.4 0.6 0.8 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RatePermission AUC = 0.989API AUC = 0.983Dynamic AUC = 0.979ICC AUC = 0.826Figure 3.9 ROC curve and AUC for individual feature categorieswill still be different. These behaviors will not be captured by the classifier exceptfor the unlikely scenario where every application uses the same name for their com-ponents. As a result, a relatively lower performance by the ICC features is observed.Figure 3.9 also demonstrates that the lowest AUC value is obtained by ICC (0.826)while the best is obtained by permission features (0.989).3.4.2 Malware Detection using Different Combinations of Fea-ture CategoriesThis 



Section describes malware detection capability with all possible combinationsof feature categories. The first part presents the six combinations where only onecategory is combined with another which helps to analyze which feature categoriescomplement each other well. The second part combined two or more categories re-sulting in another five combinations.77Table 3.2 Malware detection performance with combination of two featurecategories.MetricFeaturesPermission, API Permission, ICC Permission, Dynamic API, ICC API, Dynamic ICC, DynamicTP 2910 2752 2833 2900 2840 2743TN 5319 5431 5454 5363 5492 5343FP 258 146 123 214 85 234FN 62 220 139 72 132 229Accuracy 96.26 95.72 96.94 96.65 97.46 94.58Precision 0.919 0.950 0.958 0.931 0.971 0.921Recall 0.979 0.926 0.953 0.976 0.956 0.923Specificity 0.954 0.974 0.978 0.962 0.985 0.958G-mean 0.966 0.950 0.966 0.969 0.970 0.940F1-score 0.948 0.938 0.956 0.953 0.963 0.922Table 3.2 shows the malware detection performance when only one feature category iscombined with another. Drawing from the 

Conclusion of the previous 



Section, we seethat dynamic features complement the static features most effectively when combined.The best performance in these experiments was obtained by combining “permissionand dynamic features" and “API calls and dynamic features" achieving 96.94% and97.46% accuracy respectively. Considering other performance metrics, overall, thelater combination performed better.Table 3.3 shows the malware detection performance when more than two featurecategories are combined. It is noteworthy that the combination of permission, APIcalls, and dynamic features gives the best performance here with 97.78% accuracyand 0.968 F1-score. As reflected by the 

Results of “all" feature category combination,further addition of ICC features does not improve the classification performance.Rather, in some cases, it slightly degrades performance, for example, the accuracyand F1-score falls to 97.6% and 0.966 respectively. Figure 3.10 further confirms thisas the AUC value falls to 0.994 from 0.996. This is consistent with our findings fromSec. 3.4.1 where we experimented with individual feature categories and found ICCto be the least performing. Also, since the number of ICC features becomes very78Table 3.3 Malware detection performance with combination of more thantwo feature categoriesMetricFeaturesPermission, API, ICC Permission, API, Dynamic Permission, ICC, Dynamic API, ICC, Dynamic AllTP 2905 2863 2910 2905 2909TN 5422 5496 5291 5373 5435FP 155 81 286 204 142FN 67 109 62 67 63Accuracy 97.40 97.78 95.93 96.83 97.60Precision 0.949 0.972 0.911 0.934 0.953Recall 0.977 0.963 0.979 0.977 0.979Specificity 0.972 0.985 0.949 0.963 0.975G-mean 0.975 0.974 0.964 0.970 0.977F1-score 0.963 0.968 0.944 0.955 0.966 0.9 0.92 0.94 0.96 0.98 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateAUC1 = 0.996AUC2 = 0.994Figure 3.10 ROC curve and AUC for combination of feature categories.AUC1 = Permission, API, and Dynamic, AUC2 = All feature categories.79large (around 87,000) due to different components names, the feature space becomesvery sparse. This 

Results in over-fitting due to “The Curse of Dimensionality" [216].Hence, it is important to make an informed decision, rather than blindly combiningthe feature categories, when designing a malware detection system.3.4.3 Effect of Contextual InformationTable 3.4 shows the impact of embedding contextual information of API calls intothe feature space. It shows that with this additional contextual information, API calloutperforms permissions as a single feature category. This is because API calls candistinguish between malicious and benign behaviors more effectively than permissionfeatures when contextual information is provided. For example, sending an SMS bya benign application and malicious application is represented by the same permis-sion, i.e., “android.permission.SEND_SMS" and therefore, is not distinguishable bythe permission feature. On the other hand, with the contextual information (i.e.,user aware or unaware use cases; described in Sec. 3.3), API features can differen-tiate between potential malicious and benign usages. Table 3.4 also shows that thecombination of permission, API, and dynamic features gives the best result for allthe performance measures, attaining 98.21% overall accuracy. Figure 3.11 shows thebest AUC of 0.996 is obtained by the combination of permission, API, and dynamicfeatures.3.4.4 Validating with Other Widely Used ClassifiersThis next set of experiments were conducted with various other classifiers to validatewhether the relative performance with different combinations of feature categoriesfor these classifiers stays consistent with our findings from DNN-based experiments.The result of these experiments is shown in Fig. 3.12. It can be seen that the actualperformance of the individual classifier varies. However, the relative performance fordifferent feature category combinations is consistent with our previous finding – the80Table 3.4 Malware detection performance using different categories of fea-ture and their combination with embedded contextual information in relationto API calls.MetricFeatures API Permission, API, Dynamic ALLTP 2867 2899 2876TN 5502 5497 5496FP 75 80 81FN 105 73 96Accuracy 97.89 98.21 97.93Precision 0.973 0.973 0.973Recall 0.965 0.975 0.968Specificity 0.986 0.986 0.985G-mean 0.976 0.981 0.977F1-score 0.970 0.974 0.970 0.9 0.92 0.94 0.96 0.98 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateAUC1 = 0.996AUC2 = 0.995AUC3 = 0.994Figure 3.11 ROC curve and AUC with embedded contextual information.AUC1 = Permission, API, and Dynamic, AUC2 = All feature categories, andAUC3 = API features.81 0.7 0.75 0.8 0.85 0.9 0.95 1C1 C2 C3 C4Feature category combinationsAccuracy G-mean F1-score(a) SVM 0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985C1 C2 C3 C4Feature category combinationsAccuracy G-mean F1-score(b) Random Forest 0.94 0.945 0.95 0.955 0.96 0.965 0.97C1 C2 C3 C4Feature category combinationsAccuracy G-mean F1-score(c) BayesianFigure 3.12 Performance of different classifiers with various feature categorycombinations. Combination - C1 : Permission, API, Dynamic, C2 : All, C3:Permission, API with context, Dynamic and C4 : All with context82combination of permission, API, and dynamic feature categories performs better thanall other category combination as measured by SVM and random forest (Fig. 3.12aand Fig. 3.12b) and it gives a comparable performance as measured by Bayesianclassifier (Fig. 3.12c). This again substantiates that using all feature categories doesnot necessarily provide any advantage in terms of detection performance.3.4.5 Relevancy Analysis of Feature CategoriesAfter observing that the ICC feature category has less impact on classifier perfor-mance and possibly introduces redundancy when combined with all other categories,we further studied the relevancy of each feature category. We used a relevancy mea-sure based on entropy and information gain for this analysis. The entropy of a randomvariable X (e.g., a feature) is defined as:En(X) = −∑iP (xi)log2(P (xi)) (3.5)The entropy of X given another variable Y is:En(X|Y ) = −∑jP (yj)∑iP (xi|yi)log2(P (xi|yi)) (3.6)The information gain of X given Y can be calculated as [217]:Ig(X|Y ) = En(X)− En(X|Y ). (3.7)We computed the information gain of each feature based on the class label and usedit as the feature relevancy indicator. If this value was greater than a threshold ϑ, thefeature was considered to be relevant. The value of ϑ is predefined manually. Figure3.13 shows the number of relevant features with respect to the ϑ threshold for eachcategory. Increasing the threshold indicates a more stringent condition for a featureto be selected as relevant which, in other words, means that only features that exhibit83higher influence on prediction outcome are selected at an increased threshold. Figure3.13 shows that the number of relevant ICC features decreased sharply with increasedthreshold, while other feature categories showed a much slower decrement. Dynamiccategory showed the least decrement with increased threshold, suggesting that almostall of the feature set is highly relevant. However, we see that the number of relevantfeatures for the ICC category drastically reduces with a slight increase in thresholdvalue after ϑ = 1 × 10−4. As a further step, we also inspected the individual ICCfeatures deemed not relevant (i.e., redundant) at the threshold value 1 × 10−4. Atthis threshold value, about 60000 ICC features were found to be redundant and, aftercloser inspection, we found that these features were absent in most of the apps. In fact,none of these features were present in more than 10 apps in the dataset. This suggeststhat this large number of ICC features do not provide any useful information aboutmalware classification; rather, they introduce redundancy and mislead the classifierleading to decreased classification performance.10−4 10−3 10−2102103104ϑ thresholdno.ofrelevantfeaturesPermissionAPIICCDynamicFigure 3.13 Number of relevant features843.4.6 Further Analysis of ICC Feature CategoryPrevious experiments showed that the ICC feature category does not have a posi-tive impact on malware classification models. However, this could also indicate thatthe dataset does not have a sufficient number of applications that perform collusionattacks, i.e., the type of attack ICC features can detect. Hence, to further analyzethe efficacy of ICC features we augmented the dataset with malware samples thatare specifically generated. For this purpose, we used the ACID tool [218] and gener-ated 832 colluding apps that access one of the sensitive data listed in Table 3.5 andcooperate with another app to leak the information through ICC.Table 3.5 Information accessed by generated colluding apps.Function name Accessed informationIMEI Accesses the device IMEI numberMicrophone Records phone audioContacts Reads phone contactsHistory Accesses the navigation historyLocation Accesses the device locationWiFi Gets the list of WiFi SSIDsAccounts Reads the device account informationWith this augmented dataset we performed additional experiments with the DNNclassifier. Table 3.6 shows the 

Results of these experiments. As the table shows,there is not much difference in the performance and only a slight improvement isobserved, with the accuracy increasing from 98.39% to 98.66% and the AUC value(in Fig. 3.14) increasing from 0.982 to 0.997. However, this improvement comes witha huge computation overhead as depicted in Fig. 3.15 which shows that when ICCfeatures are combined with other categories the model training time increases by amagnitude of ten. This can create serious issues especially when a faster deploymentof the model is required. Also, increased training time hampers model retrainingwhich is required for adaptation and for making the model robust against newerattacks and vulnerabilities.85Table 3.6 Malware detection performance with generated collusion attacksusing ICC feature category. Note that the dataset used here is the augmenteddataset comprising the previous samples and 832 colluding malware apps.MetricFeatures ICC Permission, API, Dynamic ALLTP 2332 3746 3756TN 5051 5484 5499FP 526 93 78FN 1472 58 48Accuracy 78.70 98.39 98.66Precision 0.816 0.976 0.980Recall 0.613 0.985 0.987Specificity 0.906 0.983 0.986G-mean 0.745 0.984 0.987F1-score 0.700 0.980 0.984 0 0.2 0.4 0.6 0.8 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateAUC1 = 0.997AUC2 = 0.982AUC3 = 0.979Figure 3.14 ROC curve and AUC with generated attacks using ICC featurecategory. AUC1 = All feature categories, AUC2 = Permission, API, andDynamic, and AUC3 = ICC features.86 0 2000 4000 6000 8000 10000C1 C2 C3 C4Training time (sec)Feature category combinationsFigure 3.15 Training time for combination C1 : Permission, API, Dynamic,C2 : All, C3: Permission, API with context, Dynamic and C4 : Allwith contextThe aim of this study was to investigate the relative influence of different featurecategories on mobile malware detection. Considering the performance and executiontime of different feature category combinations, we recommend that the permission,API calls, and dynamic features be used for the mobile malware detection systems.3.5 

ConclusionIn this 



Chapter, we analyzed the impact of different feature categories of mobile appli-cations on machine learning-based malware detection systems. Through systematicinvestigation and extensive experiments, we showed that blind combination of fea-tures degrades classifier performance and some feature categories add redundancyto the feature space. In the process, feature categories that complement each otherwere determined and the combination of permission, API calls, and dynamic featurecategories was found to yield the best performance. Additionally, a novel embeddingtechnique for contextual information was devised in this 



Chapter. This allows the con-textual information to be embedded in the feature space which facilitates its combinedusage with other feature categories. The classifier designed in this 



Chapter provides87the basis for the experiments in the rest of the work. The next 



Chapter discusses anevasive attack (known as adversarial attack) that breaks down the detection systemsdiscussed in this 



Chapter and presents an intelligent selective sampling technique todefend against such attacks.88



Chapter FourRobustness Against AdversarialMalware Attacks4.1 



IntroductionIn the previous 



Chapter, we conducted an extensive study on the feature categories fordesigning a mobile malware detection system and identified their best combinationin terms of classification performance which provides the basis for the rest of ourworks. We recall from Sec. 2.10 that machine learning models are vulnerable toadversarial attacks, which is an evasive technique designed to bypass a detectionsystem. An already detected malware can morph itself with carefully introducedperturbation and render a detection system ineffective. Retraining a model withself-crafted adversarial samples can be used to defend against such attacks. Thequality of the retraining samples plays a crucial role in this regard since retrainingwith too many samples 

Results in performance degradation due to over-fitting. Also,randomly selected sample subset, as suggested by some previous works, can lead tosub-optimal performance since good quality samples can be left out. No prior worksin the literature addressed these issues. In this 



Chapter, we design an intelligenttechnique for evaluating the sample quality and selectively choose the best possiblesample set for adversarial retraining to make a malware detection system robust89against adversarial attacks.4.2 Adversarial ExamplesAdversarial attacks are conducted through adversarial samples crafted by introducinga small perturbation into correctly classified samples so that the classifier is misledand classifies them into incorrect classes. Formally, we take a malware detectionmodel M (described in Sec. 2.5) and represent a correctly classified sample X asM(X) = y, where y is its original class label. An adversarial sample X ′ is crafted byintroducing perturbation δx such thatargminδxM(X + δx) = {y′ | y′ 6= y} (4.1)where y′ de

Notes an incorrect class label and δx is the perturbation introduced to thesample X.Figure 4.1 shows an illustrative example of an adversarial sample. The blue and greenstripes represent the value ‘0’ and ‘1’ in the feature vector, respectively. The top partof the figure represents a malicious sample correctly classified by the detection system.The bottom part is an adversarially crafted sample with some of the features modifiedfrom the original malware sample. As shown in Fig. 4.1, the features were modifiedin two of the regions (indicated by two brackets at the bottom) and left unchangedin the other two (indicated by two brackets on the top). The unchanged regionsrepresent the functionalities preserved from the original sample. Due to this carefulmodification, an adversarial sample can mislead the classifier. As long as the maliciousfunctionalities of the application are preserved, an adversarially crafted sample canfunction as a malware application and go undetected by the detection system.adversarial attacks can be divided into several types depending on the attacker’s goaland knowledge about the detection system. These are described below.90Figure 4.1 Illustrative example of an adversarial sampleTypes of Adversarial AttackBased on the attacker’s knowledge about the detection system, adversarial attackscan be of three types:• White-box attack assumes the attacker has a detailed knowledge about the de-tection system (i.e., knows the model architecture, hyper-parameters, trainingand testing data, and model weights).• Semi black-box attack assumes the attacker has partial knowledge of the targetsystem. For example, only the training and testing data are known without theinternal detail of the model.• Back-box attack assumes that the attacker does not have any knowledge of thetarget system regarding its data or parameters. The system is only accessibleas a standard user, i.e., only the output of the model is available either in theform of class label or probability score.Based on the attacker’s goal, adversarial attacks can be of two types:91• Targeted attack specifies the target class of the adversarial sample. This isachieved by setting the condition to y′ = yt in Eq. (4.1) where yt is the intendedtarget class.• Non-targeted attack only tries to mislead the classifier while minimizing theperturbation. This could be achieved by launching several targeted attacks andtaking a successful sample.In a binary classification scenario (e.g., benign and malware), targeted attacks areequivalent to non-targeted attacks. It is noteworthy from the above 



Discussion thatan adversary is strongest in a white-box attack scenario where it has access to all theinformation about the target model. Most of the approaches in the literature are alsowhite-box in nature [151, 154, 161, 219]. With a view to achieving robustness againstthe strongest possible attack, we also adopted a white-box attack model in our work.In the following 



Section, the attack model and our proposed defense methods aredetailed.4.3 



MethodologyIn this 



Section, we describe the 



Methodology of our work. Figure 4.2 shows the work-flow of adversarial retraining using our selective samples strategy (i.e., crafting andidentifying the samples that, when used in retraining the detection model, will makethe model more robust). A classifier trained with clean data (i.e., the dataset withoutadversarial samples) is used as a target for adversarial attacks. Afterward, the mal-ware samples from the test data are perturbed to craft adversarial samples to evadedetection by the classifier. The malicious samples that were correctly detected byclassifier but evaded detection after perturbation were considered successful adver-sarial samples. Afterwards, a subset of adversarial samples was selected and combinedwith the original samples to retrain the same classifier. For this purpose, we proposetwo mechanisms to select adversarial samples. 



Section 4.3.1 describes the adversar-92ial sample crafting process adopted in our work. The detail of our sample selectiontechnique is presented in Sec. 4.3.2, followed by the experiments and 

Results in Sec.4.4.4.3.1 Crafting Adversarial Malware SamplesThe technique of crafting adversarial samples described in this 



Section can be appliedto any differentiable classification function. However, the most common applicationof this is with DNN [151, 154, 161]. Our work also adopted a DNN to craft theadversarial samples. Note that once adversarial samples are crafted, they can also beused to test against other classification methods.We begin with a malware sample X ∈ {0, 1}n and note the prediction outcomesusing the DNN. Using two neurons to represent two classes, the final layer of thetrained DNN model M outputs two values M(X) = [M0(X),M1(X)], indicatingthe probability of X being a benign application or a malware respectively. As theprediction, we choose the class that has the highest probability. For crafting anadversarial example, we want to find a small perturbation δx so that the outputM(X + δx) is different from the original prediction and matches the attacker’s goal.This can be done by solving Eq. (4.1) described in Sec. 4.2.However, the complex functions learned by neural networks are generally non-linearand non-convex. As a result, finding a solution to this problem is difficult. Re-searchers have proposed various heuristic solutions including forward gradient-based[154] and saliency map-based solutions [219]. The most notable works in this regardare described in Sec. 2.10.2.In our work, we adopt a saliency map-based adversarial sample crafting techniqueusing the Jacobian matrix since it is more suitable for binary features [219]. Thegoal here is to craft adversarial malware samples so that they are classified as benignapplications. The Jacobian matrix is defined as93TrainedClassifierFrom Objective 1Adversarial Sample CraftingAdversarial SamplesSelection MethodAdversarialRetraining1. Compute Jacobian matrix of the output function with respect to input features2. Apply constraints to limit modifiable features3. Change features with the highest positive gradient towards the target classRandom Distance basedKBLRobust ClassifierFigure 4.2 Work flow of selective adversarial retrainingJM =∇M(X)∇X=∇M0(X)∇X0 ... ∇M0(X)∇Xn∇M1(X)∇X0 ...∇M1(X)∇Xn (4.2)here ∇ de

Notes the gradient of a function. After computing the Jacobian matrix (i.e.,the derivatives of the cost function with respect to the input), we modify the featurethat is most influential towards our target class. Since our target class is 0 (i.e., tofool the classifier to misclassify the adversarial sample as begin) we find the featurethat has the highest value in ∇M0(X) for modification. This process is iterated untilthe malicious application is misclassified as benign.However, modifying the value of such features is not as straightforward. For example,the method was originally proposed for image processing where several constraintswere imposed so that the modification of pixel values does not visually distort theimage. An example of such a constraint can be allowing only a small change in pixelvalue (i.e., keeping the change under a certain threshold). However, unlike imageprocessing, where the values of the image pixels are continuous, we only have 0 and 194as feature values (except the contextual values which cannot be modified as we will seelater). As a result, thresholding for feature values cannot be applied here. Further,an attacker will ensure that the malicious functionality of a malicious applicationis not hampered while adversarial samples are crafted and hence, arbitrary featurescannot be modified. For this, we imposed a different set of constraints for craftingadversarial malware samples. These are described below.Constraints for Adversarial Malware CraftingAs crafted malware samples need to preserve their malicious functionality, the featuresthat could potentially hamper the maliciousness of an application cannot be modified.Taking the malicious behavior of a broad range of malware into account, we impose thefollowing specific constraints on adversarial malware crafting to preserve its maliciousfunctionality:• Features are allowed to be added but not removed since removing a featurecould potentially break down the corresponding code execution. This means weonly allow modification to the features that have value 0.• We allow the addition of only those features extracted from the AndroidMan-ifest.xml file (discussed in Sec. 2.2) since adding features in this way does notnecessitate modification of the original code, hence the original functionality ofthe code can be preserved.• Modification of dynamic (i.e., system call) features is not allowed. This isbecause adding a dynamic feature (e.g., system call) requires modifying thecode for executing the corresponding function which, in turn, may hamper theoriginal functionality of the malware.To implement the above constraints, we first define a vector ν ∈ {0, 1}n, where νi = 1if feature i is allowed to be modified and νi = 0 otherwise. After computing thederivatives from Eq. (4.2), we obtain the final score using the following equation95S(X) = ∇M0(X)× neg(X)× ν (4.3)where neg(X) is a negation function on vector X (i.e., for each index it changes thevalue from 0 to 1 and vice versa). After this, we choose the index i for modificationusing the following equation:i = argmaxjS(Xj) (4.4)4.3.2 Proposed Selection Methods for Adversarial RetrainingThis 



Section proposes two methods for selecting adversarial samples.Method 1: Based on Distance from Cluster CenterIn this method, adversarial samples are selected by computing the distance of thesesamples from the malware cluster center. For this, a distance score, Sd(X), is firstcalculated for each adversarial samples X using the following equation:Sd(X) = min∀k∈KDistk(X) (4.5)where K is the set of malware clusters, and Distk(X) is a distance measure betweensample X and cluster center k. Note that malware samples may form multiple clus-ters depending on the types of malware in the dataset. Any clustering algorithm(e.g., K-means clustering) can be used for this purpose. To investigate the impactof distance measure types on the proposed selection strategy, we experimented withvarious distance measures such as:96The Euclidean distance :√∑i=1(ai − bi)2 (4.6)Hamming distance :∑i=1(ai 6= bi) (4.7)L1-norm :∑i=1|ai − bi| (4.8)Thereafter, the samples were sorted based on the distance score and the first n numberof samples with the least score were selected for retraining. Intuitively, these are thesamples that should have been correctly classified since they are closer to the clustercenter, yet they were able to fool the classifier.Method 2: Based on Probability Derived from Kernel-based Learning(KBL)In this method, adversarial samples are selected based on the probability derivedfrom a kernel based learning. We implement this by first training a SVM kernel onthe dataset free of any adversarial sample. The decision function f(x) is computedsuch that sign(f(x)) is used to predict the label of a sample. Afterward, in order tocompute the class probability Pr(y = 1|x)–that is, the probability that a sample is amalware–we consider the following approximation based on Platt [220]:Pr(y = 1|x) ≈ PA,B(f) ≡11 + exp(Af +B)(4.9)where f = f(x). Assuming fi is an estimate of f(xi), the best value of A,B isdetermined by minimizing the negative log likelihood of the training data:minA,B−∑itilog(pi) + (1− ti)log(1− pi) (4.10)where pi = PA,B(fi) (Eq. (4.9)) and ti’s are the target label for the optimizationproblem defined as:97ti =N++1N++2, if yi = +1.1N−+2, if yi = −1.(4.11)where N+ and N− are the number of positive and negative samples respectively. UsingEq. (4.9) - (4.11), the probability of adversarial samples to be a malware is computed.We chose the first n adversarial samples that have the least probability of being amalware. Showing a lower probability while maintaining the malware functionalitiesmeans these samples are more likely to fool a classifier.4.4 Experiments and 

ResultsThe following 



Section describes the experimental 

Results. The first set of experimentswere conducted with DNN followed by three other classifiers used to verify perfor-mance improvements.4.4.1 Resilience by Retraining with Deep Neural NetworkIn this 



Section, we present the performance evaluation of adversarial retraining withDNNs when the samples are chosen using our proposed selection strategy and com-pared with the prior works in the literature where retraining samples were randomlyselected. We first trained a DNN on the clean dataset with thee hidden layers con-sisting of 2000, 1000, and 500 neurons respectively. The network achieved 98.3%accuracy. Applying the method and satisfying the constraints outlined in Sec. 4.3.1,293 adversarial samples were generated by modifying the malware samples used inthe test dataset. The classification accuracy dropped to 71% when tested on the ad-versarial data. Thereafter, a certain number (n) of adversarial samples were selectedfor retraining the classifier either randomly or using our proposed strategies.For our first proposed method, based on the distance from the malware cluster center,we considered the malware samples as a single cluster for implementation simplicity.98However, it can easily be extended for multiple clusters using Eq. (4.5) and forany clustering algorithm. The rest of the adversarial samples were combined withthe clean test data for evaluation. Once the adversarial samples were selected, theclassifier was retrained using a new training set consisting of original training samplesand the selected adversarial samples as additional malicious samples. We incrementedthe value of n from 100 with an interval of 10. Each of these experiments was run for20 trials varying random initialization of DNN weights and biases, and the averageof these trials is reported in the 

Results presented below.Figure 4.3 shows the comparison of accuracy and recall values between random selec-tion [161] and our selection based on the Euclidean distance, L1 norm or KBL after aDNN is retrained with adversarial samples. 

Results show that our proposed selectionmethods outperform the random selection method. It is also noteworthy that theKBL approach performs better than other selective strategies.We postulate that, the probability function f in Eq. (4.9) used in the KBL method es-sentially reflects a distance from the hyper-plane, separating the samples in the trans-formed kernel space. Arguably, the better separation between samples, as learned bythe kernel for the transformed feature space, also captures better information fromthe samples, leading to better performance.A comparison of random selection with our other distance-based selection methodsshowed only a few cases of random selection performing slightly better. This occurredmostly in cases where the number of samples for retraining is small (e.g., 100 and110 samples) where the probability of randomly selecting better samples is higher.However, the performance difference is not significant in these cases and this effectdiminishes as more samples are chosen for retraining. Figure 4.3 shows the 

Results ofKBL using the RBF kernel. We also explored the possibility of using a linear kernel,however, it did not result in any performance gain. It has already been shown inthe literature that the linear kernel is a degenerate case of RBF kernel and, hence, a99 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)AccuracyNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(a) Accuracy 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)RecallNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(b) RecallFigure 4.3 a) Accuracy and b) recall values of deep neural network afteradversarial retraining when adversarial samples are selected randomly or se-lectively based on the Euclidean distance, L1 norm or KBL100properly tuned RBF kernel gives similar or better performance in most cases [221].Figure 4.3a further shows that a selection of 58–65% adversarial samples for retrainingachieves better performance for all methods. This is especially evident when com-pared to random selection; KBL achieves a 6% accuracy improvement when trainedwith 65% of adversarial samples. Figure 4.3b also shows that KBL attains betterrecall value, which is more prominent when more than 50% of adversarial samples areselected for retraining. Higher recall value indicates the misclassification of fewer ma-licious samples which is a desired property since the cost of misclassifying a maliciousapplication is significantly higher than that of misclassifying a benign application.We further recorded G-mean and F1-score that represent a balance between the falsepositives and false negatives and the 

Results are shown in Fig. 4.4. The figure showsthat in most of the cases the selective sampling, especially KBL, performs betterthan the random selection strategy, and the highest value for G-mean (Fig. 4.4a) andF1-score (Fig. 4.4b) were obtained when the DNN model was retrained with 65% ofthe adversarial samples.However, when more than 65% of the adversarial samples are selected for retraining,the performance degrades with the increasing number of samples. If too many adver-sarial samples are used for retraining, the classifier starts to over-fit towards malwareand starts misclassifying benign samples at a higher rate. This can further be observedin Fig. 4.3b which shows that the recall value keeps increasing as more adversarialsamples are used for retraining, indicating that more malicious applications are beingcorrectly classified (i.e., the number of false negative is decreasing). This occurs atthe expense of increased misclassification of benign samples when there are too manyadversarial samples, such as above 65%. As a result, a gradual decrease of precisionvalues was also observed as more adversarial samples were used for retraining.A Wilcoxon Signed-Rank Test [21] comparing random selection and KBL based se-lective methods yielded a p-value of 4.4 × 10−4, indicating that the performance101 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)G-meanNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(a) G-mean 0.7 0.72 0.74 0.76 0.78 0.8 0.82 0.84100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)F1-scoreNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(b) F1-scoreFigure 4.4 a) G-mean and b) F1-score of DNN after adversarial retrainingwith random and selective samples102Table4.1AccuracyandG-meanofadversarialretrainingwithdifferentdeepneuralnetworkarchitecturesusingrandom,theEuclideandistancebased,L1normbasedandKBLselection.65%adversarialsampleswereusedforretraining.#ofhiddenlayers(#ofneurons)Accuracy(%)G-meanRandomEuclideanL1KBLRandomEuclideanL1KBL2([500,500])80.2783.8382.4582.550.7810.82180.80560.8092([1000,500])80.2783.8383.7385.250.7810.82290.81710.85353([1000,1000,500])79.8583.6184.0485.740.77770.82110.81790.86163([2000,1000,500])80.6984.3984.5286.080.78430.82620.82020.86554([2000,1000,500,500])80.1783.2479.3381.80.78020.81480.77960.80184([2000,1000,750,500])78.4079.6579.6580.580.77630.78980.78980.7997103improvement of KBL, as compared to random selection, is statistically significant.We further evaluated the impact of network architecture on the performance of ad-versarial retraining. Table 4.1 shows the accuracy and G-mean values of differentadversarial retraining methods with varying network architecture. The table showsa slight variation in terms of performance depending on the number of layers andneurons in each layer of the network. Since networks with less than two hidden layersare considered shallow, we varied the number of hidden layers from two to four witha different number of neurons in each layer. 

Results show that our proposed selectionmechanisms yield better performance than blind random selection, such as 80.69%and 0.7843 (random) vs 86.08% and 0.8655 (KBL) in terms of accuracy and G-meanrespectively.A plot of ROC curve and the AUC using the selection methods is shown in Fig. 4.5.The figure shows our selection methods achieve better AUC than random selection,and the best AUC of 0.958 is achieved by the KBL method vs 0.884 in randomselection. 0 0.2 0.4 0.6 0.8 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateRandom AUC = 0.884Euclidean AUC = 0.934L1 AUC = 0.945KBL AUC = 0.958Figure 4.5 ROC curve and AUC for adversarial retraining using random,the Euclidean distance-based and KBL-based selection1044.4.2 Resilience by retraining with Other ClassifiersWe further evaluated the selective strategies of adversarial retraining with three otherwidely used classifiers, namely, Random forest, SVM and Bayesian classifier. We firstexperimented with how effectively the adversarial samples crafted using DNNs canmislead those classifiers. For this, we first trained a classifier with original samples andtested it with clean test samples ensure its performance was satisfactory. Afterward,we mixed adversarial samples with the clean samples and evaluated the classifier’sperformance. Table 4.2 shows the accuracy of different classifiers when tested withclean samples and samples mixed with adversarial ones. The table shows a significantdrop in classifiers’ performance when tested with adversarial samples. This suggeststhat adversarial samples crafted using DNNs are similarly effective in misleading otherclassifiers as well.We subsequently retrained these classifiers with the adversarial samples crafted usingDNN and compared the efficacy of our selective methods to that of random selection.Figure 4.6 shows the comparison of malware detection accuracy after adversarialretraining with the classifiers as measured by a) SVM, b) Random forest, and c)Bayesian. It is observed that the KBL- and the Euclidean distance-based selectionmethods clearly outperform the random selection method for adversarial retraining inall the classifiers while L1 norm-based selection performs better than random selectionfor SVM and Random forest classifier. Calculating G-mean and F1-score also showedimproved performance by our KBL method. For example, when using the SVMclassifier at 65% samples, G-mean and F1-scores are 0.936 and 0.942 respectively forKBL selection, and 0.885 and 0.902 for random selection. The G-mean and F1-scorevalues are shown in Figures 4.7 and 4.8 respectively.For these classifiers, a Wilcoxon Signed-Rank Test comparing KBL with randomselection method yielded a p-value of < 3.8 × 10−4, validating the performance im-provement as being statistically significant.105 0.82 0.84 0.86 0.88 0.9 0.92100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)AccuracyNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(a) SVM 0.84 0.86 0.88 0.9 0.92100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)AccuracyNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(b) Random Forest 0.7 0.71 0.72 0.73 0.74100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)AccuracyNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(c) BayesianFigure 4.6 Malware detection accuracy of adversarial training for a) SVMb) Random forest and c) Bayesian classifier.106 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)G-meanNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(a) SVM 0.88 0.89 0.9 0.91 0.92 0.93 0.94100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)G-meanNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(b) Random Forest 0.94 0.944 0.948 0.952 0.956 0.96100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)G-meanNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(c) BayesianFigure 4.7 G-mean value of adversarial training for a) SVM b) Randomforest and c) Bayesian classifier.107 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)F-scoreNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(a) SVM 0.88 0.89 0.9 0.91 0.92 0.93 0.94100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)F-scoreNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(b) Random Forest 0.944 0.948 0.952 0.956 0.96100(34%)110(38%)120(41%)130(44%)140(48%)150(51%)160(55%)170(58%)180(61%)190(65%)200(68%)F-scoreNumber (%) of adversarial samples used for retrainingRandom Euclidean KBL L1(c) BayesianFigure 4.8 F1-score value of adversarial training for a) SVM b) Randomforest and c) Bayesian classifier.108Table 4.2 Accuracy of different classifiers when tested on clean samples andsamples mixed with adversarial samples.ClassifierAccuracy (%)Clean Samples Mixed with adveresarial samplesSVM 98.5 73.19Random forest 97.75 73.83Bayesian 97.25 72.824.5 

ConclusionIn this 



Chapter, we studied the vulnerability of malware detection systems against ad-versarial attacks. Rather than selecting random adversarial samples for retraining aclassifier, we proposed two techniques for selecting adversarial samples. Experimental

Results reveal that, compared to the random selection strategy proposed in previousworks, our selective sample selection strategy leads to better performance. Our KBLselection method achieved a 6% performance improvement in accuracy over random-ized selection. While prior works are mostly based on deep neural networks, we showthat other well-known classifiers are also vulnerable against adversarial samples inmalware detection. Our selection methods yielded similar performance improvementfor those classifiers as well. In the next 



Chapter, we deal with an important datacharacteristic known as imbalanced data problem which can make a malware detec-tion system biased and degrade its performance in detecting malicious applications.We propose a novel technique to balance the data and develop a hybrid system thateffectively deals with imbalanced data problem in mobile malware detection.109



Chapter FiveTackling Data Imbalance in ModelTraining5.1 



IntroductionIn the last 



Chapter, we investigated adversarial attacks on malware detection systems.We showed that adversarial samples can successfully evade malware detection bycarefully changing their features. We developed a selective strategy as a defensemechanism against such attacks that retrains a classifier with an optimal adversarialsample set. However, machine learning-based classifiers are designed for balanceddata distribution, that is, the number of samples from each class (i.e., malware andbenign) are expected to be similar. Proposed detection systems in the literature alsosuffer from this inherent assumption. In real-world malware detection problems, thedata distribution is highly imbalanced since benign applications greatly outnumbermalware applications. This imbalance data problem contributes to the degradedperformance of machine learning models for malware detection. We address this gapin the literature. In the following 



Sections, we present a hybrid approach using a novelsynthetic oversampling strategy and an adaptive cost schema to handle imbalanceddata problem in mobile malware detection.1105.2 



MethodologyFigure 5.1 shows the workflow of our proposed approaches for handling imbalanceddata in mobile malware detection. We first propose a new synthetic over-samplingtechnique for mobile malware detection and combine this with our first approach,which is based on the fuzzy synthetic minority over-sampling technique (Fuzzy-SMOTE [16]). This approach is labelled “Proposed Method 1" in Fig. 5.1. Af-terward, we propose a dynamic minority class weight scheme combined with thesynthetic sampling technique, labelled as “Proposed Method 2" in Fig. 5.1. Ourproposed approaches are detailed in following 



Sections.5.2.1 Proposed Technique for Synthetic Malware Oversam-plingData imbalance in mobile malware detection is characterized by a large number ofbenign applications compared to the number of malicious applications. We define theimbalance ratio, ρ, as followsρ =no. of malwaretotal no. of apps× 100% (5.1)To ensure balanced data for training a classifier it is necessary to either increasethe number of samples of the minority class (over-sampling) or decrease the num-ber of samples of the majority class (under-sampling). However, over-sampling andunder-sampling have their own drawbacks. Over-sampling the minority class by sim-ply replicating the samples often 

Results in redundant information for the classifierresulting in over-fitting. On the other hand, under-sampling by removing minoritysamples leads to valuable information loss resulting in poor classification performance,especially for the majority classes. Many works in the literature are, thus, in favor ofover-sampling since it preserves the available information. Synthetic over-sampling111ImbalancedDatasetSyntheticMalwareOver-samplingModifiedFuzzy-smoteDynamicMinority ClassWeightDeep Neural NetworkProposed Method 1Proposed Method 2ClassificationClassificationFigure 5.1 Work flow of proposed imbalanced data handling approachesmitigates these two problems by generating artificial samples of the minority class,thus preserving all the available information of the majority class. Hence, creat-ing synthetic samples of the minority class is generally a better choice for handlingimbalanced data.



Section 2.11 described the most notable works in the literature for data balancingwith synthetic samples. However, these techniques cannot be used in mobile malwaredetection for a few reasons. First, the techniques to generate synthetic samples areprimarily for features having continuous values but the features in mobile malwaredetection (except the features with contextual information) in general are binary.Also, the general idea of interpolating artificial samples between two original samplesby calculating the distance between them is not applicable here. Synthetic samplescreated in this way often do not represent a valid application since we cannot justrandomly remove or add features to an application. A sample generated in this way112may represent an invalid application with broken code sequences. To illustrate this,we take the example of the MoonSMS malware described in Sec. 3.3 which sendspremium messages to steal money from the user. It requires the “SEND_SMS" per-mission to function properly and calls the “sendTextMessage()" function for sendingmessages. This means that the feature vector of this application will have 1 in theindex corresponding to “SEND_SMS" permission, and a specific value in the index rep-resenting the “sendTextMessage()" function depending on the contextual use of thatfunction. If the synthetic sample generation process arbitrarily modifies these values,the corresponding application code will no longer have the functionality to send anSMS. Further, an interpolated value (i.e., a fractional number) cannot be used fora permission feature since it cannot have real values. This means this process ofsynthetic sampling may end up generating additional noise rather than useful datapoints.We propose a technique for over-sampling malicious applications with the followingconditions:• The samples should not be copies of the original application, which 

Results indata redundancy, rather, samples should be generated synthetically to provideadditional information for the classifier.• The generated samples should represent a valid application (i.e., the correspond-ing code should not have any broken sequences).• The synthetic malware samples should have their malicious functionality pre-served.Our goal is to generate synthetic malware samples until a balance between the twoclasses is reached. For this, we took the original malware samples and modified theirfeatures to generate new samples. However, during the synthetic malware samplingprocess, we imposed the constraints described in Sec. 4.3.1 so that the above condi-tions are met and valid samples are generated.113Algorithm 7 Synthetic Malware OversamplingInput: Xmal, Nmal, Nben, Idxm, ρ (candidate malware feature matrix, no. ofcandidate malware, no. of benign apps, indices of the features that are allowedto change, target malware to benign ratio)Output: Xmal // augmented with synthetic malware1: η ← ρ×Nben1−ρ −Nmal // required no. of malware2: while η > 0 do3: if η > Nmal then4: S ← randomly choose Nmal no. of malware from Xmal5: η ← η −Nmal6: else7: S ← randomly choose η no. of malware of from Xmal8: η ← 09: end if10: Syn← SynMal(S, Idxm) // generate synth. malware11: Xmal ← Xmal ∪ Syn12: end while13: function SynMal(V ectors, Idx_mutable)14: for each V in V ectors do15: randomly choose index i such that, V [i] = 0 & i ∈ Idx_mutable16: V [i]← 117: end for18: return V ectors19: end functionAlgorithm 7 shows the pseudo-code of our proposed approach for synthetic malwareoversampling. It uses a candidate malware feature matrix (Xmal), the number ofcandidate malware (Nmal) and benign applications (Nben), indices of mutable features(Idxm), and the desired ratio between malware and benign applications (ρ) as inputs.The desired ratio allows a user the flexibility to change the level of balance betweenmalware and benign applications, oversampling can thus be undertaken to reach bothperfect balance (ρ = 0.5) and moderate imbalance (ρ 6= 0.5). For our experiments,we consider ρ=0.5.Line 1 of the algorithm, depending on Nmal, Nben and ρ, calculates the number ofmalware necessary to reach the target malware-to-benign ratio. An iterative processthen follows. At each iteration except the last, we select Nmal samples from the114current malware set, modify them to generate synthetic malware samples, and addthem to the current set of malware samples (lines 2 to 12). This way of randomchoosing of malware, mutation of features, and augmenting the malware set beforesampling in the next iteration reduces the probability that the same copy of syntheticmalware is generated too many times. Thus, our approach reduces redundancy andalso allows broader coverage of feature space. For the last iteration, only the numberof malware samples required to reach the desired ratio (ρ) is generated and added toXmal. Lines 13 to 19 define the function that modifies the malware feature vector togenerate synthetic samples. It is noteworthy that our model is not affected by anyof the 

Limitations posed by distance measures (e.g., the Euclidean distance) since weare permuting the features rather than interpolating them.5.2.2 Proposed Approach 1: Modified Fuzzy-SMOTEThis approach is based on Fuzzy-SMOTE [16] that over-samples the minority classsamples depending on their membership degree to the minority class based on fuzzyset theory. The fuzzy set theory states that minority samples that have a lower degreeof membership to the minority class are more likely to be misclassified. By over-sampling these samples, the decision region of the minority class can be broadenedso that the classification bias is removed. To illustrate this, we consider two classesCb and Cm denoting classes of benign and malicious applications respectively. Let vband vm denote the centroids of the classes respectively which are defined as followsvb =1NbNb∑n=1xbn (5.2)vm =1NmNm∑n=1xmn (5.3)where Nb and Nm de

Notes the number of samples in benign and malware class re-spectively and xbn and xmn de

Notes the nth sample from benign and malware class115   xj xl vm vb Cm Cb Malware Benign Figure 5.2 Illustrating membership degree for sample xi and xj to class Cband Cmrespectively. The membership degree of a sample x to a class Ci, MCi(x), is thendefined as follows:MCi(x) =[m∑k=b(‖x− vi‖2‖x− vk‖2)1/(d−1)]−1(5.4)where k is set to b and m for benign and malware class respectively,‖x− vi‖ de

Notesthe Euclidean distance of sample x from centroid vi, and d de

Notes the fuzziness ofmembership to each class and is set to 2 as in the original work [16]. A higher valueof MCi(x) indicates that the sample x has a higher membership degree to class Ciand is likely to be classified as a class Ci sample.Figure 5.2 illustrates membership determination. Let xl and xj be two malware classsamples. Because xl is in a region distinctly characterized by malware samples, it willhave a higher membership degree to class Cm than to class Cb, hence it is likely to beclassified to class Cm. But sample xj lies in the fuzzy region. Thus, if a sample xj116has a membership degree such that MCm(xj) ≤ 0.5 and MCb(xj) > 0.5, the sample iscloser to class Cb than to Cm and is more likely to be classified to class Cb. To balancethe data, the minority class samples (i.e., malware) whose membership degree to theminority class satisfy MCm(.) ≤ 0.5 are identified first. These samples in the fuzzyregion are those that Fuzzy-SMOTE over-samples using an interpolation technique.As our first proposed approach, we consider a modification of Fuzzy-SMOTE. Moreprecisely, similar to [16], we also consider the cluster centers of malware (minority)and benign (majority) class samples and consider the same membership function asin Eq. (5.4). Based on the membership function, we also identify malware samplesthat fall within the fuzzy regions. However, contrary to [16], whose interpolationtechnique leads to continuous variables and therefore invalid malware samples, weapply the over-sampling technique proposed in Sec. 5.2.1 to balance the data. Thisensures that our approach considers only the synthetic samples that are valid andpreserve malware functionalities. The candidate malware feature matrix passed asinput for the first approach are only the samples that the fuzzy membership functionin Eq. (5.4) characterizes as having a higher likelihood of misclassification.5.2.3 Proposed Approach 2: Modified Loss Function with Dy-namic Minority Class WeightHere, we propose a hybrid of data level and algorithm level technique to deal withimbalanced data. We use the technique proposed in Sec. 5.2.1 as the data level tech-nique. Additionally, at the algorithm level, we propose a modified loss function with aweight dynamically adjusted for the minority class. Since the cost of misclassifying amalware is greater than that of a benign application, our cost function initially givesmore weight (penalty) to a malicious application. However, as the training progresseskeeping a high penalty can further lead to model instability [222]. Hence, we adjustthis weight dynamically based on the performance of the model on the validation set117during the training phase of the model.For this we use the cross entropy loss and the usual cross entropy loss is defined as− (ylog(p) + (1− y)log(1− p)) (5.5)where y is the original label and p is the probability predicted by the model. Wedefine the weighted cross-entropy loss as− (ylog(p) + Pw(1− y)log(1− p)) (5.6)where Pw is the minority class weight. We initialize this weight as followsPw0 =Nm −Nbγ ×Nb(5.7)Pw = 1 + Pw0 (5.8)where γ is a parameter that controls the scale of the minority class weight. Ourempirical study shows that, depending on the imbalance ratio, the optimal valuefor γ is between 20 and 90. The weight, Pw, is dynamically adjusted based on themodel’s performance during training. We calculate the F1-score of the model on thevalidation set after each epoch and then adjust the weight as followsPw = 1 + Pw0(1 + log(1− F1-score)) (5.9)where Pw0 is defined in Eq. (5.6). The lower bound of this weight is set to 1, i.e., whenthe majority and minority class weights become the same. Here, “F1-score" measurewas used for weight adjustment instead of “accuracy" since accuracy can providemisleading information about classifier performance when dealing with imbalanceddata (detailed in later 



Sections). The characteristic curve of this dynamic weight isshown in Fig. 5.3 with varying initial weights. It shows that the minority class weightremains high where the model is suffering from class imbalance and its F1-score is118 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1(1 - F1-score) 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 1.45 1.5Initial minority class weight 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 1.45 1.5Minority class weight 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 1.45 1.5Figure 5.3 Minority class weight characteristic curvepoor. The minority class weight starts to decrease towards the region where the modelstarts to adapt and the F1-score improves.5.3 Experiments and 

Results5.3.1 Dataset PreparationWe took 50,000 samples from the datasets described in Sec. 2.6. Five hundred mal-ware samples were randomly selected as minority class samples and the number ofbenign samples was varied from 4500 to 49500 to formulate several datasets withvarious imbalance ratios. A total of 10 datasets were produced in the process withan imbalance ratio varying from 10% to 1%. Stratified 10-fold cross-validation wasused for each of the datasets. We took 20 trials of the experiments for each datasetand the average of these trials is reported in the 

Results shown below.1195.3.2 



DiscussionImpact of Data Imbalance on Detection PerformanceFigure 5.4 shows the impact of data imbalance on the classifier performance. Whilethe classification accuracy is above 97% for all the datasets, the figure clearly showsthat the model suffers greatly from data imbalance in terms of recall rate and F1-score. It also shows that as the data imbalance increases, the classifier becomesmore biased towards the majority class (benign applications) as evidenced throughincreased overall accuracy but decreasing recall value and F1-score. Such models withlow recall and F1-score are useless as far as malware detection is concerned.Performance ComparisonFigure 5.5 shows the performance comparison among the over-sampling, under-sampling,and our two proposed approaches in terms of accuracy, precision, recall, F1-score, andG-mean. The figure shows that both of our proposed approaches outperform othertechniques for all the imbalanced ratios. The performance improvement is especiallynoticeable when the imbalanced ratio is lower than 5% for precision and F1-score (Fig-ures 5.5a and 5.5c respectively). For example, with the dataset of 1% imbalance ratio,our two approaches achieve 0.87 and 0.89 F1-score respectively while over-samplingand under-sampling achieve 0.64 and 0.17 respectively.We noticed different trends in terms of recall and F1-score between over-samplingand under-sampling. In the case of over-sampling, duplicates of the malware samplesare created to balance the dataset. Even though the classifier suffers less from dataimbalance, the duplication leads to over-fitting, and thus, it does not perform aswell on the unseen malware samples as the under-sampling technique. As a result,the recall value for over-sampling is much lower than that of the under-samplingtechnique. However, since no information is lost from the benign samples, the classifierstill performs well on benign samples which is reflected in the better F1-score values.120 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 110% 9% 8% 7% 6% 5% 4% 3% 2% 1%Imbalance RatioAccuracy Recall F1-scoreFigure 5.4 Effect of imbalance on malware detection (Imbalanced ratio of10% indicates only 10% of the total samples are malware)This can further be observed in precision values which are better for the over-samplingtechnique compared to under-sampling.On the other hand, under-sampling removes samples from the benign class to balancethe data. This removes the bias towards the majority class, resulting in better recallvalue. However, since a lot of information from the majority class is lost, the detectionof benign samples suffers greatly. This is reflected by the drop in F1-score withincreasing level of imbalance, for example, 0.52 and 0.27 for 5% and 2% imbalanceratio respectively (Fig. 5.5c).Our proposed approaches outperform all these methods. Since our approaches donot remove samples from the majority class they do not suffer from information loss.Further, unlike under-sampling, they do not create mere copies of the minority classsample; rather, they create synthetic samples by modifying feature values. Hence,they do not suffer from over-fitting. Moreover, our sample generation technique(described in Sec. 5.2.1) permutes the features in the binary domain rather than121interpolating their values. This enables it to explore valid sample space to reducedata imbalance. Consequently, we see better performance by our approaches.Further, in our second approach (Sec. 5.2.3), we devised a loss function that assignsdynamic weights to the minority class to reduce classification bias towards the major-ity class. This allows this approach to perform better on minority class. This effect isdemonstrated in Fig. 5.5b and 5.5d which shows our second approach achieves higherrecall and G-mean value.In Fig. 5.5b, we see that in a very few cases, and only in terms of recall value, under-sampling performs slightly better than our proposed methods. This is observed whenthe imbalance ratio goes lower than 3%. Our detailed inspection of the 

Results showsthat when data imbalance is this high (i.e., imbalance ratio is low), under-samplingremoves most of the benign samples from the training data. This allows the classifierto learn the malware class while ignoring most of the information from the benignclass, resulting in slightly better recall value. However, this comes with a significantcompromise in the overall performance which is evident through very low precisionvalue and F1-score of the under-sampling method (Figures 5.5a and 5.5c). In contrast,our methods do not suffer from such 

Limitations, as seen in Fig. 5.5a and Fig. 5.5cwhich show our methods outperform under-sampling by a significant margin in termsof both precision and F1-score.Table 5.1 shows a detailed performance comparison of our methods with existingmethods for 10% and 5% imbalance ratio. The table shows that under-samplingreduces more false negatives (FN) than over-sampling; however, since it removes asignificant number of majority samples, the false positive (FP) increases significantly.On the other hand, over-sampling reduces FP but they perform poorer than ourmethods due to over-fitting. Our proposed approaches outperform other methodswhile our second approach achieves the best true positive (TP) count (482 for 10%and 477 for 5%). It is also noteworthy that while the two proposed approaches122 0.2 0.4 0.6 0.810% 9% 8% 7% 6% 5% 4% 3% 2% 1%PrecisionImbalance RatioImbalancedOver-sampleUnder-sampleProposed app. 1Proposed app. 2(a) Precision 0.7 0.75 0.8 0.85 0.9 0.9510% 9% 8% 7% 6% 5% 4% 3% 2% 1%Recall RateImbalance RatioImbalancedOver-sampleUnder-sampleProposed app. 1Proposed app. 2(b) RecallFigure 5.5 Performance comparison of the proposed approaches with over-sampling and under-sampling techniques and when no imbalance-specifictechnique is used.123 0.2 0.4 0.6 0.810% 9% 8% 7% 6% 5% 4% 3% 2% 1%F1-scoreImbalance RatioImbalancedOver-sampleUnder-sampleProposed app. 1Proposed app. 2(c) F1-score 0.8 0.85 0.9 0.9510% 9% 8% 7% 6% 5% 4% 3% 2% 1%G-meanImbalance RatioImbalancedOver-sampleUnder-sampleProposed app. 1Proposed app. 2(d) G-meanFigure 5.5 (cont.) performance comparison of the proposed approacheswith over-sampling and under-sampling techniques and when no imbalance-specific technique is used.124Table5.1Detailedperformancecomparisonfor10%and5%imbalancedratio.ImbalanceRatioMethod#ofmalware#ofbenignTPTNFPFNAccuracy(%)G-meanF1-score10%Imbalanced50045004054450509597.100.8950.848Oversample50045004334415856796.960.9220.851Undersample500450046041903104093.000.9260.724Fuzzy-SMOTE50045004754452482598.540.9690.929ProposedApproach150045004804477232099.140.9770.957ProposedApproach250045004824480201899.240.9800.9625%Imbalanced50095004019407939998.080.8910.807Oversample500950041193841168997.950.9010.800Undersample500950046486858153691.490.9210.522Fuzzy-SMOTE50095004509450505099.000.9460.900ProposedApproach150095004649465353699.290.9620.929ProposedApproach250095004779464362399.410.9750.942perform similarly in terms of accuracy, G-mean, and F1-score, the second approachachieves slight improvement in terms of FN compared to the first approach (18 for10% and 23 for 5%). This is because the second approach assigns dynamic weight tothe minority class which allows it to further reduce the classification bias towards themajority class.Figures 5.6a and 5.6b show the ROC curve along with the AUC for 5% and 10%imbalanced ratio respectively. The figures show that our proposed techniques achievebetter AUC values than the existing techniques for all the imbalanced ratios.A Wilcoxon Signed-Rank Test [21] comparing our techniques with existing techniquesshows that the p-value is always less than 5.1×10−3, indicating that the performanceimprovement of our techniques is statistically significant.Further Analysis of the Proposed ApproachesTo further analyze the characteristics of our proposed approaches we conducted ex-periments by varying the range of fuzzy region (described in Sec. 5.2.2). Figure 5.7shows the performance variation of the first proposed approach with various fuzzyregions. The model’s performance is highly dependent on the chosen range of thefuzzy region. We see that different imbalance ratio of the dataset requires differentfuzzy region for optimal performance (e.g., region 0.6 for 7% imbalance ratio, and0.5 and 0.8 for 5% and 3% imbalance ratio respectively for F1-score measure). Also,for different performance measures, different fuzzy region yields optimal performance(e.g., in case of 3% imbalance ratio, the best performance is found when the fuzzyregion is 0.8 for F1-score, 0.6 for G-mean, and 0.4 for specificity). Hence, deciding thefuzzy region for the best performance can prove to be a difficult task. On the otherhand, the dynamic minority class weight in the second proposed approach is auto-matically adjusted based on the model performance during the training time usingEq. (5.9). As a result, no manual parameter adjustment is needed and the approachdoes not suffer from such drawbacks. Thus, of the two proposed approaches, the126 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateImbalanced AUC = 0.946Over-sample AUC = 0.953Under-sample AUC = 0.962Proposed approach 1 AUC = 0.975Proposed approach 2 AUC = 0.985(a) Imbalance ratio = 5% 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0  0.2  0.4  0.6  0.8  1True Positive RateFalse Positive RateImbalanced AUC = 0.948Over-sample AUC = 0.954Under-sample AUC = 0.967Proposed approach 1 AUC = 0.991Proposed approach 2 AUC = 0.992(b) Imbalance ratio = 10%Figure 5.6 ROC curve and AUC of the proposed approaches, over-samplingand under-sampling techniques, and when no imbalance-specific technique isused.127 98.8 98.9 99 99.1 99.2 99.3 99.4 99.5 99.6 99.7 0.4  0.5  0.6  0.7  0.8  0.9Accuracy (%)Fuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(a) Accuracy 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.4  0.5  0.6  0.7  0.8  0.9PrecisionFuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(b) Precision 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.4  0.5  0.6  0.7  0.8  0.9RecallFuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(c) Recall 0.991 0.992 0.993 0.994 0.995 0.996 0.997 0.998 0.999 0.4  0.5  0.6  0.7  0.8  0.9SpecificityFuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(d) Specificity 0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.4  0.5  0.6  0.7  0.8  0.9G-meanFuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(e) G-mean 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.4  0.5  0.6  0.7  0.8  0.9F1-scoreFuzzy RegionImb. ratio = 3%Imb. ratio = 7%Imb. ratio = 10%(f) F1-scoreFigure 5.7 Performance of the proposed approach 1 with varying fuzzyregion.128second approach has higher potentials for application in mobile malware detectionwith imbalanced data.5.4 

ConclusionIn this 



Chapter, we addressed the real-world data imbalance characteristics in mobilemalware detection. We showed that traditional over-sampling and under-sampling donot work well since they suffer from over-fitting and information loss. The existingsynthetic sampling technique also suffers from invalid sample generation. We pro-posed a novel synthetic malware generation technique that generates mobile malwaresamples with valid functionalities. Additionally, we proposed a hybrid techniquethat utilizes a loss function assigning weights to the minority class (i.e., malware)dynamically during the training process. Our experimental 

Results show that ourproposed approaches do not suffer from the above-mentioned drawbacks and the hy-brid approach improves F1-score by 9%. In the next 



Chapter, we conclude this thesisby presenting conclusive remarks and some directions for further extensions of thiswork.129



Chapter Six

Conclusion and 



Future WorksIn 



Chapter 1 we formulated three major research objectives that we wanted to addressin this thesis. In the previous three 



Chapters, those problems have been addressedin detail. In this 



Chapter, we make concluding remarks based on the findings of theresearch that addressed those objectives. First, this work systematically investigatedthe impact of different feature categories on malware detection performance. Al-though several works in the literature have performed feature selection in a randomlychosen feature category set, none have systematically analyzed the effect of individualfeature categories and their different combinations on malware detection performance.This study filled this gap. Additionally, this work proposed a technique to embedcontextual information into the feature space allowing this information to be usedin combination with other feature categories. Extensive experiments were conductedto analyze the performance impact of different feature categories on mobile malwaredetection. The experiments show that as a single feature category, “permission" pro-vides the best performance with 96.26% accuracy without contextual information,and “API calls" provides the best performance with contextual information obtaining97.89% accuracy. However, when features were combined, the combination of permis-sion, API calls, and dynamic features provided the best performance attaining 98.21%accuracy and outperforming the combination of all categories (97.93% accuracy).130This work then proceeded to make the detection system robust against adversarialattacks. Tackling adversarial attacks is especially important in malware detectiondomain since they can take down a well-performing detection system by carefullycrafting malware samples. The work proposed two approaches to select the best sam-ple set from self-crafted adversarial samples for retraining classifiers because retrainingwith too many samples further degrades detection performance. Randomly selectinga sample set, as done in prior studies, 

Results in sub-optimal performance. The KBLmethod proposed in this work achieves a 6% improvement in terms of detection ac-curacy. The Wilcoxon Signed-Rank test indicates that the performance improvementof our proposed approach is statistically significant with p-value < 3.8 × 10−4. Onepossible extension of our approach for further improvement could be to implementan ensemble learning system where different types of classifiers with different archi-tectures are trained and used together for classification. However, this drasticallyincreases the complexity of the system which may not be practical especially whendealing with large amounts of data and when fast deployment is required.Finally, to address the real-world malware data characteristics, this thesis consideredimbalanced class distribution in the training data set. Since existing data balancingtechniques generate invalid malware samples, this work proposed a novel syntheticmalware sample generation technique that preserves the malicious functionality of theoriginal samples. To further counter the majority class bias, a dynamic cost schemawas devised that automatically assigns minority class weight during model training.This hybrid technique improved F1-score by 9%.The techniques developed in this thesis have great practical aspects that can provideassistance to system planners especially in designing security components. The meth-ods proposed in this thesis can be used as a standalone component and integratedinto the system for malware detection making use of its feature extraction and robustclassification mechanism. Also, the individual contributions of this thesis such asprotection against adversarial attacks, and imbalanced data handling mechanism can131be used separately to improve the robustness of a malware detection system.



Future WorksConsidering that a malware detection system is required to be robust and continuouslyadapt itself to newfound vulnerabilities and exploits, we present some directions forextending the work presented in this thesis:• Since attackers are constantly looking for new vulnerabilities in the system andattempting to exploit them, the problem of mobile malware detection is alwaysevolving. Handling such issues (generally known as concept drift) in this regardis very important to ensure that the system is able to detect new generations ofmalware. The imbalanced data handling technique in this work can be utilizedto address this. The method for handling imbalanced data deals with differencesin data distribution in terms of sample size. It can be extended for conceptdrift that changes the data distribution from a different perspective since newerfeatures get added to the feature space or certain features become strongerrelative to others over time. In addition to being able to learn newer attacks,it is also important to form a mechanism to forget obsolete information. Forexample, Sec. 2.6 mentioned some attacks becoming invalidated or outdated dueto OS upgrades. The information particularly associated with such attacks needto be forgotten by the model for better generalization and overall performanceof the detection system. Identifying the point where concept drift starts tooccur is also another major challenge in this regard and which can be addressedwith time-stamped data and periodically evaluating the classifier looking forsignificant performance drop.• The adversarial sample crafting technique in 



Chapter 4 can be extended toprovide explainable decisions. For example, as demonstrated in this thesis, it ispossible to identify the minimum number of features that need to be modified in132order to mislead a classifier. These features can be taken as the most influentialfeatures and analyzing their semantics and corresponding codes can provide thereasoning behind a particular prediction. This can be valuable in designing abetter detection system, especially for resolving the false negatives since it canexplain why a particular malicious behavior went undetected.• The robustness mechanisms against adversarial attacks can be extended toachieve resilience against zero-day attacks. Zero-day attacks refer to the ex-ploitation of newfound vulnerabilities in the system that have not been patchedyet. Similar to adversarial retraining, this requires a proactive strategy. Con-tinual research on current systems must be conducted and whenever a newvulnerability is found, the associated feature set should be identified. Malwaresamples utilizing the features corresponding with newfound vulnerabilities canthen be crafted and a classifier can be retrained with these samples using theadversarial retraining strategies.• The models designed in this work can be utilized to develop a federated machinelearning system where a central machine learning system resides in a remoteserver and local detection models are trained in individual mobile devices. Eachdevice trains their model on local data and periodically sends updated infor-mation about the model to the server. The server aggregates the informationand updates the central model which is then propagated back to the individ-ual devices. This enables a model to generalize better and learn from a widerange of data. The major challenge here is to design a proper system that caneffectively aggregate the information obtained from the individual devices thatcan update the central model. An efficient protocol that reduces the number ofcommunication rounds and optimally transfers useful information between theserver and the devices also needs to be designed.133



References[1] Natarajan Sundaram, Cherian Thomas, and Loganathan Agilandeeswari. “AReview: Customers Online Security on Usage of Banking Technologies in Smart-phones and Computers.” In: Pertanika Journal of Science & Technology 27.1(2019).[2] MF Brunette et al. “Use of smartphones, computers and social media amongpeople with SMI: opportunity for intervention”. In: Community mental healthjournal 55.6 (2019), pp. 973–978.[3] Elia Abi-Jaoude, Karline Treurnicht Naylor, and Antonio Pignatiello. “Smart-phones, social media use and youth mental health”. In: CMAJ 192.6 (2020),E136–E141.[4] Yongliang Li and Linli Wu. “Research on Marketing and Profit Mode Innova-tion of Intelligent Mobile E-commerce”. In: (2019).[5] Smartphone users worldwide. https://www.statista.com/statistics/330695/.Accessed: April 23, 2020.[6] Chao Yang, Jialong Zhang, and Guofei Gu. “Understanding the Market-leveland Network-level Behaviors of the Android Malware Ecosystem”. In: Dis-tributed Computing Systems (ICDCS), 2017 IEEE 37th International Confer-ence on. IEEE. 2017, pp. 2452–2457.[7] Cyber security report.(https://www.cyber.gov.au/report). Accessed: Jun 27, 2020. July 2019.[8] Kimberly Tam. “The Analysis and Classification of Android Malware”. In:(2016).[9] Li Chen et al. “Semi-supervised classification for dynamic Android malwaredetection”. In: arXiv preprint arXiv:1704.05948 (2017).[10] Aiman A Abu Samra, Kangbin Yim, and Osama A Ghanem. “Analysis ofclustering technique in android malware detection”. In: 2013 Seventh Interna-tional Conference on Innovative Mobile and Internet Services in UbiquitousComputing. IEEE. 2013, pp. 729–733.134[11] Suleiman Y Yerima, Sakir Sezer, and Gavin McWilliams. “Analysis of Bayesianclassification-based approaches for Android malware detection”. In: IET Infor-mation Security 8.1 (2014), pp. 25–36.[12] Veelasha Moonsamy, Jia Rong, and Shaowu Liu. “Mining permission patternsfor contrasting clean and malicious android applications”. In: Future Genera-tion Computer Systems 36 (2014), pp. 122–132.[13] Qi Li and Xiaoyu Li. “Android malware detection based on static analysis ofcharacteristic tree”. In: Cyber-Enabled Distributed Computing and KnowledgeDiscovery (CyberC), 2015 International Conference on. IEEE. 2015, pp. 84–91.[14] Daniel Arp et al. “DREBIN: Effective and Explainable Detection of AndroidMalware in Your Pocket.” In: Ndss. Vol. 14. 2014, pp. 23–26.[15] Ke Xu, Yingjiu Li, and Robert H Deng. “ICCDetector: ICC-based malwaredetection on Android”. In: IEEE Transactions on Information Forensics andSecurity 11.6 (2016), pp. 1252–1264.[16] Yanping Xu et al. “Fuzzy–synthetic minority oversampling technique: Over-sampling based on fuzzy set theory for Android malware detection in imbal-anced datasets”. In: International Journal of Distributed Sensor Networks 13.4(2017), p. 1550147717703116.[17] Nitesh V Chawla et al. “SMOTE: synthetic minority over-sampling technique”.In: Journal of artificial intelligence research 16 (2002), pp. 321–357.[18] Yajin Zhou and Xuxian Jiang. “Dissecting android malware: Characterizationand evolution”. In: Security and Privacy (SP), 2012 IEEE Symposium on.IEEE. 2012, pp. 95–109.[19] Kevin Allix et al. “Androzoo: Collecting millions of android apps for the re-search community”. In: 2016 IEEE/ACM 13th Working Conf. on MSR. IEEE.2016, pp. 468–471.[20] Ron Kohavi et al. “A study of cross-validation and bootstrap for accuracyestimation and model selection”. In: Ijcai. Vol. 14. 2. Montreal, Canada. 1995,pp. 1137–1145.[21] Frank Wilcoxon. “Individual comparisons by ranking methods”. In: Break-throughs in statistics. Springer, 1992, pp. 196–202.[22] Mahbub E Khoda et al. “Mobile Malware Detection: An Analysis of DeepLearning Model.” In: ICIT. 2019, pp. 1161–1166.135[23] Mahbub E Khoda et al. “Mobile Malware Detection-An Analysis of the Impactof Feature Categories”. In: International Conference on Neural InformationProcessing. Springer. 2018, pp. 486–498.[24] Mahbub E Khoda et al. “Robust Malware Defense in Industrial IoT Applica-tions using Machine Learning with Selective Adversarial Samples”. In: IEEETransactions on Industry Applications (2019).[25] Mahbub Khoda et al. “Selective Adversarial Learning for Mobile Malware”. In:2019 18th IEEE International Conference On Trust, Security And Privacy InComputing And Communications/13th IEEE International Conference On BigData Science And Engineering (TrustCom/BigDataSE). IEEE. 2019, pp. 272–279.[26] Mahbub Khoda et al. “Mobile Malware Detection with Imbalanced Data usinga Novel Synthetic Oversampling Strategy and Deep Learning”. In: 2020 16thInternational Conference on Wireless and Mobile Computing, Networking andCommunications (WiMob). IEEE. 2020.[27] Upkar Varshney, Ronald J Vetter, and Ravi Kalakota. “Mobile commerce: Anew frontier”. In: Computer 33.10 (2000), pp. 32–38.[28] Eric WT Ngai and Angappa Gunasekaran. “A review for mobile commerceresearch and applications”. In: Decision support systems 43.1 (2007), pp. 3–15.[29] Viswanath Venkatesh, Venkataraman Ramesh, and Anne P Massey. “Under-standing usability in mobile commerce”. In: Communications of the ACM 46.12(2003), pp. 53–56.[30] Upkar Varshney and Ron Vetter. “Mobile commerce: framework, applicationsand networking support”. In: Mobile networks and Applications 7.3 (2002),pp. 185–198.[31] Keng Siau, Ee-Peng Lim, and Zixing Shen. “Mobile commerce: Promises, chal-lenges and research agenda”. In: Journal of Database Management (JDM) 12.3(2001), pp. 4–13.[32] Ellen Opsahl Vinbæk et al. “On Online Banking Authentication for All: AComparison of BankID Login Efficiency Using Smartphones Versus Code Gen-erators”. In: International Conference on Human-Computer Interaction. Springer.2019, pp. 365–374.[33] Oktay Yildiz and Ibrahim Alper Doğru. “Permission-based android malwaredetection system using feature selection with genetic algorithm”. In: Inter-national Journal of Software Engineering and Knowledge Engineering 29.02(2019), pp. 245–262.136[34] Harshverdhan Shukla. “A Survey Paper on Android Operating System”. In:Journal of the Gujarat Research Society 21.5 (2019), pp. 299–305.[35] Pantawee Pantaweesak, Phongpat Sontamino, and Danupon Tonnayopas. “Al-ternative Software for Evaluating Preliminary Rock Stability of Tunnel usingRock Mass Rating (RMR) and Rock Mass Quality (Q) on Android Smart-phone”. In: Engineering Journal 23.1 (2019), pp. 95–108.[36] Ming Fan. “Modeling of android software behavior feature and its applica-tions in malicious program analysis”. PhD thesis. The Hong Kong PolytechnicUniversity, 2019.[37] Jie Ling, Xuejing Wang, and Yu Sun. “Research of Android Malware Detectionbased on ACO Optimized Xgboost Parameters Approach”. In: 3rd Interna-tional Conference on Mechatronics Engineering and Information Technology(ICMEIT 2019). Atlantis Press. 2019.[38] Number of android devices. https://www.macrumors.com/2017/05/17/2-billion-active-android-devices/. Accessed: May 7, 2020.[39] Mohammed K Alzaylaee, Suleiman Y Yerima, and Sakir Sezer. “Emulator vsreal phone: Android malware detection using machine learning”. In: Proceed-ings of the 3rd ACM on International Workshop on Security And PrivacyAnalytics. ACM. 2017, pp. 65–72.[40] Android OS architecture. Accessed: June 30, 2020. url: https://en.wikipedia.org/wiki/File:Android-System-Architecture.svg.[41] Fauzia Idrees Abro. “Investigating Android permissions and intents for mal-ware detection”. PhD thesis. City, Universtiy of London, 2018.[42] Attia Qamar, Ahmad Karim, and Victor Chang. “Mobile malware attacks: Re-view, taxonomy & future directions”. In: Future Generation Computer Systems97 (2019), pp. 887–909.[43] Shreya Khemani, Darshil Jain, and Gaurav Prasad. “Android malware detec-tion techniques”. In: Emerging Research in Computing, Information, Commu-nication and Applications. Springer, 2019, pp. 449–457.[44] Darell JJ Tan, Tong-Wei Chua, and Vrizlynn LL Thing. “Securing android: asurvey, taxonomy, and challenges”. In: ACM Computing Surveys (CSUR) 47.4(2015), pp. 1–45.[45] Moses Aprofin Ashawa and Sarah Morris. “Analysis of Android malware de-tection techniques: a systematic review”. In: (2019).[46] Md Russel and Omar Faruque Khan. “AndroShow: Pattern Identification ofObfuscated Android Malware Application”. In: (2019).137[47] Chenglin Li et al. “Android malware detection based on factorization machine”.In: IEEE Access 7 (2019), pp. 184008–184019.[48] William Enck et al. “TaintDroid: an information-flow tracking system for real-time privacy monitoring on smartphones”. In: ACM Transactions on ComputerSystems (TOCS) 32.2 (2014), p. 5.[49] Chao Yang et al. “Droidminer: Automated mining and characterization of fine-grained malicious behaviors in android applications”. In: European symposiumon research in computer security. Springer. 2014, pp. 163–182.[50] Guillermo Suarez-Tangil et al. “Dendroid: A text mining approach to analyzingand classifying code structures in android malware families”. In: Expert Systemswith Applications 41.4 (2014), pp. 1104–1117.[51] Sen Chen et al. “GUI-Squatting Attack: Automated Generation of AndroidPhishing Apps”. In: IEEE Transactions on Dependable and Secure Computing(2019).[52] Mahdi Moodi and Mahdieh Ghazvini. “A new method for assigning appro-priate labels to create a 28 Standard Android Botnet Dataset (28-SABD)”.In: Journal of Ambient Intelligence and Humanized Computing 10.11 (2019),pp. 4579–4593.[53] Yu Feng et al. “Apposcopy: Semantics-based detection of android malwarethrough static analysis”. In: Proceedings of the 22nd ACM SIGSOFT Inter-national Symposium on Foundations of Software Engineering. 2014, pp. 576–587.[54] Xin Su et al. “A deep learning approach to android malware feature learn-ing and detection”. In: Trustcom/BigDataSE/ISPA, 2016 IEEE. IEEE. 2016,pp. 244–251.[55] Zhenlong Yuan et al. “Droid-Sec: deep learning in android malware detection”.In: ACM SIGCOMM Computer Communication Review. Vol. 44. 4. ACM.2014, pp. 371–372.[56] Zi Wang et al. “DroidDeepLearner: Identifying Android malware using deeplearning”. In: Sarnoff Symposium, 2016 IEEE 37th. IEEE. 2016, pp. 160–165.[57] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. “Deep learning”. In: nature521.7553 (2015), pp. 436–444.[58] Yoshiko Ariji et al. “Automatic detection and classification of radiolucent le-sions in the mandible on panoramic radiographs using a deep learning objectdetection technique”. In: Oral surgery, oral medicine, oral pathology and oralradiology 128.4 (2019), pp. 424–430.138[59] Zhong-Qiu Zhao et al. “Object detection with deep learning: A review”. In:IEEE transactions on neural networks and learning systems 30.11 (2019),pp. 3212–3232.[60] Barret Zoph et al. “Learning data augmentation strategies for object detec-tion”. In: arXiv preprint arXiv:1906.11172 (2019).[61] Seijoon Kim et al. “Spiking-yolo: Spiking neural network for real-time objectdetection”. In: arXiv preprint arXiv:1903.06530 (2019).[62] Longlong Jing and Yingli Tian. “Self-supervised visual feature learning withdeep neural networks: A survey”. In: IEEE Transactions on Pattern Analysisand Machine Intelligence (2020).[63] Iván González-Díaz et al. “Perceptually-guided deep neural networks for ego-action prediction: Object grasping”. In: Pattern Recognition 88 (2019), pp. 223–235.[64] David P Williams. “Demystifying deep convolutional neural networks for sonarimage classification”. In: (2019).[65] Yian Seo and Kyung-shik Shin. “Hierarchical convolutional neural networks forfashion image classification”. In: Expert Systems with Applications 116 (2019),pp. 328–339.[66] David Balderas, Pedro Ponce, and Arturo Molina. “Convolutional long shortterm memory deep neural networks for image sequence prediction”. In: ExpertSystems with Applications 122 (2019), pp. 152–162.[67] Yunyang Xiong, Hyunwoo J Kim, and Varsha Hedau. “Antnets: Mobile convo-lutional neural networks for resource efficient image classification”. In: arXivpreprint arXiv:1904.03775 (2019).[68] Michał Grochowski, Arkadiusz Kwasigroch, and Agnieszka Mikołajczyk. “Se-lected technical issues of deep neural networks for image classification pur-poses”. In: Bulletin of the Polish Academy of Sciences. Technical Sciences 67.2(2019).[69] Francisco Erivaldo Fernandes Junior and Gary G Yen. “Particle swarm op-timization of deep neural networks architectures for image classification”. In:Swarm and Evolutionary Computation 49 (2019), pp. 62–74.[70] Yanan Sun et al. “Evolving deep convolutional neural networks for image clas-sification”. In: IEEE Transactions on Evolutionary Computation (2019).[71] Sabato Marco Siniscalchi et al. “Exploiting deep neural networks for detection-based speech recognition”. In: Neurocomputing 106 (2013), pp. 148–157.139[72] Yanghao Li et al. “Scale-aware trident networks for object detection”. In: Pro-ceedings of the IEEE International Conference on Computer Vision. 2019,pp. 6054–6063.[73] Saining Xie et al. “Exploring randomly wired neural networks for image recog-nition”. In: Proceedings of the IEEE International Conference on ComputerVision. 2019, pp. 1284–1293.[74] Yuanwei Wu, Ziming Zhang, and Guanghui Wang. “Unsupervised deep featuretransfer for low resolution image classification”. In: Proceedings of the IEEEInternational Conference on Computer Vision Workshops. 2019.[75] Bohan Zhuang et al. “Structured binary neural networks for accurate imageclassification and semantic segmentation”. In: Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition. 2019, pp. 413–422.[76] Tong He et al. “Bag of tricks for image classification with convolutional neuralnetworks”. In: Proceedings of the IEEE Conference on Computer Vision andPattern Recognition. 2019, pp. 558–567.[77] Weijie Chen et al. “All you need is a few shifts: Designing efficient convolu-tional neural networks for image classification”. In: Proceedings of the IEEEConference on Computer Vision and Pattern Recognition. 2019, pp. 7241–7250.[78] Shuyang Dou et al. “Deep learning’s impact on pattern matching for designbased metrology and design based inspection”. In: Metrology, Inspection, andProcess Control for Microlithography XXXIII. Vol. 10959. International Societyfor Optics and Photonics. 2019, 109592G.[79] Dmitri Roussinov and Nadezhda Puchnina. “Combining neural networks andpattern matching for ontology mining-a meta learning inspired approach”. In:2019 IEEE 13th International Conference on Semantic Computing (ICSC).IEEE. 2019, pp. 63–70.[80] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. “A fast learning al-gorithm for deep belief nets”. In: Neural computation 18.7 (2006), pp. 1527–1554.[81] Yoshua Bengio et al. “Greedy layer-wise training of deep networks”. In: Ad-vances in neural information processing systems. 2007, pp. 153–160.[82] Vinod Nair and Geoffrey E Hinton. “Rectified linear units improve restrictedboltzmann machines”. In: ICML. 2010.[83] Geoffrey E Hinton. “Training products of experts by minimizing contrastivedivergence”. In: Neural computation 14.8 (2002), pp. 1771–1800.140[84] Vladimir Vapnik. Statistical learning theory. 1998. Vol. 3. Wiley, New York,1998.[85] Leif E Peterson. “K-nearest neighbor”. In: Scholarpedia 4.2 (2009), p. 1883.[86] Leo Breiman. “Random forests”. In: Machine learning 45.1 (2001), pp. 5–32.[87] Harald Binder and Martin Schumacher. “Adapting prediction error estimatesfor biased complexity selection in high-dimensional bootstrap samples”. In:Statistical Applications in Genetics and Molecular Biology 7.1 (2008).[88] Mila Parkour. Contagio malware database. 2013.[89] VirusTotal. url: https://www.virustotal.com/.[90] Meenu Ganesh et al. “Cnn-based android malware detection”. In: 2017 In-ternational Conference on Software Security and Assurance (ICSSA). IEEE.2017, pp. 60–65.[91] Wenjia Li et al. “An Android malware detection approach using weight-adjusteddeep learning”. In: 2018 International Conference on Computing, Networkingand Communications (ICNC). IEEE. 2018, pp. 437–441.[92] Luo Shiqi et al. “Android malicious code Classification using Deep Belief Net-work.” In: KSII Transactions on Internet & Information Systems 12.1 (2018).[93] Yi Zhang, Yuexiang Yang, and Xiaolei Wang. “A novel android malware detec-tion approach based on convolutional neural network”. In: Proceedings of the2nd International Conference on Cryptography, Security and Privacy. 2018,pp. 144–149.[94] ElMouatez Billah Karbab et al. “MalDozer: Automatic framework for androidmalware detection using deep learning”. In: Digital Investigation 24 (2018),S48–S59.[95] Dongfang Li, Zhaoguo Wang, and Yibo Xue. “Fine-grained android malwaredetection based on deep learning”. In: 2018 IEEE Conference on Communica-tions and Network Security (CNS). IEEE. 2018, pp. 1–2.[96] Chihiro Hasegawa and Hitoshi Iyatomi. “One-dimensional convolutional neu-ral networks for Android malware detection”. In: 2018 IEEE 14th Interna-tional Colloquium on Signal Processing & Its Applications (CSPA). IEEE.2018, pp. 99–102.[97] Fabio Martinelli, Fiammetta Marulli, and Francesco Mercaldo. “Evaluatingconvolutional neural network for effective mobile malware detection”. In: Pro-cedia Computer Science 112 (2017), pp. 2372–2381.141[98] Hani Alshahrani et al. “DDefender: Android application threat detection us-ing static and dynamic analysis”. In: 2018 IEEE International Conference onConsumer Electronics (ICCE). IEEE. 2018, pp. 1–6.[99] R Winsniewski. Android–apktool: A tool for reverse engineering android apkfiles. 2012.[100] Androguard. androguard/androguard. Nov. 2017. url: https://github.com/androguard/androguard.[101] Monkey tool. https://cuckoo-droid.readthedocs.io/en/latest/. Accessed: May12, 2020.[102] Genymotion. Accessed: May 31, 2020. url: https://www.genymotion.com/.[103] Monkey tool. https : / / developer . android . com/ studio / test /monkey. html.Accessed: February 17, 2020.[104] Caren Marzban. “The ROC curve and the area under it as performance mea-sures”. In: Weather and Forecasting 19.6 (2004), pp. 1106–1114.[105] Andrew P Bradley. “The use of the area under the ROC curve in the evaluationof machine learning algorithms”. In: Pattern recognition 30.7 (1997), pp. 1145–1159.[106] Jerome Fan, Suneel Upadhye, and Andrew Worster. “Understanding receiveroperating characteristic (ROC) curves”. In: Canadian Journal of EmergencyMedicine 8.1 (2006), pp. 19–20.[107] Jean Bergeron et al. “Static detection of malicious code in executable pro-grams”. In: Int. J. of Req. Eng 2001.184-189 (2001), p. 79.[108] David Barrera et al. “A 



Methodology for Empirical Analysis of Permission-based Security Models and Its Application to Android”. In: Proceedings ofthe 17th ACM Conference on Computer and Communications Security. CCS’10. Chicago, Illinois, USA: ACM, 2010, pp. 73–84. isbn: 978-1-4503-0245-6.doi: 10.1145/1866307.1866317. url: http://doi.acm.org/10.1145/1866307.1866317.[109] Hossein Fereidooni et al. “ANASTASIA: ANdroid mAlware detection usingSTatic analySIs of Applications”. In: 2016 8th IFIP international conferenceon new technologies, mobility and security (NTMS). IEEE. 2016, pp. 1–5.[110] Laura Gheorghe et al. “Smart malware detection on Android”. In: Security andCommunication Networks 8.18 (2015), pp. 4254–4272. doi: 10.1002/sec.1340.eprint: https : //onlinelibrary.wiley. com/doi/pdf/10 .1002/sec . 1340. url:https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.1340.142[111] Lok Kwong Yan and Heng Yin. “Droidscope: Seamlessly reconstructing the{OS} and dalvik semantic views for dynamic android malware analysis”. In:Presented as part of the 21st {USENIX} Security Symposium ({USENIX}Security 12). 2012, pp. 569–584.[112] Wei Yang et al. “AppContext: Differentiating Malicious and Benign MobileApp Behaviors Using Context”. In: Proceedings of the 37th International Con-ference on Software Engineering - Volume 1. ICSE ’15. Florence, Italy: IEEEPress, 2015, pp. 303–313. isbn: 978-1-4799-1934-5. url: http://dl.acm.org/citation.cfm?id=2818754.2818793.[113] Annamalai Narayanan et al. “Context-aware, Adaptive and Scalable AndroidMalware Detection through Online Learning (extended version)”. In: CoRRabs/1706.00947 (2017). arXiv: 1706.00947. url: http://arxiv.org/abs/1706.00947.[114] Annamalai Narayanan et al. “A multi-view context-aware approach to Androidmalware detection and malicious code localization”. In: Empirical SoftwareEngineering (Aug. 2017). issn: 1573-7616. doi: 10.1007/s10664-017-9539-8.url: https://doi.org/10.1007/s10664-017-9539-8.[115] Erika Chin et al. “Analyzing inter-application communication in Android”. In:Proceedings of the 9th international conference on Mobile systems, applica-tions, and services. ACM. 2011, pp. 239–252.[116] Lucas Davi et al. “Privilege escalation attacks on android”. In: InternationalConference on Information Security. Springer. 2010, pp. 346–360.[117] William Enck, Machigar Ongtang, and Patrick McDaniel. “Understanding an-droid security”. In: IEEE security & privacy 7.1 (2009), pp. 50–57.[118] Adrienne Porter Felt et al. “Permission Re-Delegation: Attacks and Defenses.”In: USENIX Security Symposium. Vol. 30. 2011.[119] Roman Schlegel et al. “Soundcomber: A Stealthy and Context-Aware SoundTrojan for Smartphones.” In: NDSS. Vol. 11. 2011, pp. 17–33.[120] Fengguo Wei, Sankardas Roy, Xinming Ou, et al. “Amandroid: A precise andgeneral inter-component data flow analysis framework for security vetting ofandroid apps”. In: Proceedings of the 2014 ACM SIGSAC Conference on Com-puter and Communications Security. ACM. 2014, pp. 1329–1341.[121] Damien Octeau et al. “Effective inter-component communication mapping inandroid with epicc: An essential step towards holistic security analysis”. In:Effective Inter-Component Communication Mapping in Android with Epicc:An Essential Step Towards Holistic Security Analysis (2013).143[122] Damien Octeau et al. “Composite constant propagation: Application to an-droid inter-component communication analysis”. In: Proceedings of the 37th In-ternational Conference on Software Engineering-Volume 1. IEEE Press. 2015,pp. 77–88.[123] Li Li et al. “Iccta: Detecting inter-component privacy leaks in android apps”.In: Proceedings of the 37th International Conference on Software Engineering-Volume 1. IEEE Press. 2015, pp. 280–291.[124] Ali Feizollah et al. “AndroDialysis: analysis of android intent effectiveness inmalware detection”. In: computers & security 65 (2017), pp. 121–134.[125] Martin Szydlowski et al. “Challenges for dynamic analysis of ios applications”.In: Open Problems in Network Security. Springer, 2012, pp. 65–77.[126] Sangeeta Rani and Kanwalvir Singh Dhindsa. “Android Malware Detectionin Official and Third Party Application Stores”. In: International Journal ofAdvanced Networking and Applications 9.4 (2018), pp. 3506–3509.[127] Yongkai Fan et al. “Software Malicious Behavior Analysis Model based onSystem Call and Function Interface”. In: 2019 IEEE 9th Annual InternationalConference on CYBER Technology in Automation, Control, and IntelligentSystems (CYBER). IEEE. 2019, pp. 59–64.[128] Sungtaek Oh, Woong Go, and Taejin Lee. “A Study on The behavior-basedMalware Detection Signature”. In: International Conference on Broadbandand Wireless Computing, Communication and Applications. Springer. 2016,pp. 663–670.[129] Hiromu Yakura et al. “Malware analysis of imaged binary samples by con-volutional neural network with attention mechanism”. In: Proceedings of theEighth ACM Conference on Data and Application Security and Privacy. 2018,pp. 127–134.[130] Dewashish Upadhyay et al. “Detecting Malicious Behavior of Android Appli-cations”. In: International Journal of Science Technology & Engineering 2.10(2016), pp. 663–668.[131] Laura Gheorghe et al. “Smart malware detection on Android”. In: Security andCommunication Networks 8.18 (2015), pp. 4254–4272.[132] Xi Xiao et al. “Android malware detection based on system call sequences andLSTM”. In: Multimedia Tools and Applications 78.4 (2019), pp. 3979–3999.[133] Vitor Monte Afonso et al. “Identifying Android malware using dynamicallyobtained features”. In: Journal of Computer Virology and Hacking Techniques11.1 (2015), pp. 9–17.144[134] Marko Dimjašević et al. “Evaluation of android malware detection based onsystem calls”. In: Proceedings of the 2016 ACM on International Workshop onSecurity And Privacy Analytics. ACM. 2016, pp. 1–8.[135] Iker Burguera, Urko Zurutuza, and Simin Nadjm-Tehrani. “Crowdroid: Behavior-based Malware Detection System for Android”. In: Proceedings of the 1st ACMWorkshop on Security and Privacy in Smartphones and Mobile Devices. SPSM’11. Chicago, Illinois, USA: ACM, 2011, pp. 15–26. isbn: 978-1-4503-1000-0.doi: 10.1145/2046614.2046619. url: http://doi.acm.org/10.1145/2046614.2046619.[136] Deepa K. et al. “Identification of Android malware using refined system calls”.In: Concurrency and Computation: Practice and Experience 31.20 (2019), e5311.doi: 10.1002/cpe.5311. url: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5311.[137] Juan Ramos et al. “Using tf-idf to determine word relevance in documentqueries”. In: Proceedings of the first instructional conference on machine learn-ing. Vol. 242. Piscataway, NJ. 2003, pp. 133–142.[138] Abada Abderrahmane et al. “Android Malware Detection Based on SystemCalls Analysis and CNN Classification”. In: 2019 IEEE Wireless Communica-tions and Networking Conference Workshop (WCNCW). IEEE. 2019, pp. 1–6.[139] Taniya Bhatia and Rishabh Kaushal. “Malware detection in android based ondynamic analysis”. In: 2017 International Conference on Cyber Security AndProtection Of Digital Services (Cyber Security). IEEE. 2017, pp. 1–6.[140] Jae-wook Jang et al. “Andro-dumpsys: anti-malware system based on the sim-ilarity of malware creator and malware centric information”. In: computers &security 58 (2016), pp. 125–138.[141] Gerardo Canfora, Francesco Mercaldo, and Corrado Aaron Visaggio. “An hmmand structural entropy based detector for android malware: An empirical study”.In: Computers & Security 61 (2016), pp. 1–18.[142] Shifu Hou et al. “Deep neural networks for automatic android malware de-tection”. In: Proceedings of the 2017 IEEE/ACM International Conference onAdvances in Social Networks Analysis and Mining 2017. 2017, pp. 803–810.[143] Robin Nix and Jian Zhang. “Classification of android apps and malware us-ing deep neural networks”. In: 2017 International joint conference on neuralnetworks (IJCNN). IEEE. 2017, pp. 1871–1878.[144] Wei Wang, Mengxue Zhao, and Jigang Wang. “Effective android malware de-tection with a hybrid model based on deep autoencoder and convolutional145neural network”. In: Journal of Ambient Intelligence and Humanized Comput-ing 10.8 (2019), pp. 3035–3043.[145] Xu Jiang et al. “Android malware detection using fine-grained features”. In:Scientific Programming 2020 (2020).[146] Hongliang Liang, Yan Song, and Da Xiao. “An end-To-end model for Androidmalware detection”. In: 2017 IEEE International Conference on Intelligenceand Security Informatics (ISI). IEEE. 2017, pp. 140–142.[147] Zhenlong Yuan, Yongqiang Lu, and Yibo Xue. “Droiddetector: android mal-ware characterization and detection using deep learning”. In: Tsinghua Scienceand Technology 21.1 (2016), pp. 114–123.[148] R Vinayakumar et al. “Detecting Android malware using long short-term mem-ory (LSTM)”. In: Journal of Intelligent & Fuzzy Systems 34.3 (2018), pp. 1277–1288.[149] Fei Tong and Zheng Yan. “A hybrid approach of mobile malware detectionin Android”. In: Journal of Parallel and Distributed Computing 103 (2017),pp. 22–31.[150] Ankita Kapratwar. “Static and Dynamic Analysis for Android Malware De-tection”. In: (2016).[151] Christian Szegedy et al. “Intriguing properties of neural networks”. In: CoRRabs/1312.6199 (2014).[152] Christoph Molnar. Interpretable machine learning. Lulu. com, 2019.[153] Dong C Liu and Jorge Nocedal. “On the limited memory BFGS methodfor large scale optimization”. In: Mathematical programming 45.1-3 (1989),pp. 503–528.[154] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining andHarnessing Adversarial Examples”. In: CoRR abs/1412.6572 (2015).[155] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. “One pixel at-tack for fooling deep neural networks”. In: IEEE Transactions on EvolutionaryComputation 23.5 (2019), pp. 828–841.[156] TB Brown et al. “Adversarial patch. arxiv e-prints (dec. 2017)”. In: arXivpreprint cs.CV/1712.09665 1.2 (2017), p. 4.[157] Anish Athalye et al. “Synthesizing robust adversarial examples”. In: arXivpreprint arXiv:1707.07397 (2017).146[158] Hyrum S Anderson et al. “Evading machine learning malware detection”. In:Black Hat (2017).[159] Hung Dang, Yue Huang, and Ee-Chien Chang. “Evading classifiers by mor-phing in the dark”. In: Proceedings of the 2017 ACM SIGSAC Conference onComputer and Communications Security. ACM. 2017, pp. 119–133.[160] Wei Yang et al. “Malware detection in adversarial settings: Exploiting featureevolutions and confusions in android apps”. In: Proceedings of the 33rd AnnualComputer Security Applications Conference. ACM. 2017, pp. 288–302.[161] Kathrin Grosse et al. “Adversarial examples for malware detection”. In: Euro-pean Symposium on Research in Computer Security. Springer. 2017, pp. 62–79.[162] Nicolas Papernot et al. “Distillation as a defense to adversarial perturbationsagainst deep neural networks”. In: 2016 IEEE Symposium on Security andPrivacy (SP). IEEE. 2016, pp. 582–597.[163] Kathrin Grosse et al. “On the (statistical) detection of adversarial examples”.In: arXiv preprint arXiv:1702.06280 (2017).[164] Zhitao Gong, Wenlu Wang, and Wei-Shinn Ku. “Adversarial and clean dataare not twins”. In: arXiv preprint arXiv:1704.04960 (2017).[165] Seyda Ertekin et al. “Learning on the border: active learning in imbalanceddata classification”. In: Proceedings of the sixteenth ACM conference on Con-ference on information and knowledge management. ACM. 2007, pp. 127–136.[166] Julio Hernandez, Jesús Ariel Carrasco-Ochoa, and José Francisco Martínez-Trinidad. “An empirical study of oversampling and undersampling for instanceselection methods on imbalance datasets”. In: Iberoamerican Congress on Pat-tern Recognition. Springer. 2013, pp. 262–269.[167] Tom Fawcett and Foster J Provost. “Combining Data Mining and MachineLearning for Effective User Profiling.” In: KDD. 1996, pp. 8–13.[168] Kazuo J Ezawa, Moninder Singh, and Steven W Norton. “Learning goal ori-ented Bayesian networks for telecommunications risk management”. In: ICML.1996, pp. 139–147.[169] David D Lewis and Jason Catlett. “Heterogeneous uncertainty sampling forsupervised learning”. In: Proceedings of the eleventh international conferenceon machine learning. 1994, pp. 148–156.[170] Susan Dumais et al. “Inductive learning algorithms and representations fortext categorization”. In: Proceedings of the seventh international conference onInformation and knowledge management. ACM. 1998, pp. 148–155.147[171] Marko Grobelnik. “Feature selection for unbalanced class distribution andnaive bayes”. In: ICML. 1999.[172] David D Lewis and Marc Ringuette. “A comparison of two learning algorithmsfor text categorization”. In: Third annual symposium on document analysis andinformation retrieval. Vol. 33. 1994, pp. 81–93.[173] William W Cohen. “Learning to classify English text with ILP methods”. In:Advances in inductive logic programming 32 (1995), pp. 124–143.[174] Miroslav Kubat, Robert C Holte, and Stan Matwin. “Machine learning for thedetection of oil spills in satellite radar images”. In: Machine learning 30.2-3(1998), pp. 195–215.[175] Jerzy Błaszczyński and Jerzy Stefanowski. “Neighbourhood sampling in bag-ging for imbalanced data”. In: Neurocomputing 150 (2015), pp. 529–542.[176] Bartosz Krawczyk. “Learning from imbalanced data: open challenges and fu-ture directions”. In: Progress in Artificial Intelligence 5.4 (2016), pp. 221–232.[177] Michael Pazzani et al. “Reducing misclassification costs”. In: Proceedings of theEleventh International Conference on Machine Learning. 1994, pp. 217–225.[178] Pedro Domingos. “Metacost: A general method for making classifiers cost-sensitive”. In: Proceedings of the fifth ACM SIGKDD international conferenceon Knowledge discovery and data mining. ACM. 1999, pp. 155–164.[179] Nathalie Japkowicz. “The class imbalance problem: Significance and strate-gies”. In: Proc. of the Int’l Conf. on Artificial Intelligence. 2000.[180] Charles X Ling and Chenghui Li. “Data mining for direct marketing: Problemsand solutions.” In: KDD. Vol. 98. 1998, pp. 73–79.[181] Edward Raff and Charles Nicholas. “Malware Classification and Class Imbal-ance via Stochastic Hashed LZJD”. In: Proceedings of the 10th ACM Workshopon Artificial Intelligence and Security. ACM. 2017, pp. 111–120.[182] Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. “Experimentalperspectives on learning from imbalanced data”. In: Proceedings of the 24thinternational conference on Machine learning. ACM. 2007, pp. 935–942.[183] Rehan Akbani, Stephen Kwek, and Nathalie Japkowicz. “Applying supportvector machines to imbalanced datasets”. In: Machine learning: ECML 2004(2004), pp. 39–50.[184] Miroslav Kubat, Stan Matwin, et al. “Addressing the curse of imbalancedtraining sets: one-sided selection”. In: Icml. Vol. 97. Nashville, USA. 1997,pp. 179–186.148[185] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. “Borderline-SMOTE: a newover-sampling method in imbalanced data sets learning”. In: International con-ference on intelligent computing. Springer. 2005, pp. 878–887.[186] Chumphol Bunkhumpornpat, Krung Sinapiromsaran, and Chidchanok Lursin-sap. “Safe-level-smote: Safe-level-synthetic minority over-sampling techniquefor handling the class imbalanced problem”. In: Pacific-Asia conference onknowledge discovery and data mining. Springer. 2009, pp. 475–482.[187] Taeho Jo and Nathalie Japkowicz. “Class imbalances versus small disjuncts”.In: ACM Sigkdd Explorations Newsletter 6.1 (2004), pp. 40–49.[188] Hansang Lee, Minseok Park, and Junmo Kim. “Plankton classification on im-balanced large scale database via convolutional neural networks with transferlearning”. In: 2016 IEEE international conference on image processing (ICIP).IEEE. 2016, pp. 3713–3717.[189] Inderjeet Mani and I Zhang. “kNN approach to unbalanced data distributions:a 

Case Study involving information extraction”. In: Proceedings of workshop onlearning from imbalanced datasets. Vol. 126. 2003.[190] Ricardo Barandela et al. “The imbalanced training sample problem: Underor over sampling?” In: Joint IAPR international workshops on statistical tech-niques in pattern recognition (SPR) and structural and syntactic pattern recog-nition (SSPR). Springer. 2004, pp. 806–814.[191] Dennis L Wilson. “Asymptotic properties of nearest neighbor rules using editeddata”. In: IEEE Transactions on Systems, Man, and Cybernetics 3 (1972),pp. 408–421.[192] Shoujin Wang et al. “Training deep neural networks on imbalanced data sets”.In: 2016 international joint conference on neural networks (IJCNN). IEEE.2016, pp. 4368–4374.[193] Justin M Johnson and Taghi M Khoshgoftaar. “Survey on deep learning withclass imbalance”. In: Journal of Big Data 6.1 (2019), p. 27.[194] Tsung-Yi Lin et al. “Focal loss for dense object detection”. In: Proceedings ofthe IEEE international conference on computer vision. 2017, pp. 2980–2988.[195] Keisuke Nemoto et al. “Classification of rare building change using cnn withmulti-class focal loss”. In: IGARSS 2018-2018 IEEE International Geoscienceand Remote Sensing Symposium. IEEE. 2018, pp. 4663–4666.[196] Haishuai Wang et al. “Predicting hospital readmission via cost-sensitive deeplearning”. In: IEEE/ACM transactions on computational biology and bioinfor-matics 15.6 (2018), pp. 1968–1978.149[197] Yulu Zhang et al. “Image classification with category centers in class imbal-ance situation”. In: 2018 33rd Youth Academic annual conference of ChineseAssociation of Automation (YAC). IEEE. 2018, pp. 359–363.[198] Wan Ding et al. “Facial action recognition using very deep networks for highlyimbalanced class distribution”. In: 2017 Asia-Pacific Signal and InformationProcessing Association Annual Summit and Conference (APSIPA ASC). IEEE.2017, pp. 1368–1372.[199] Chen Huang et al. “Learning deep representation for imbalanced classifica-tion”. In: Proceedings of the IEEE conference on computer vision and patternrecognition. 2016, pp. 5375–5384.[200] Ziwei Liu et al. “Large-scale celebfaces attributes (celeba) dataset”. In: Re-trieved August 15 (2018), p. 2018.[201] Qi Dong, Shaogang Gong, and Xiatian Zhu. “Imbalanced deep learning by mi-nority class incremental rectification”. In: IEEE transactions on pattern anal-ysis and machine intelligence 41.6 (2018), pp. 1367–1381.[202] Lei Cen et al. “A probabilistic discriminative model for android malware de-tection with decompiled source code”. In: IEEE Transactions on Dependableand Secure Computing 12.4 (2015), pp. 400–412.[203] Borja Sanz et al. “Puma: Permission usage to detect malware in android”.In: International Joint Conference CISIS’12-ICEUTE 12-SOCO 12 SpecialSessions. Springer. 2013, pp. 289–298.[204] Rajvardhan Oak et al. “Malware Detection on Highly Imbalanced Data throughSequence Modeling”. In: Proceedings of the 12th ACM Workshop on ArtificialIntelligence and Security. 2019, pp. 37–48.[205] Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers forlanguage understanding”. In: arXiv preprint arXiv:1810.04805 (2018).[206] Palo Alto Networks. 2019. Accessed: May 31, 2020. url: https : / / www .paloaltonetworks.com/products/secure-the-network/wildfire.[207] Songqing Yue. “Imbalanced malware images classification: a CNN based ap-proach”. In: arXiv preprint arXiv:1708.08042 (2017).[208] Zhenxiang Chen et al. “Machine learning based mobile malware detection us-ing highly imbalanced network traffic”. In: Information Sciences 433 (2018),pp. 346–364.[209] Vasileios Kouliaridis et al. “A survey on mobile malware detection techniques”.In: IEICE Transactions on Information and Systems 103.2 (2020), pp. 204–211.150[210] Number of android applications. https://www.statista.com/statistics/276623/number - of - apps - available - in - leading - app- stores/. Accessed: February 17,2020.[211] Kathy Wain Yee Au et al. “Pscout: analyzing the android permission specifi-cation”. In: Proceedings of the 2012 ACM conference on Computer and com-munications security. ACM. 2012, pp. 217–228.[212] Android debug bridge. https : / /developer . android . com/ studio / command -line/adb.html. Accessed: April 16, 2020.[213] strace command. https://linux.die.net/man/1/strace. Accessed: May 17, 2020.[214] Robert Tarjan. “Depth-first search and linear graph algorithms”. In: SIAMjournal on computing 1.2 (1972), pp. 146–160.[215] Mu Zhang et al. “Semantics-aware android malware classification using weightedcontextual api dependency graphs”. In: Proceedings of the 2014 ACM SIGSACconference on computer and communications security. 2014, pp. 1105–1116.[216] Curse of dimensionality. Accessed: July 14, 2020. url: https://medium.com/@cxu24/dd60b5611543.[217] J. Ross Quinlan. C4.5: Programs for Machine Learning. San Francisco, CA,USA: Morgan Kaufmann Publishers Inc., 1993. isbn: 1-55860-238-0.[218] Jorge Blasco and Thomas M Chen. “Automated generation of colluding appsfor experimental research”. In: Journal of Computer Virology and HackingTechniques 14.2 (2018), pp. 127–138.[219] Nicolas Papernot et al. “The 

Limitations of deep learning in adversarial set-tings”. In: 2016 IEEE European Symposium on Security and Privacy (Eu-roS&P). IEEE. 2016, pp. 372–387.[220] John C. Platt. “Probabilistic Outputs for Support Vector Machines and Com-parisons to Regularized Likelihood Methods”. In: ADVANCES IN LARGEMARGIN CLASSIFIERS. MIT Press, 1999, pp. 61–74.[221] S Sathiya Keerthi and Chih-Jen Lin. “Asymptotic behaviors of support vectormachines with Gaussian kernel”. In: Neural computation 15.7 (2003), pp. 1667–1689.[222] Salman H Khan et al. “Cost-sensitive learning of deep feature representationsfrom imbalanced data”. In: IEEE transactions on neural networks and learningsystems 29.8 (2017), pp. 3573–3587.151
